<result>
	<can>
		<phrase>In this paper we consider &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of learning control policies for textbased games</example>
		<phraseLemma>in this paper we consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We employ a deep reinforcement learning framework to jointly learn state representations and action policies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using game rewards as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; feedback</example>
		<phraseLemma>np use np as np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate our approach on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our approach on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two game worlds comparing against baselines using bagofwords and bagofbigrams for state representations</example>
		<phraseLemma>we evaluate we approach on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We evaluate our approach on two game worlds comparing against baselines &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using bagofwords and bagofbigrams for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; state representations</example>
		<phraseLemma>np use np for np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we address &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we address&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of learning control policies for textbased strategy games</example>
		<phraseLemma>in this paper we address np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The simplest method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a bagofwords representation derived from the text description</example>
		<phraseLemma>np be to use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to convert &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>An alternative approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to convert&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text descriptions to prespeciﬁed representations using annotated training data commonly used in language grounding tasks</example>
		<phraseLemma>np be to convert np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to learn &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In contrast our goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; useful representations in conjunction with control policies</example>
		<phraseLemma>np be to learn np</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; against &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our algorithm against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; baselines of a random player and models that use bagofwords or bagofbigrams representations for a state</example>
		<phraseLemma>we compare np against np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We compare our algorithm against baselines of a random player and models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that use bagofwords or bagofbigrams representations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a state</example>
		<phraseLemma>np that use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learned from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The grounding of commands to actions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learned from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a transcript manually annotated with actions and state attributes</example>
		<phraseLemma>np be learn from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learn &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance Mnih &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learn control strategies using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; convolutional neural networks trained with a variant of Qlearning</example>
		<phraseLemma>np learn np use np</phraseLemma>
	</can>
	<can>
		<phrase>The need for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The need for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a better semantic representation of the text is evident from the average performance of this representation in playing MUDgames</example>
		<phraseLemma>the need for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are able to capture &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>They are more robust than BOW to small variations in word usage and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are able to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; underlying semantics of sentences to some extent</example>
		<phraseLemma>np be able to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; available in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We consider all possible actions and objects &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;available in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the game and predict both for each state using the same network</example>
		<phraseLemma>np available in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the value of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In each iteration i we update the parameters to reduce the discrepancy between the predicted value of the current state Q i and the expected Qvalue given the reward rt and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the value of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the next state maxa Q</example>
		<phraseLemma>np the value of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We keep track of the agents previous experiences in a memory D 1 Instead of performing updates to the Qvalue &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using transitions from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current episode we sample a random transition from D Updating the parameters in this way avoids issues due to strong correlation when using transitions of the same episode</example>
		<phraseLemma>np use np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; instead of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In practice online updates to the parameters θ are performed over a mini batch of state transitions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;instead of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single transition</example>
		<phraseLemma>np instead of np</phraseLemma>
	</can>
	<can>
		<phrase>We conduct experiments on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We conduct experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two worlds a smaller Home world we created ourselves and a larger more complex Fantasy world created by Evennias developers</example>
		<phraseLemma>we conduct experiment on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; up to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We observe that the Fantasy world is moderately sized with a vocabulary of words and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;up to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different descriptions for a room</example>
		<phraseLemma>np up to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provides &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Therefore this domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provides an interesting challenge for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; language understanding</example>
		<phraseLemma>np provide np for np</phraseLemma>
	</can>
	<can>
		<phrase>For instance &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance the kitchen has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an apple that the player can eat</example>
		<phraseLemma>for instance np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is placed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>At the start of each game episode the player &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is placed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a random room and provided with a randomly selected quest</example>
		<phraseLemma>np be place in np</phraseLemma>
	</can>
	<can>
		<phrase>We make use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Due to the large command space in this game &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we make use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; cues provided by the game itself to narrow down the set of possible objects to consider in each state</example>
		<phraseLemma>np we make use of np</phraseLemma>
	</can>
	<can>
		<phrase>For instance in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MUD example in Figure 1 the game provides a list of possible exits</example>
		<phraseLemma>for instance in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Figure 1 &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance in the MUD example &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the game provides a list of possible exits</example>
		<phraseLemma>np in figure 1 np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we run &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For all evaluation episodes we run&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the agent following an greedy policy with = 1 which makes the agent choose the best action according to its Qvalues 1 of the time</example>
		<phraseLemma>for np we run np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we used &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the prioritized sampling we used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ρ = 1 for both worlds</example>
		<phraseLemma>for np we use np</phraseLemma>
	</can>
	<can>
		<phrase>We can observe that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can observe that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Random baseline performs quite poorly completing only around 1 of quests on average obtaining a low reward of around 1</example>
		<phraseLemma>we can observe that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; of 1</phrase>
		<frequency>10</frequency>
		<example>The BOWDQN model performs signiﬁcantly better and is able to complete around 1 of the quests &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with an average reward of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np of 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is due to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The improvement in reward &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both greater quest success rate and a lower rate of issuing invalid commands</example>
		<phraseLemma>np be due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; along with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Figure 1 illustrates the performance of the BOWDQN and BIDQN models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;along with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their simpler versions BOWLIN and BILIN which use a single linear layer for φA</example>
		<phraseLemma>np along with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Figure 1 illustrates the performance of the BOWDQN and BIDQN models along with their simpler versions BOWLIN and BILIN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which use a single linear layer for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; φA</example>
		<phraseLemma>np which use np for np</phraseLemma>
	</can>
	<can>
		<phrase>It can be seen that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It can be seen that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the DQN models clearly achieve better performance than their linear counterparts which points to them modeling the control policy better</example>
		<phraseLemma>it can be see that np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate all the models on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Fantasy world in the same manner as before and report reward quest completion rates and Qvalues</example>
		<phraseLemma>we evaluate np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; but &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To test this we created a 1 Home world &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the same rooms but&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a completely different map changing the locations of the rooms and the pathways between them</example>
		<phraseLemma>np with np but np</phraseLemma>
	</can>
	<can>
		<phrase>We initialized &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We initialized the LSTM part of an LSTMDQN agent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parameters θR learnt from the original home world and trained it on the new world</example>
		<phraseLemma>we initialize np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to learn &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Figure 1 demonstrates that the agent with transferred parameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; quicker than an agent starting from scratch initialized with random parameters reaching the optimal policy almost epochs earlier</example>
		<phraseLemma>np be able to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In fact we observe 1 different subspaces &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one type of room along with its corresponding object and quest words</example>
		<phraseLemma>np each for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are very close to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance food items like pizza and rooms like kitchen &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are very close to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the word hungry which appears in a quest description</example>
		<phraseLemma>np be very close to np</phraseLemma>
	</can>
	<can>
		<phrase>We are grateful to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We are grateful to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the developers of Evennia the game framework upon which this work is based</example>
		<phraseLemma>we be grateful to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tend to occur in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Distributional models induce vectorbased semantic representations of words from their contextual distributions in corpora exploiting the observation that words with related meanings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tend to occur in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similar linguistic contexts</example>
		<phraseLemma>np tend to occur in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moreover approximating semantic similarity by graded geometric distance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a vector space is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an effective strategy to address the many linguistic phenomena that are better characterized in gradient rather than discrete terms such as synonymy selectional preferences and semantic priming</example>
		<phraseLemma>np in np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; rather than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moreover approximating semantic similarity by graded geometric distance in a vector space is an effective strategy to address the many linguistic phenomena that are better characterized in gradient &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;rather than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; discrete terms such as synonymy selectional preferences and semantic priming</example>
		<phraseLemma>np rather than np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As our source of referential attributes we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; FreeBase a knowledge base of structured information on a wide range of entities of different semantic types</example>
		<phraseLemma>as np we use np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As our source of referential attributes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use FreeBase a knowledge base of structured information&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on a wide range of entities of different semantic types</example>
		<phraseLemma>np we use np on np</phraseLemma>
	</can>
	<can>
		<phrase>We built &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We built two datasets for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our experiments one for countries and one for cities with data automatically extracted from FreeBase 1 We consider two datasets in order to check that the mapping we seek can be established not just for one possible handpicked type of entities we leave it to future work to study very different kinds of entities such as people or institutions</example>
		<phraseLemma>we build np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The dataset records all simple attributes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; complex attributes of at most two hops in the FreeBase graph without manual inspection</example>
		<phraseLemma>np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>We apply &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We apply the same process to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Cities dataset which consists of cities from the intersection of the distributional and FreeBase city lists</example>
		<phraseLemma>we apply np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which consists of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We apply the same process to the Cities dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which consists of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; cities from the intersection of the distributional and FreeBase city lists</example>
		<phraseLemma>np which consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In effect we predict each output variable &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with an independent logistic regression model based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a constant set of input features</example>
		<phraseLemma>np with np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; apply it to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We design the model using the Countries dataset and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;apply it to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Cities without further tuning to test its robustness</example>
		<phraseLemma>np apply it to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We optimize the parameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with gradient descent using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Cross Entropy error function</example>
		<phraseLemma>np with np use np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we report &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For binary attributes we report&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the attributes mean accuracy</example>
		<phraseLemma>for np we report np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we consider &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For numeric attributes we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; attribute prediction a ranking task</example>
		<phraseLemma>for np we consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are consistent with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However we want to measure not only how well the model can rank the countries in the test set but also whether these predictions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training set</example>
		<phraseLemma>np be consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; indicating &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It has range 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with smaller numbers indicating&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; better ranking</example>
		<phraseLemma>np with np indicate np</phraseLemma>
	</can>
	<can>
		<phrase>The amount of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The amount of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; skew differs considerably between the two datasets though</example>
		<phraseLemma>the amount of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the length of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In other words its average prediction is off by about one 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the length of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ranked list for each attribute</example>
		<phraseLemma>np the length of np</phraseLemma>
	</can>
	<can>
		<phrase>We obtain &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We obtain numeric and binary attribute groups with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; median sizes of 1 and 1 attributes per group respectively</example>
		<phraseLemma>we obtain np with np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the attribute groups for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both types sorted by quality</example>
		<phraseLemma>table 1 show np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; respectively for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For each group we report average normalized rank score and accuracy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;respectively for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both DIST 1 REF and the baseline</example>
		<phraseLemma>np respectively for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represent &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However since distributional models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represent words as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; aggregated distributions of their contexts and compute semantic similarity from these context distributions the contexts that they use need to be generic enough to yield meaningful overlap between concepts</example>
		<phraseLemma>np represent np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; since there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We decided not to exclude them from evaluation for robustness sake &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;since there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no automatic way to identify contextually unsupported attributes in a new dataset</example>
		<phraseLemma>np since there be np</phraseLemma>
	</can>
	<can>
		<phrase>We obtain &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We obtain good results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; meaningful attributes that are arguably strongly contextually grounded such as geographical and geopolitical attributes member of</example>
		<phraseLemma>we obtain np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; makes &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While this seems surprising at ﬁrst glance the form of government attribute &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in FreeBase makes&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; very ﬁnegrained distinctions</example>
		<phraseLemma>np in np make np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; DIST 1 REF extracts even more precise distance information from distributional vectors</example>
		<phraseLemma>table 1 show that np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Groundtruth great circle distances between items are computed using the FreeBase longitude and latitude values for DIST 1 REF &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use its predicted latitude and longitude values for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WORD 1 VEC the cosines between the corresponding distributional vectors</example>
		<phraseLemma>np we use np for np</phraseLemma>
	</can>
	<can>
		<phrase>In line with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In line with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our goal to extract referential attributes thus we are satisﬁed to see that DIST 1 REF manages to minimize this bias and distill the referential part from the distributional representations</example>
		<phraseLemma>in line with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when combined with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>They train a tensor for each relation of interest to return high scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when combined with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vectors of two entities that hold the intended relation</example>
		<phraseLemma>np when combine with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to classify &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>At test time the system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to classify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relational tuples as true or false as well as to predict new entities that hold a certain relationship with a target entity</example>
		<phraseLemma>np be use to classify np</phraseLemma>
	</can>
	<can>
		<phrase>We treat &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This is quite close in spirit to what we do except that given an entityrelationentity tuple &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we treat relationentity as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a binary attribute of entity and we try to induce such attributes on a larger scale</example>
		<phraseLemma>np we treat np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; annotated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to test our prediction we learn a model of such relationship over a publicly available dataset of feature norms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;annotated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural language quantiﬁers</example>
		<phraseLemma>np annotated with np</phraseLemma>
	</can>
	<can>
		<phrase>As an example consider &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As an example consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence The kouprey is a mammal</example>
		<phraseLemma>as a example consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is supported by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is supported by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the lexical semantics of mammal which applies a property to all instances of a class</example>
		<phraseLemma>np be support by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; ranging from &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Indeed while systems have successfully been developed to model entailment between quantiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;ranging from natural logic approaches to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distributional semantics solutions they rely on an explicit representation of quantiﬁcation</example>
		<phraseLemma>np range from np to np</phraseLemma>
	</can>
	<can>
		<phrase>The relation between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The relation between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distributional and formal semantics has been the object of a number of studies in recent years</example>
		<phraseLemma>the relation between np</phraseLemma>
	</can>
	<can>
		<phrase>The probability of &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The probability of a sentence is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the summed probability of the possible worlds that make it true</example>
		<phraseLemma>the probability of np be np</phraseLemma>
	</can>
	<can>
		<phrase>There is &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There is a large literature on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the computational formalisation of quantiﬁers as automata starting with Van Benthem</example>
		<phraseLemma>there be np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been explored in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The mapping between different semantic modalities or semantic spaces &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been explored in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; various aspects</example>
		<phraseLemma>np have be explore in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In NLP Mikolov show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information</example>
		<phraseLemma>in np show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by relying on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In NLP Mikolov show that a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by relying on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a small amount of bilingual information</example>
		<phraseLemma>np by rely on np</phraseLemma>
	</can>
	<can>
		<phrase>In our work we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our work we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the annotation layer produced by Herbelot and Vecchi for the McRae norms</example>
		<phraseLemma>in we work we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is available for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A subset of the annotation layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is available for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training computational models corresponding to all instances with a majority label</example>
		<phraseLemma>np be available for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponding to &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A subset of the annotation layer is available for training computational models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponding to all instances with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a majority label</example>
		<phraseLemma>np correspond to np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; including &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the following we use a derived gold standard &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;including all 1 quantiﬁed classes in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; QMR with the annotation set to majority opinion instances</example>
		<phraseLemma>np include np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>First we build a cooccurrence based space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a word is represented by cooccurrence counts with content words</example>
		<phraseLemma>np in which np</phraseLemma>
	</can>
	<can>
		<phrase>We select &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We select the top 1 K content words for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the contexts using a bagofwords approach and counting cooccurrences within a sentence</example>
		<phraseLemma>we select np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; reduce &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We then apply positive Pointwise Mutual Information to the raw counts and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;reduce the dimensions to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; through Singular Value Decomposition</example>
		<phraseLemma>np reduce np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which were trained on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use the publicly available vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which were trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Google News dataset of circa billion tokens</example>
		<phraseLemma>np which be train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which relies on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>So it may be helpful to ﬁrst come back to the standard deﬁnition of a model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which relies on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two components</example>
		<phraseLemma>np which rely on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; has &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In model theory the word horse has an extension in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that world which is the set of horses with a cardinality of two</example>
		<phraseLemma>in np have np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; which is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In model theory the word horse has an extension in that world which is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set of horses with a cardinality of two</example>
		<phraseLemma>np in np which be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In model theory the word horse has an extension in that world &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is the set of horses with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a cardinality of two</example>
		<phraseLemma>np which be np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; corresponding to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>That is F takes a distribution w~k and returns a quantiﬁer for each predicate &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the model corresponding to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set overlap between and Note that we focus here on 1 quantik 1</example>
		<phraseLemma>np in np correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are annotated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As both datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are annotated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural language quantiﬁers rather than cardinality ratios we convert the annotation into a numerical format where ALL &amp;gt 1 MOST &amp;gt 1 SOME &amp;gt 1 FEW &amp;gt 1 and NO &amp;gt 1</example>
		<phraseLemma>np be annotated with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; giving &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These values correspond to the weights &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;giving the best interannotator agreement in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Herbelot and Vecchi when calculating weighted Cohens kappa on QMR</example>
		<phraseLemma>np give np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as described in 1</phrase>
		<frequency>10</frequency>
		<example>The McRaebased modeltheoretic space contains concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as described in 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as describe in 1</phraseLemma>
	</can>
	<can>
		<phrase>We learn &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We learn a function from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the distributional space to each of the modeltheoretic spaces</example>
		<phraseLemma>we learn np from np</phraseLemma>
	</can>
	<can>
		<phrase>The result on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The result on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MTanimals test set which includes animals from the AD and QMR datasets shows that this category fares indeed very well at ρ = 1</example>
		<phraseLemma>the result on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this analysis we focused primarily on the comparison &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between transformations using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; various truththeoretic datasets for training and generation</example>
		<phraseLemma>np between np use np</phraseLemma>
	</can>
	<can>
		<phrase>We perform &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We perform a more indepth analysis of the neighbourhoods for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each concept to gain a better understanding of their behaviour and quality</example>
		<phraseLemma>we perform np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is close to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We discover that in many cases the mapped vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is close to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a similar concept in the gold standard but not &amp;gt to itself</example>
		<phraseLemma>np be close to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being seen as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although the concepts share 1 features they also differ quite strongly an axe &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being seen as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a weapon with a blade while the hatchet is itself referred to as an axe</example>
		<phraseLemma>np be see as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; while &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although the concepts share 1 features they also differ quite strongly an axe being seen as a weapon &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a blade while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hatchet is itself referred to as an axe</example>
		<phraseLemma>np with np while np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not appear in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Extensionally of course there is no reason to think that a hatchet does not have a blade or might not be dangerous but those features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not appear in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the norms for the concept</example>
		<phraseLemma>np do not appear in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; close to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This means that the distribution of axe may well be mapped to a region &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;close to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hatchet but thereby ends up separated from the gold axe vector</example>
		<phraseLemma>np close to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; as follows</phrase>
		<frequency>10</frequency>
		<example>Using the distance matrix we give a score to each &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;instance in our test data as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np as follow</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In what follows we report the average performance P of the system as P = sm where sm is the score N assigned to a particular test instance and N &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; test instances</example>
		<phraseLemma>np be the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; producing &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also experimented with generating natural language quantiﬁers from the mapped vectorial representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;producing true quantiﬁed sentences with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a 1 accuracy</example>
		<phraseLemma>np produce np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; give &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Right now many parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;give the same broad analysis to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Mosquitoes are insects and Mosquitoes carry malaria involving an underspeciﬁed/generic quantiﬁer</example>
		<phraseLemma>np give np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on whether &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance we should obtain a different quantiﬁer for taxis are yellow depending &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence starts with In London</example>
		<phraseLemma>np on whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In future work we will test our system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on such contextspeciﬁc examples using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; contextualised vector representations such as the ones proposed by eg Erk and Pado´ and Dinu and Lapata</example>
		<phraseLemma>np on np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In future work we will test our system on such contextspeciﬁc examples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using contextualised vector representations such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ones proposed by eg Erk and Pado´ and Dinu and Lapata</example>
		<phraseLemma>np use np such as np</phraseLemma>
	</can>
	<can>
		<phrase>However because of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However because of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their high complexity graph grammars have not been widely used in NLP</example>
		<phraseLemma>however because of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; improves &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Largescale experiments on Chinese– English and German–English two language pairs that have a high degree of syntactic reordering &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that our method signiﬁcantly improves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation quality over both HPB and Dep 1 Str as measured by BLEU TER and METEOR</example>
		<phraseLemma>np show that np improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as measured by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Largescale experiments on Chinese– English and German–English two language pairs that have a high degree of syntactic reordering show that our method signiﬁcantly improves translation quality over both HPB and Dep 1 Str &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as measured by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLEU TER and METEOR</example>
		<phraseLemma>np as measure by np</phraseLemma>
	</can>
	<can>
		<phrase>We also ﬁnd that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also ﬁnd that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the rules in our model are more suitable for longdistance reordering and translating long sentences</example>
		<phraseLemma>we also ﬁnd that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; limits the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Different to HRG ERG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;limits the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; external nodes to 1 at most to make sure hyperedges do not exist during a derivation</example>
		<phraseLemma>np limit the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as shown in Figure 1</phrase>
		<frequency>10</frequency>
		<example>A dependency graph is directly derived from a dependency tree by labeling edges with words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as shown in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as show in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depends on the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given a dependency graph training and decoding time using DGSG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depends on the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependencygraph fragments</example>
		<phraseLemma>np depend on the number of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we add &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For efﬁcient training and decoding we add&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a restriction to DGSG</example>
		<phraseLemma>for np we add np</phraseLemma>
	</can>
	<can>
		<phrase>We add &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For efﬁcient training and decoding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we add a restriction to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; DGSG</example>
		<phraseLemma>np we add np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we build a dependency graphtostring model so we only use one nonterminal symbol X &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; HPB on the target side</example>
		<phraseLemma>np as in np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We deﬁne the head of a dependencygraph fragment H as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a list of edges the dependency head of each of which is not in this fragment</example>
		<phraseLemma>we deﬁne np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in Equation</phrase>
		<frequency>10</frequency>
		<example>We deﬁne our model in the loglinear framework over a derivation d &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in Equation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as in equation</phraseLemma>
	</can>
	<can>
		<phrase>Then for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Then for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each fragment the decoder ﬁnds rules to translate it</example>
		<phraseLemma>then for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is taken as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>NIST 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is taken as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a development set to tune weights and NIST 1 and NIST 1 are two test sets to evaluate systems</example>
		<phraseLemma>np be take as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in our model</phrase>
		<frequency>10</frequency>
		<example>The Stanford dependency parser parses a Chinese sentence into a projective dependency tree which is then converted to a dependency graph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in we model</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; converts &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then MaltParser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;converts a parse result into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a projective dependency tree</example>
		<phraseLemma>np convert np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; take &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We implement our model in Moses and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;take the same settings as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Moses HPB in all experiments</example>
		<phraseLemma>np take np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improves &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition translation results from a recently opensource dependency treetostring system Dep 1 Str 1 which is implemented in Moses and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improves the dependencybased model in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Xie are also reported</example>
		<phraseLemma>np improve np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In all experiments word alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; GIZA with the heuristic function growdiagﬁnaland</example>
		<phraseLemma>np be perform by np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use SRILM to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a 1 gram language model on the Xinhua portion of the English Gigaword corpus 1 th edition with modiﬁed KneserNey discounting</example>
		<phraseLemma>we use np to train np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to train &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use SRILM to train a 1 gram language model on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Xinhua portion of the English Gigaword corpus 1 th edition with modiﬁed KneserNey discounting</example>
		<phraseLemma>we use np to train np on np</phraseLemma>
	</can>
	<can>
		<phrase>To obtain &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To obtain more reliable results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each experiment we run MERT three times and report average scores</example>
		<phraseLemma>to obtain np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are calculated by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are calculated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three widely used automatic metrics in caseinsensitive mode</example>
		<phraseLemma>np be calculate by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; in terms of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Similar to Li in our experiments Dep 1 Str has on average a comparable result &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with Moses HPB in terms of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLEU and METEOR scores</example>
		<phraseLemma>np with np in term of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is signiﬁcantly better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>By contrast on all test sets measured by all metrics our system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is signiﬁcantly better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Moses HPB</example>
		<phraseLemma>np be signiﬁcantly better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is found in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>When no matched rule &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is found in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the models glue grammars are applied to make sure a translation is produced</example>
		<phraseLemma>np be find in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is greater than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Therefore when the length of a phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is greater than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a certain value glue grammars are also applied</example>
		<phraseLemma>np be greater than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We ﬁnd that on all different values our system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves higher BLEU scores than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Moses HPB</example>
		<phraseLemma>np achieve np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; ie &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We ﬁrst ﬁnd a case of longdistance relation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;ie the subjectverbobject structure in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source sentence</example>
		<phraseLemma>np ie np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as shown in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this example this relation implies a longdistance reordering which moves the translation of the object to the front of its modiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the given reference</example>
		<phraseLemma>np as show in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Dependency structures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SMT for a few years</example>
		<phraseLemma>np have be use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; propose to use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To avoid this problem Xie &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;propose to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; full headdependent structures of a dependency tree and build a new dependencytostring model</example>
		<phraseLemma>np propose to use np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we present &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we present&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dependency graphtostring grammar based on a graph grammar which we call edge replacement grammar</example>
		<phraseLemma>in this paper we present np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>With a restriction of using contiguous edges our translation model built using this grammar can decode an &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;input dependency graph which is directly converted from a dependency tree in cubic time using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CYK algorithm</example>
		<phraseLemma>np in np use np</phraseLemma>
	</can>
	<can>
		<phrase>We present &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present a novel approach for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unsupervised induction of a Reordering Grammar using a modiﬁed form of permutation trees which we apply to preordering in phrasebased machine translation</example>
		<phraseLemma>we present np for np</phraseLemma>
	</can>
	<can>
		<phrase>We report &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report signiﬁcant performance gains over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrase reordering and over two known preordering baselines for EnglishJapanese</example>
		<phraseLemma>we report np over np</phraseLemma>
	</can>
	<can>
		<phrase>We obtain &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We obtain permutations in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data by segmenting every wordaligned sourcetarget pair into minimal phrase pairs the resulting alignment between minimal phrases is written as a permutation 1 and onto on the source side</example>
		<phraseLemma>we obtain np in np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows two alternative PETs for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same permutation over minimal phrases</example>
		<phraseLemma>figure 1 show np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We solve this by a new MinimumBayes Risk decoding approach using Kendall reordering score as loss function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is an efﬁcient measure over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; permutations</example>
		<phraseLemma>np which be np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A novel latent hierarchical source reordering model working over all derivations of PETs A label splitting approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on PCFGs over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; minimal phrases as terminals learned from an ambiguous treebank where the label splits start out from prime permutations</example>
		<phraseLemma>np base on np over np</phraseLemma>
	</can>
	<can>
		<phrase>We report results for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extensive experiments on EnglishJapanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrasebased models outperforming two existing baselines for this task</example>
		<phraseLemma>we report result for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; showing that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>We report results for extensive experiments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on EnglishJapanese showing that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our Reordering PCFG gives substantial improvements when used as preordering for phrasebased models outperforming two existing baselines for this task</example>
		<phraseLemma>np on np show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outperforming &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We report results for extensive experiments on EnglishJapanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrasebased models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outperforming two existing baselines for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task</example>
		<phraseLemma>np outperform np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Here we have access &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a wordaligned parallel corpus not a treebank</example>
		<phraseLemma>np only to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; leads to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The unary trick &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;leads to substantial reduction in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; grammar size eg for arity 1 rules and splits we could have had = 1 splitrules but with the unary trick we only have 1 = 1 split rules</example>
		<phraseLemma>np lead to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The unary trick &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; early lexicalized parsing work 1 This split PCFG constitutes a latent PCFG because the splits cannot be read of a treebank</example>
		<phraseLemma>np be use in np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to parse &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use CKY to parse&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a source sentence s into a forest using the learned split PCFG</example>
		<phraseLemma>we use np to parse np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Hence we opt for minimizing the risk of making an error under a loss function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over permutations using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MBR decision rule</example>
		<phraseLemma>np over np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the sample</phrase>
		<frequency>10</frequency>
		<example>An empirical distribution over permutations P is given by the relative frequency of π &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the sample&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the sample</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is given by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>An empirical distribution over permutations P &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is given by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the relative frequency of π in the sample</example>
		<phraseLemma>np be give by np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used LADER in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard settings without any linguistic features</example>
		<phraseLemma>we use np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was trained on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Baseline C &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences because training is prohibitively slow</example>
		<phraseLemma>np be train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was trained for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The Reordering Grammar &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was trained for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; iterations of EM on train RG data</example>
		<phraseLemma>np be train for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We test how well our model predicts gold reorderings before translation by training the alignment model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using MGIZA 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training corpus and using it to align the test corpus</example>
		<phraseLemma>np use np on np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows that &lt;NP&gt; outperform &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows that our models outperform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all baselines on this task</example>
		<phraseLemma>table 1 show that np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; here is that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The only strange result &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;here is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rulebased preordering obtains a lower score than no preordering which might be an artifact of the Enju parser changing the tokenization of its input so the Kendall τ of this system might not really reﬂect the real quality of the preordering</example>
		<phraseLemma>np here be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is trained on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The reordered output of all the mentioned baselines and versions of our model are translated with phrasebased MT system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; gold preordering of the training data 1 ´s t</example>
		<phraseLemma>np that be train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; more than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We additionally report RIBES score that concentrates on word order &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;more than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other metrics</example>
		<phraseLemma>np more than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>So even though both PETs describe the same reordering RGright captures reordering over English input &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; RGleft</example>
		<phraseLemma>np better than np</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare the phrasebased system with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distance based cost function for reordering with and without preordering</example>
		<phraseLemma>we compare np with np</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; with &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare the phrasebased system with distance based cost function for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reordering with and without preordering</example>
		<phraseLemma>we compare np with np for np</phraseLemma>
	</can>
	<can>
		<phrase>The advantages of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The advantages of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model</example>
		<phraseLemma>the advantage of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The majority of work on preordering &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntactic parse trees eg</example>
		<phraseLemma>np be base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between the two</phrase>
		<frequency>10</frequency>
		<example>However there are major differences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between the two&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np between the two</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are labeled with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A related work with a similar intuition is presented in where nodes of a tree structure similar to PETs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are labeled with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reordering patterns obtained by factorizing word alignments into Hierarchical Alignment Trees</example>
		<phraseLemma>np be label with np</phraseLemma>
	</can>
	<can>
		<phrase>To the best of our knowledge this is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To the best of our knowledge this is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst time both extensions are shown to improve performance</example>
		<phraseLemma>to the best of we knowledge this be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that operate on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We design grammaticality and meaningpreserving syntactic transformation rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that operate on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; constituent parse trees</example>
		<phraseLemma>np that operate on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; closer to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We apply the rules to reference translations to make their word order &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;closer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source language word order</example>
		<phraseLemma>np closer to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; incorporating &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On JapaneseEnglish translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;incorporating the rewritten more monotonic reference translation into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a phrasebased machine translation system enables better translations faster than a baseline system that only uses gold reference translations</example>
		<phraseLemma>np incorporate np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to incorporate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To produce such monotone translations a straightforward approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to incorporate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; interpretation data into the learning of a machine translation system because human interpreters use a variety of strategies to ﬁnetune the word order</example>
		<phraseLemma>np be to incorporate np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we focus on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Japanese to English translation because Japanese and English have signiﬁcantly different word orders and consequently the syntactic constituents required earlier by an English sentence often come late in the corresponding Japanese sentence</example>
		<phraseLemma>in this work we focus on np</phraseLemma>
	</can>
	<can>
		<phrase>In the next section we describe &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the next section we describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the rules in more detail</example>
		<phraseLemma>in the next section we describe np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in more detail</phrase>
		<frequency>10</frequency>
		<example>In the next section we describe the rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in more detail&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in more detail</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produces &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given this constraint a typical simultaneous interpretation system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produces partial translations of consecutive segments in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source sentence and concatenates them to produce a complete translation</example>
		<phraseLemma>np produce np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; is that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The most signiﬁcant difference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between Japanese and English is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the head of a verb phrase comes at the end of Japanese sentences</example>
		<phraseLemma>np between np be that np</phraseLemma>
	</can>
	<can>
		<phrase>In addition to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; quotative verbs candidates typically include factive factivelike belief and antifactive verbs</example>
		<phraseLemma>in addition to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are followed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>When these verbs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are followed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a clause we move the verb and its subject to the end of the clause</example>
		<phraseLemma>np be follow by np</phraseLemma>
	</can>
	<can>
		<phrase>To obtain &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To obtain a list with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reasonable coverage we exploit the fact that Japanese has an unambiguous quotative particle to that precedes such verbs</example>
		<phraseLemma>to obtain np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; for which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Identify nodes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the parse tree for which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the transformation is applicable</example>
		<phraseLemma>np in np for which np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the detection step we ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the subtree that satisﬁes the condition of applying a rule</example>
		<phraseLemma>in np we ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; such that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>In this case we look for an S node whose children include an NP the subject and a VP to its right &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;such that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the VP node has a leaf VB the main verb followed by another NP the object</example>
		<phraseLemma>np such that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; such that &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this case we look for an S node whose children include an NP the subject and a VP to its right &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;such that the VP node has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a leaf VB the main verb followed by another NP the object</example>
		<phraseLemma>np such that np have np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we compare &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the evaluation step we compare&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation delay before and after applying the rule</example>
		<phraseLemma>in np we compare np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is aligned to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 English word love &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is aligned to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the last Japanese word which means the system cannot start to translate until 1 more Japanese words are revealed</example>
		<phraseLemma>np be align to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is trained by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our phrasebased MT system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is trained by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Moses with standard parameters settings</example>
		<phraseLemma>np be train by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is evaluated by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The translation quality &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is evaluated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLEU and RIBES 1 To obtain the parse trees for English sentences we use the Stanford Parser and the included English model</example>
		<phraseLemma>np be evaluate by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; To obtain &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The translation quality is evaluated by BLEU and RIBES 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To obtain the parse trees for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English sentences we use the Stanford Parser and the included English model</example>
		<phraseLemma>np to obtain np for np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate the quality of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate the quality of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our rewritten sentences from two perspectives</example>
		<phraseLemma>we evaluate the quality of np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To examine how close the rewritten sentences are to standard English &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we train a 1 gram language model using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the English data from the Europarl corpus consisting of million words and use it to compute perplexity</example>
		<phraseLemma>np we train np use np</phraseLemma>
	</can>
	<can>
		<phrase>To ensure that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To ensure that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rewrites leave meaning unchanged we use the SEMAFOR semantic role labeler on the original and modiﬁed sentence for each rolelabeled token in the reference sentence we examine its corresponding role in the rewritten sentence and calculate the average accuracy acrosss all sentences</example>
		<phraseLemma>to ensure that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; matches &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As we read in the source Japanese sentence if the input segment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;matches an entry in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the learned phrase table we query the RP of the Japanese/English phrase pair</example>
		<phraseLemma>np match np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is lower than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Following if the RP of the current phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is lower than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a ﬁxed threshold we cache the current phrase and wait for more words from the source sentence otherwise we translate all cached phrases</example>
		<phraseLemma>np be lower than np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we combine &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For RWGD we combine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the translation models of RW and GD by ﬁllup combination where all entries in the tables of RW are preserved and entries from the tables of GD are added if new</example>
		<phraseLemma>for np we combine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; due to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We suspect this is because the transformation rules sometimes generate ungrammatical sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parsing errors which impairs learning</example>
		<phraseLemma>np due to np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; verbs in the translations of the test sentences produced by GD RW RWGD as well as the number in the gold reference translation</example>
		<phraseLemma>table 1 show the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as explained in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One limitation of these methods is that when learning with standard batch MT corpus their gain can be restricted by natural word reordering between the source and the target sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as explained in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np as explain in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; to predict &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In an SOVSVO context methods to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unseen words are proposed to alleviate the above restriction</example>
		<phraseLemma>in np to predict np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to reduce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First while the approaches resemble each other our motivation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to reduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation delay</example>
		<phraseLemma>np be to reduce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; improves &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Training MT systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with more monotonic sentences improves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the speedaccuracy tradeoff for simultaneous machine translation</example>
		<phraseLemma>np with np improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improves &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Training MT systems with more monotonic sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improves the speedaccuracy tradeoff for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; simultaneous machine translation</example>
		<phraseLemma>np improve np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is dominated by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While there exist approaches that extract syntactic tree transformation rules automatically one of the difﬁculties is that most parallel corpora &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is dominated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lexical paraphrasing instead of syntactic paraphrasing</example>
		<phraseLemma>np be dominate by np</phraseLemma>
	</can>
	<can>
		<phrase>This work was supported by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This work was supported by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NSF grant IIS 1</example>
		<phraseLemma>this work be support by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; inherent to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We present a novel application of contextual sentiment with this task and identify several semisupervised learning algorithms that are needed to address the reference resolution challenge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inherent to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; country names</example>
		<phraseLemma>np inherent to np</phraseLemma>
	</can>
	<can>
		<phrase>We present &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present intrinsic evaluations of our learners on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; labeled datasets as well as 1 extrinsic political science evaluations that show strong alignment with our largescale sentiment extraction</example>
		<phraseLemma>we present np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We present intrinsic evaluations of our learners &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on labeled datasets as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 extrinsic political science evaluations that show strong alignment with our largescale sentiment extraction</example>
		<phraseLemma>np on np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This paper hypothesizes that sentiment analysis &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a proxy to track international relations between nation states</example>
		<phraseLemma>np can be use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieve &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We ﬁrst present standard NLP sentiment experiments that show the classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieve good performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; individual tweets</example>
		<phraseLemma>np achieve np on np</phraseLemma>
	</can>
	<can>
		<phrase>This paper focuses on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This paper focuses on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; social media and contextual polarity so we only address the closest work in those areas</example>
		<phraseLemma>this paper focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used &lt;NP&gt; to compute &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>OConnor &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used Twitter data to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a ratio of positive and negative words to measure consumer conﬁdence and presidential approval</example>
		<phraseLemma>np use np to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can serve as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In a similar vein hashtags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can serve as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; noisy labels</example>
		<phraseLemma>np can serve as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to model &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In fact OConnor modeled events to detect international relations but our goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; long term relation trends not isolated events</example>
		<phraseLemma>np be to model np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to identify &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given a tweet containing a countrys name our goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentiment of the text toward that nation</example>
		<phraseLemma>np be to identify np</phraseLemma>
	</can>
	<can>
		<phrase>One of &lt;NP&gt; was that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;One of our early observations was that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mentions of nations are often in the context of eating and dining as evidenced here</example>
		<phraseLemma>one of np be that np</phraseLemma>
	</can>
	<can>
		<phrase>We ran &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ran a logistic regression for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two labels dine and notdine</example>
		<phraseLemma>we run np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; computes &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 counts token occurrences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in this set and computes&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pointwise mutual information scores for each unigram by comparing with the unigram counts over the entire corpus</example>
		<phraseLemma>np in np compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; computes &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 counts token occurrences in this set and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;computes pointwise mutual information scores for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each unigram by comparing with the unigram counts over the entire corpus</example>
		<phraseLemma>np compute np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is classiﬁed as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is classiﬁed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a topic if its average token PMI score is above a learned threshold for that topic</example>
		<phraseLemma>np be classiﬁed as np</phraseLemma>
	</can>
	<can>
		<phrase>We trained &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We trained a logistic regression&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classiﬁer on the relevant tweets in the Section 1 dataset and mapped all other labels to irrelevant</example>
		<phraseLemma>we train np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was determined by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The threshold λT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was determined by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; visual inspection of a held out tweets to maximize accuracy</example>
		<phraseLemma>np be determine by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is replaced with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The target &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is replaced with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a variable to capture generalized patterns</example>
		<phraseLemma>np be replace with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Stanfords CoreNLP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a MaxEnt classiﬁer with its default settings</example>
		<phraseLemma>np be use to train np</phraseLemma>
	</can>
	<can>
		<phrase>It is &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is 1 precise with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 recall on detecting irrelevant tweets</example>
		<phraseLemma>it be np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is computed for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The dataset spans months from out and R &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is computed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all pairs</example>
		<phraseLemma>np be compute for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by assigning &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We created our own ranking &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by assigning a world score to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each nation n the average sentiment ratio of all other nations toward n</example>
		<phraseLemma>np by assign np to np</phraseLemma>
	</can>
	<can>
		<phrase>We also compare against &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also compare against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Frequency Baseline to eliminate the possibility that its simply a matter of topic popularity</example>
		<phraseLemma>we also compare against np</phraseLemma>
	</can>
	<can>
		<phrase>We limit &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We limit the evaluation to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; conﬂicts after to keep relations current</example>
		<phraseLemma>we limit np to np</phraseLemma>
	</can>
	<can>
		<phrase>To evaluate &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To evaluate positive relations we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; current alliances as a fourth evaluation</example>
		<phraseLemma>to evaluate np we use np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To evaluate positive relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use current alliances as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a fourth evaluation</example>
		<phraseLemma>np we use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; of Table 1</phrase>
		<frequency>10</frequency>
		<example>We compare our sentiment ratios to these pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the top of Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np of table 1</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our sentiment ratios to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these pairs in the top of Table 1</example>
		<phraseLemma>we compare np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance the nations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the most positive view of the EU are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Uruguay Lithuania Belarus Moldova and Slovakia</example>
		<phraseLemma>np with np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Over the past few years much work has focussed on inferring political preferences of people from their behavior &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unsupervised and supervised settings</example>
		<phraseLemma>np both in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that exists between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While much work has investigated methods for establishing the truth content of individual sentences — whether from the perspective of veridicality fact assessment or subjectivity analysis — the structure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that exists between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; users and their assertions gives us an opportunity to situate them both in the same political space</example>
		<phraseLemma>np that exist between np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne &lt;NP&gt; to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We deﬁne a proposition to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a tuple comprised of a subject and predicate each consisting of one or more words such as hglobal warming is a hoaxi</example>
		<phraseLemma>we deﬁne np to be np</phraseLemma>
	</can>
	<can>
		<phrase>In order to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extract propositions that are likely to be political in nature and exhibit variability according to ideology we collect data from a politically volatile source</example>
		<phraseLemma>in order to np</phraseLemma>
	</can>
	<can>
		<phrase>In order to extract &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; structured hsubject predicatei propositions from text we ﬁrst parse all comments using the collapsed dependencies of the Stanford parser and identify all subjects as those that hold an nsubj or nsubjpass relation to their head</example>
		<phraseLemma>in order to extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we extract &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to balance the tradeoff between generality and speciﬁcity in the representation of assertions we extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three representations of each predicate</example>
		<phraseLemma>np in np we extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; paired with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This includes all subjects &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;paired with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their heads and all descendants of that head</example>
		<phraseLemma>np paired with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this case a proposition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the nominal subject and its lemmatized head</example>
		<phraseLemma>np be deﬁned as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; was &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For each sentence we paid 1 annotators in the United States to a conﬁrm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that the extracted sentence was&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a wellformed assertion and b to rate the most likely political belief of the person who would say it” on a ﬁvepoint scale</example>
		<phraseLemma>np that np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can provide &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First users latent political preferences while unobserved &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can provide an organizing principle for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inference about propositions in an unsupervised setting</example>
		<phraseLemma>np can provide np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with respect to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the experiments reported below we set the prior distributions on η β and ψ to be standard normals and perform maximum a posteriori inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with respect to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; η β and ψ in turn for a total of iterations</example>
		<phraseLemma>np with respect to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performing &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the experiments reported below we run inference using collapsed Gibbs sampling for iterations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performing hyperparameter optimization&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on α γ and γs every using the ﬁxedpoint method of Minka</example>
		<phraseLemma>np perform np on np</phraseLemma>
	</can>
	<can>
		<phrase>In order to compare &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to compare&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the subjectspeciﬁc predicate distributions across categories we ﬁrst calculate the posterior predictive distribution by taking a single sample of all latent variables z to estimate the following</example>
		<phraseLemma>in order to compare np</phraseLemma>
	</can>
	<can>
		<phrase>We perform &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We perform PCA with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; K = 1 on two representations of our data</example>
		<phraseLemma>we perform np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in our data</phrase>
		<frequency>10</frequency>
		<example>While the input data is sparse we must center each column to have a 1 mean and perform PCA through a singular value decomposition of that columncentered data using the method of Halko in using SVD for PCA the right singular vectors correspond to the principal directions from these we directly read off a K = 1 dimensional score for each proposition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our data&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in we datum</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using the method of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While the input data is sparse we must center each column to have a 1 mean and perform PCA through a singular value decomposition of that columncentered data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the method of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Halko in using SVD for PCA the right singular vectors correspond to the principal directions from these we directly read off a K = 1 dimensional score for each proposition in our data</example>
		<phraseLemma>np use the method of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; give &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While purely supervised models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;give more control over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the exact decision boundary being learned they can suffer by learning from a much smaller training set than unsupervised methods have access to</example>
		<phraseLemma>np give np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the past</phrase>
		<frequency>10</frequency>
		<example>We do not expect these users to be a truly random sample of the population — those who selfdeclare their political afﬁliation are more likely to engage with political content differently from those who do not — but is a method that has been used for political prediction tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the past&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the past</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; of features</phrase>
		<frequency>10</frequency>
		<example>We build a predictive model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using two classes of features&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use np of feature</phraseLemma>
	</can>
	<can>
		<phrase>We build &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We build a predictive model using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two classes of features</example>
		<phraseLemma>we build np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is given as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to establish realvalued scores for propositions we follow the same method as for the single membership model described above using the log likelihood ratio of the probability of the proposition under each condition where that probability &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is given as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the count of the proposition among users classiﬁed as liberals divided by the total number of propositions used by them overall</example>
		<phraseLemma>np be give as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; divided by the total number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to establish realvalued scores for propositions we follow the same method as for the single membership model described above using the log likelihood ratio of the probability of the proposition under each condition where that probability is given as the count of the proposition among users classiﬁed as liberals &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;divided by the total number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; propositions used by them overall</example>
		<phraseLemma>np divide by the total number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Here we train two different logistic regression classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; access to only the unigrams and multiword expressions employed by the user or to binary indicators of the blogs posted to and the identity of the most frequent blog</example>
		<phraseLemma>np each with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; yielded &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In a tenfold crossvalidation cotraining &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;yielded a slightly higher accuracy over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pure supervision 1 ± 1</example>
		<phraseLemma>np yield np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; available to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the experiments that follow we limit the input data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;available to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all models to only those propositions whose subject falls within the evaluation benchmark and include only propositions used by at least ﬁve different users and only users who make at least ﬁve different assertions yielding a total dataset of 1 users and 1 million propositions 1 unique containing the union of all three kinds of extracted propositions from 1</example>
		<phraseLemma>np available to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; like &lt;NP&gt; IS &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each of the automatic methods that we discuss above assigns a realvalued score to propositions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;like OBAMA IS&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; A SOCIALIST</example>
		<phraseLemma>np like np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based only on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For both of the models described in we present results for scoring a proposition like OBAMA IS A SOCIALIST &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based only on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the conditional predicate score</example>
		<phraseLemma>np base only on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since both models are ﬁt &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using approximate inference with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a nonconvex objective function we run ﬁve models with different random initializations and present the average across all ﬁve</example>
		<phraseLemma>np use np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; For &lt;NP&gt; show &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We estimate conﬁdence intervals using the block jackknife calculating purity and Spearmans ρ over resampled subsets of the full elements each leaving out 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For both metrics the two best performing models show&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; statistically signiﬁcant improvement over all other models but are not signiﬁcantly different from each other</example>
		<phraseLemma>np for np show np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We estimate conﬁdence intervals using the block jackknife calculating purity and Spearmans ρ over resampled subsets of the full elements each leaving out 1 For both metrics the two best performing models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show statistically signiﬁcant improvement over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all other models but are not signiﬁcantly different from each other</example>
		<phraseLemma>np show np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are able to exploit &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both unsupervised models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are able to exploit&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the natural structure without being constrained by a small amount of training data that may be more biased than helpful</example>
		<phraseLemma>np be able to exploit np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that exists in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The two generative models also widely outperform PCA which may reﬂect a mismatch between its underlying assumptions and the textual data we observe PCA treats data sparsity as structural zeros and so must model not only the variation that exists between users but also the variation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that exists in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their frequency of use other latent component models may be a better ﬁt for this kind of data</example>
		<phraseLemma>np that exist in np</phraseLemma>
	</can>
	<can>
		<phrase>For all of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For all of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these users we regress binary indicators of the top 1 unigrams in their proﬁles against the MAP estimate of their political afﬁliation in the singlemembership model</example>
		<phraseLemma>for all of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has focused on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We introduce the task of estimating the political import of propositions such as OBAMA IS A SOCIALIST while much work in open information extraction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has focused on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; learning facts such as OBAMA IS PRESIDENT from text we are able to exploit structure in the users and communities who make such assertions in order to align them all within the same political space</example>
		<phraseLemma>np have focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was supported by &lt;NP&gt; 1</phrase>
		<frequency>10</frequency>
		<example>The research reported in this article was largely performed while both authors were at Carnegie Mellon University and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was supported by NSF grant IIS 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be support by np 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; made available by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This work was made possible through the use of computing resources &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;made available by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Open Science Data Cloud an Open Cloud Consortium sponsored project</example>
		<phraseLemma>np make available by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is a combination of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Personality &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is a combination of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; characteristics or qualities that form an individuals distinctive character Personal values reﬂect what are important to different individuals and what motivate them in their decision making</example>
		<phraseLemma>np be a combination of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; due to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Previously the relationship between personal traits and brand preference/purchase decisions has drawn limited &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;interest in marketing research due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the difﬁculty in obtaining consumer traits on a large scale</example>
		<phraseLemma>np in np due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; those of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Among these efforts Westfall found that differences exist between the personalities of the owners of convertible cars and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;those of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard &amp; compact cars</example>
		<phraseLemma>np those of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast automatically derived trait features based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; social media analytics require no user effort and can be applied to millions of social media users</example>
		<phraseLemma>in np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be applied to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In contrast automatically derived trait features based on social media analytics require no user effort and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; millions of social media users</example>
		<phraseLemma>np can be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; so far</phrase>
		<frequency>10</frequency>
		<example>Our study is one of the most comprehensive analyses &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;so far&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np so far</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was to infer &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In sentiment analysis the main focus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was to infer&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentiment associated with a post that mentions a particular brand/product</example>
		<phraseLemma>np be to infer np</phraseLemma>
	</can>
	<can>
		<phrase>In contrast with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our system if we know that he likes to seek excitement and enjoys luxury products we can guess he may like BMW although he has never explicitly mentioned BMW in his social media posts before</example>
		<phraseLemma>in contrast with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are related to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To investigate how personal traits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an individuals brand preferences we collected two datasets</example>
		<phraseLemma>np be related to np</phraseLemma>
	</can>
	<can>
		<phrase>We collected &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally for each user &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we collected her preferences for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; brands in six categories</example>
		<phraseLemma>np we collect np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are listed in Table 1</phrase>
		<frequency>10</frequency>
		<example>All the measures used in our PTBP survey &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are listed in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be list in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To obtain the trait scores for each user &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on his answers in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the PTBP survey we ﬁrst computed the raw trait scores based on the original survey guidelines</example>
		<phraseLemma>np base on np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition each demographics feature such as age education income &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was ﬁrst mapped to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an integer and then normalized into a number between 1 and 1</example>
		<phraseLemma>np be np to np</phraseLemma>
	</can>
	<can>
		<phrase>To derive &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To derive the trait scores for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a user in the TAE survey we crawled all the tweets in his Twitter account</example>
		<phraseLemma>to derive np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is mentioned in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The value θui is an indication of how likely Topic i &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is mentioned in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; user us tweets</example>
		<phraseLemma>np be mention in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shows &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As a summary table 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows all the features from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the TAE survey including those automatically inferred from tweets</example>
		<phraseLemma>np show np from np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we built &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each brand we built&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; threeway classiﬁers using different classiﬁcation algorithms including AdaBoost Decision Tree Logistic Regression Naive Bayes Random Forest and SVM</example>
		<phraseLemma>for np we build np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on both &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Among all the classiﬁers we tested we found that overall Naive Bayes has the best performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the PTBP and the TAE datasets</example>
		<phraseLemma>np on both np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; across &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the following we report the average Fscores and AUC &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;across different brands using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Naive Bayes with fold cross validations</example>
		<phraseLemma>np across np use np</phraseLemma>
	</can>
	<can>
		<phrase>The numbers in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The numbers in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parentheses show the Fscore percentage increase from the baselines</example>
		<phraseLemma>the number in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performed signiﬁcantly better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Overall all the classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performed signiﬁcantly better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baselines</example>
		<phraseLemma>np perform signiﬁcantly better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition comparing the models trained on the PTBP In summary for the task of differentiating people who have positive negative or neutral opinions towards different brands automatically inferred traits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be a good proxy for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the clean data derived from psychometric surveys</example>
		<phraseLemma>np can be np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was conducted using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The feature selection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was conducted using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; logistic regression in SPSS 1</example>
		<phraseLemma>np be conduct use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We aggregate the ranks from all the users and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show the overall brand preference ranks for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both datasets</example>
		<phraseLemma>np show np for np</phraseLemma>
	</can>
	<can>
		<phrase>To predict &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To predict the rank of a product in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each category we trained a multiclass classiﬁer to estimate how 1 Experiment 1 likely a user will like a brand</example>
		<phraseLemma>to predict np in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we computed &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each user and each product category we computed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Spearmans rank correlation coefﬁcient ρ</example>
		<phraseLemma>for np we compute np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used multinomial logistic regression to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most signiﬁcant predicting features for each brand category</example>
		<phraseLemma>we use np to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To further improve the prediction accuracy in the future we will extend our current study by incorporating new features such as the properties of a brand &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; social inﬂuence from people in ones social network</example>
		<phraseLemma>np as well np</phraseLemma>
	</can>
	<can>
		<phrase>By exploiting &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By exploiting temporal information from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Wikipedia edit history and page view logs we have improved the annotation performance by 1 as compared to the competitive baselines</example>
		<phraseLemma>by exploit np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can lead to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>When put together the unprecedentedly massive adoption of a hashtag within a short time period &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bursts and often reﬂect trending social attention</example>
		<phraseLemma>np can lead to np</phraseLemma>
	</can>
	<can>
		<phrase>We propose &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose a novel and efﬁcient learning algorithm based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inﬂuence maximization to automatically annotate hashtags</example>
		<phraseLemma>we propose np base on np</phraseLemma>
	</can>
	<can>
		<phrase>We conduct &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We conduct thorough experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a realworld dataset and show that our system can outperform competitive baselines by 1</example>
		<phraseLemma>we conduct np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Recently Bansal attempt to segment a hashtag and link &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its tokens to a Wikipedia page</example>
		<phraseLemma>np each of np</phraseLemma>
	</can>
	<can>
		<phrase>However most of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However most of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the existing studies focus on the statistical signals of Wikipedia</example>
		<phraseLemma>however most of np</phraseLemma>
	</can>
	<can>
		<phrase>The number of times &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The number of times&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an entitys article has been requested is called the entity view count</example>
		<phraseLemma>the number of time np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we compute &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the 1 step we compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different similarities between each candidate and the hashtag based on different types of contexts which are derived from either side</example>
		<phraseLemma>in np we compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; choose &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally we learn a uniﬁed ranking function for each pair and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;choose the topk entities with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the highest scores</example>
		<phraseLemma>np choose np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is constructed from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our lexicon &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is constructed from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Wikipedia page titles hyperlink anchors redirects and disambiguation pages which are mapped to the corresponding entities</example>
		<phraseLemma>np be construct from np</phraseLemma>
	</can>
	<can>
		<phrase>We extract &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As for the tweet phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we extract all ngrams from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input tweets within T</example>
		<phraseLemma>np we extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; works well for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While the lexiconbased linking &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;works well for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; single tweets applying it on the hashtag level has subtle implications</example>
		<phraseLemma>np work well for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which serve as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To overcome this drawback we incorporate the temporal dynamics of hashtags and entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which serve as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a proxy to the change of user interests towards the underlying topics</example>
		<phraseLemma>np which serve as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; so that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>We further constrain that α β γ = 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;so that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ranking scores of entities are normalized between 1 and 1 and that our learning algorithm is more tractable</example>
		<phraseLemma>np so that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; are computed</phrase>
		<frequency>10</frequency>
		<example>We now discuss in detail how the similarity measures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between hashtags and entities are computed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np between np be compute</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be interpreted as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The similarity of an entity with one individual mention in a tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be interpreted as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the probabilistic prior in mapping the mention to the entity via the lexicon</example>
		<phraseLemma>np can be interpret as np</phraseLemma>
	</can>
	<can>
		<phrase>Based on &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Based on this observation we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Wikipedia revision history – an archive of all revisions of Wikipedia articles – to calculate the entity context</example>
		<phraseLemma>base on np we use np</phraseLemma>
	</can>
	<can>
		<phrase>The distribution of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The distribution of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a word w for the entity e is estimated by a mixture between the probability of generating w from the temporal context and from the general context C of the entity</example>
		<phraseLemma>the distribution of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is computed using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 similarity ft &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is computed using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; temporal signals from both sources – Twitter and Wikipedia</example>
		<phraseLemma>np be compute use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; present in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Based on this assumption we propose to gauge the entity prominence as its potential in maximizing the information spreading within all entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;present in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tweets of the hashtag</example>
		<phraseLemma>np present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that lead to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In other words the problem of ranking the most prominent entities becomes identifying the set of entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the largest number of entities in the candidate set</example>
		<phraseLemma>np that lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; similarly to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can estimate rh efﬁciently using random walk models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;similarly to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline method suggested by Liu</example>
		<phraseLemma>np similarly to np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to calculate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use matrix multiplication to calculate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the similarities efﬁciently</example>
		<phraseLemma>we use np to calculate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are equal to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since both B and fω are normalized such that their column sums &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are equal to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 Equation 1 is convergent</example>
		<phraseLemma>np be equal to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We train the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the same training data as in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original paper</example>
		<phraseLemma>np use np as in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we obtained &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For all baselines we obtained&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the implementation from the authors</example>
		<phraseLemma>for np we obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as compared to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In general all baselines perform worse than reported in the literature conﬁrming the higher complexity of the hashtag annotation task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; traditional tasks</example>
		<phraseLemma>np as compare to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are dependent on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Second language models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are dependent on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; direct word representations which are different between Twitter and Wikipedia</example>
		<phraseLemma>np be dependent on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The explanation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; endogenous hashtags the topical consonance between tweets is very low thus most of the assessments become just verifying general concepts In this case topical annotation is trumped by conceptual annotation</example>
		<phraseLemma>np be that for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which has &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this work we address the new problem of topically annotating a trending hashtag using Wikipedia entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which has many important applications in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; social media analysis</example>
		<phraseLemma>np which have np in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each input set we generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; candidate summaries by combining whole sentences from the summaries generated by different systems</example>
		<phraseLemma>for np we generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; comparable to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also achieve very competitive performance on six DUC/TAC datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;comparable to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart on most datasets</example>
		<phraseLemma>np comparable to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been proposed for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Second even though many systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been proposed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multidocument summarization the output of them are often available only on one dataset or even unavailable</example>
		<phraseLemma>np have be propose for np</phraseLemma>
	</can>
	<can>
		<phrase>Our model achieves &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our model achieves competitive performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; six DUC/TAC datasets which is on par with the stateoftheart on most of these datasets</example>
		<phraseLemma>we model achieve np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This method selects among the outputs of the basic systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on their overlaps with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input in terms of DG</example>
		<phraseLemma>np base on np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compare &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Rather their evaluations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compare the summaries to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input based on the overlap of DG</example>
		<phraseLemma>np compare np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are regarded as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Pei propose a supervised method which handles an issue in Wang and Li that all basic systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are regarded as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; equally important</example>
		<phraseLemma>np be regard as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that assign &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Most importantly only summarizers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that assign importance scores to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each sentence can be used as the input summarizers</example>
		<phraseLemma>np that assign np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; to ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Some work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses integer linear programming to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the exact solution other work employs supervised methods to optimize the ROUGE scores of a summary</example>
		<phraseLemma>np use np to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Among these systems ICSISumm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves the highest ROUGE 1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the TAC 1 1 workshops</example>
		<phraseLemma>np achieve np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in terms of &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the latter task we ﬁlter out the sentences that have no overlap with the query &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of content words for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the systems that we implemented</example>
		<phraseLemma>np in term of np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; divided by the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The sentence importance score is equal to the number of topic words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;divided by the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words in the sentence</example>
		<phraseLemma>np divide by the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that appear in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Based on this fact the combined summary should include sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that appear in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the summaries produced by different systems</example>
		<phraseLemma>np that appear in np</phraseLemma>
	</can>
	<can>
		<phrase>In order to generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other possible summaries one needs to swap the last sentence</example>
		<phraseLemma>in order to generate np</phraseLemma>
	</can>
	<can>
		<phrase>It has been shown that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It has been shown that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unimportant words of an input should not be considered while scoring the summary</example>
		<phraseLemma>it have be show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Features are also set to determine whether t has appeared in the ﬁrst sentence and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; times t appears in the ﬁrst sentences of an input</example>
		<phraseLemma>np the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The importance are used as features for identifying words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human summaries</example>
		<phraseLemma>np that be use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are used as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We present three summary combination methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; baselines</example>
		<phraseLemma>np that be use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The DUC 1 1 and TAC 1 1 datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the heldout test sets</example>
		<phraseLemma>np be use as np</phraseLemma>
	</can>
	<can>
		<phrase>We choose &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We choose ROUGE 1 as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training labels as it outperforms using ROUGE 1 as labels</example>
		<phraseLemma>we choose np as np</phraseLemma>
	</can>
	<can>
		<phrase>In order to ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a better learning method we have experimented with support vector regression 1 and SVMRank 1 SVR has been used for estimating sentence or document importance in summarization</example>
		<phraseLemma>in order to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to ﬁnd a better learning method we have experimented with support vector regression 1 and SVMRank 1 SVR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; estimating sentence or document importance in summarization</example>
		<phraseLemma>np have be use for np</phraseLemma>
	</can>
	<can>
		<phrase>Our experiment shows that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our experiment shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SVR outperforms SVMRank</example>
		<phraseLemma>we experiment show that np</phraseLemma>
	</can>
	<can>
		<phrase>Our model outperforms &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our model outperforms ICSISumm and GreedyKL by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 and 1 respectively</example>
		<phraseLemma>we model outperform np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared to &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also achieve a better performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared to the other top performing extractive systems RegSum on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the DUC 1 data</example>
		<phraseLemma>np compare to np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that achieves &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It also has a slightly lower R 1 and a higher R 1 compared to ClusterCMRW a graphbased system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that achieves the highest R 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the DUC 1 data</example>
		<phraseLemma>np that achieve np on np</phraseLemma>
	</can>
	<can>
		<phrase>In order to have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same size of training data with them we conduct ﬁvefold crossvalidation</example>
		<phraseLemma>in order to have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As shown in Table 1 SumCombine &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SSA and WCS on R 1 and RSU 1 but not on R 1</example>
		<phraseLemma>np perform better than np</phraseLemma>
	</can>
	<can>
		<phrase>It is worth noting that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is worth noting that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these three systems cannot be directly compared because different basic systems are used</example>
		<phraseLemma>it be worth note that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieve &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In fact compared to SumCombine SSA and WCS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieve larger improvements over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the basic systems that are used</example>
		<phraseLemma>np achieve np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in general</phrase>
		<frequency>10</frequency>
		<example>This helps to understand the contribution of different features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in general&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in general</phraseLemma>
	</can>
	<can>
		<phrase>The features derived from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The features derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the basic summaries are also effective even though removing them only lead to a small decrease in performance we can observe the decrease on all ﬁve sets</example>
		<phraseLemma>the feature derive from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to create &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The task of crosslanguage document summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to create&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a summary in a target language from documents in a different source language</example>
		<phraseLemma>np be to create np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; outperform &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Experimental results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that our methods outperform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart extractive systems while maintaining similar grammatical quality</example>
		<phraseLemma>np show that np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to produce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The task of crosslanguage summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to produce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a summary in a target language from documents written in a different source language</example>
		<phraseLemma>np be to produce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be seen as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On the other hand crosslanguage summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be seen as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a special kind of machine translation</example>
		<phraseLemma>np can be see as np</phraseLemma>
	</can>
	<can>
		<phrase>We propose &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Inspired by phrasebased machine translation models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we propose a phrasebased scoring scheme for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; crosslanguage summarization in this work</example>
		<phraseLemma>np we propose np for np</phraseLemma>
	</can>
	<can>
		<phrase>Since &lt;NP&gt; is based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since our framework is based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrases we are not limited to produce extractive summaries</example>
		<phraseLemma>since np be base on np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For experimental evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use the DUC 1 dataset with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; manually translated reference Chinese summaries</example>
		<phraseLemma>np we use np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is inspired by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our design of sentence scoring function for crosslanguage document summarization purpose &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is inspired by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrasebased machine translation models</example>
		<phraseLemma>np be inspire by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in this section</phrase>
		<frequency>10</frequency>
		<example>Summarization Inspired by the general idea of phrasebased machine translation we describe our proposed phrasebased model for crosslanguage summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in this section&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in this section</phraseLemma>
	</can>
	<can>
		<phrase>In the context of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the context of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; crosslanguage summarization here we assume that we can also have phrases in both source and target languages along with phrase alignments between the two sides</example>
		<phraseLemma>in the context of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; along with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the context of crosslanguage summarization here we assume that we can also have phrases in both source and target languages along with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrase alignments between the two sides</example>
		<phraseLemma>np in np along with np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We deﬁne our scoring function for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each sentence s as</example>
		<phraseLemma>we deﬁne np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be derived from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>All the above terms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bilingual sentence pairs with phrase alignments</example>
		<phraseLemma>np can be derive from np</phraseLemma>
	</can>
	<can>
		<phrase>Assuming that we have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Assuming that we have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a predeﬁned budget B to restrict the total length of a generated summary</example>
		<phraseLemma>assume that we have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is listed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The greedy algorithm we will use for our compressive summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is listed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1</example>
		<phraseLemma>np be list in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tries to ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In each iteration the algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tries to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the compressed sentence with maximum gaincost ratio and merge it to the summary set at the current iteration</example>
		<phraseLemma>np try to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The target &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the compression with maximum gaincost ratio</example>
		<phraseLemma>np be to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is very similar to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The outline of this algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is very similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the greedy algorithm used by Morita for subtree extraction except that in our context the increase of cost function when adding a sentence is exactly the cost of that sentence</example>
		<phraseLemma>np be very similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; available for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In our phrasebased scoring for sentences although there exist no apparent optimal substructure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;available for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; exact dynamic programming due to nonlocal distortion penalty we can have a tractable approximate procedure since the search space is only deﬁned by local decisions on whether a phrase should be kept or dropped</example>
		<phraseLemma>np available for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is displayed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our compression process for each sentence s &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is displayed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1</example>
		<phraseLemma>np be display in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; gives &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Empirically we ﬁnd this procedure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;gives almost the same results with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; exhaustive search while maintaining efﬁciency</example>
		<phraseLemma>np give np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; requires &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Therefore the whole framework &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;requires O time for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a document cluster containing N sentences in total to generate a summary with k sentences</example>
		<phraseLemma>np require np for np</phraseLemma>
	</can>
	<can>
		<phrase>We will refer to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We will refer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this dataset as the DUC 1 dataset in this paper</example>
		<phraseLemma>we will refer to np</phraseLemma>
	</can>
	<can>
		<phrase>We will refer to &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We will refer to this dataset as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the DUC 1 dataset in this paper</example>
		<phraseLemma>we will refer to np as np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are English document sets in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the DUC 1 dataset for multidocument summarization</example>
		<phraseLemma>there be np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are provided by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Three generic reference English summaries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NIST annotators for each document set</example>
		<phraseLemma>np be provide by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is similar to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The scoring function is designed to be document frequencies of English bigrams &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 term in our proposed sentence scoring function in Section 1 and is submodular</example>
		<phraseLemma>np which be similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as described by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The compression process follows from an integer linear program &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as described by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Clarke and Lapata</example>
		<phraseLemma>np as describe by np</phraseLemma>
	</can>
	<can>
		<phrase>Similar to &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Similar to traditional summarization tasks we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ROUGE metrics for automatic evaluation of all systems in comparison</example>
		<phraseLemma>similar to np we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same random sample of document sets from the DUC 1 dataset</example>
		<phraseLemma>np be perform on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; except for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also conduct pairwise ttest and ﬁnd that almost all the differences between PBCS and other systems are statistically signiﬁcant with p 1 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;except for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ROUGEW metric</example>
		<phraseLemma>np except for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is displayed in Table 1</phrase>
		<frequency>10</frequency>
		<example>For manual evaluation the average score and standard deviation for each metric &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is displayed in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be display in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be treated as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This suggest that bigrams &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be treated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; good indicators of local grammatical smoothness</example>
		<phraseLemma>np can be treat as np</phraseLemma>
	</can>
	<can>
		<phrase>In contrast in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; summarization over the years since the introduction of ROUGE summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics</example>
		<phraseLemma>in contrast in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; away from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Firstly to what degree was the divergence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;away from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; evaluation methodologies still applied to MT metrics today wellfounded?</example>
		<phraseLemma>np away from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieve &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since distinct variants of ROUGE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieve signiﬁcantly stronger correlation with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human assessment than previous recommended best variants we subsequently replicate a recent evaluation of stateoftheart summarization systems revealing distinct conclusions about the relative performance of systems</example>
		<phraseLemma>np achieve np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; take into account &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Despite unit order quite likely being something of importance to a human assessor assessment of metrics by correlation with human coverage scores does not in any respect &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;take into account&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the order in which the units of a summary appear and evaluation by human coverage scores alone means that a summary with its units scrambled or even reversed in theory receives precisely the same metric score as the original</example>
		<phraseLemma>np take into account np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; higher than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Unfortunately the evaluation of metrics with respect to how well they distinguish between highquality human summaries and all systemgenerated summaries does not provide insight into the task of metrics to score better quality systemgenerated summaries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;higher than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; worse quality systemgenerated summaries however</example>
		<phraseLemma>np higher than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; make &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Separate from the evaluation of metrics Rankel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;make the highly important recommendation of paired tests for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; identiﬁcation of signiﬁcant differences in performance of summarization systems</example>
		<phraseLemma>np make np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are computed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Human coverage scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are computed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; combining Matching PUs with coverage estimates as follows</example>
		<phraseLemma>np be compute by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are converted to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Figure 1 is a scatterplot of human coverage scores and corresponding linguistic quality scores for all humanassessed summaries from DUC 1 where for the purpose of comparison each of the 1 linguistic quality ratings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are converted to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a corresponding percentage quality</example>
		<phraseLemma>np be convert to np</phraseLemma>
	</can>
	<can>
		<phrase>For the purpose of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the purpose of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; metric evaluation we combine human coverage and linguistic quality scores using the average of the two scores and use this as a gold standard human score for evaluation of metrics</example>
		<phraseLemma>for the purpose of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; including &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>ROUGE includes a large number of distinct variants &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;including eight choices of ngram counting method binary settings such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; wordstemming of summaries and an option to remove or retain stopwords</example>
		<phraseLemma>np include np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; include the use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Additional conﬁgurations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;include the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision recall or fscore to compute individual summary scores</example>
		<phraseLemma>np include the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are comprised of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The fact that ﬁnal overall ROUGE scores for systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are comprised of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the mean or median of ROUGE scores of individual summaries is again a divergence from MT evaluation as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel</example>
		<phraseLemma>np be comprise of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as opposed to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The fact that ﬁnal overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries is again a divergence from MT evaluation as ngram counts used to compute BLEU scores are computed at the document &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as opposed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentencelevel</example>
		<phraseLemma>np as oppose to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is restricted to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However in this respect ROUGE has a distinct advantage over BLEU as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of signiﬁcance testing differences in systemlevel ROUGE scores while BLEU &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is restricted to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the application of randomized methods</example>
		<phraseLemma>np be restricted to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was used to compute &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moses multibleu &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was used to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLEU scores for summaries and prepare 1 rouge 1 applied to summaries before running ROUGE</example>
		<phraseLemma>np be use to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since the power of Williams test increases when the 1 correlation r &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between metric scores is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stronger metrics should not be ranked by the number of competing metrics they outperform as a metric that happens to correlate strongly with a higher number of competing metrics in a given competition would be at an unfair advantage</example>
		<phraseLemma>np between np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although BLEU achieves strongest correlation with human assessment overall Figure 1 reveals the difference between BLEUs correlation with human assessment and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the bestperforming ROUGE variant as not statistically signiﬁcant and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of significance testing differences in scores for systems for this reason alone we recommend the use of the bestperforming ROUGE variant over BLEU average ROUGE 1 precision with stemming and stopwords removed</example>
		<phraseLemma>np that of np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate systems using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the variant of ROUGE that achieves strongest correlation with human assessment average ROUGE 1 precision with stemming and stopwords removed</example>
		<phraseLemma>we evaluate np use np</phraseLemma>
	</can>
	<can>
		<phrase>In order to evaluate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to evaluate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; metrics by correlation with human assessment it is necessary to obtain a single human evaluation score per system</example>
		<phraseLemma>in order to evaluate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was provided with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>An analysis of evaluation of summarization metrics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was provided with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an evaluation of BLEU and variants of ROUGE</example>
		<phraseLemma>np be provide with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is generated in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While it is natural and tempting to view the linked web page as the source text from which the tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is generated in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an extractive summarization setting it is unclear to what extent actual indicative tweets behave like extractive summaries</example>
		<phraseLemma>np be generate in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is sensitive to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our results demonstrate the limits of viewing indicative tweet generation as extractive summarization and point to the need for the development of a methodology for tweet generation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is sensitive to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; genrespeciﬁc issues</example>
		<phraseLemma>np that be sensitive to np</phraseLemma>
	</can>
	<can>
		<phrase>While there has been &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While there has been&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recent progress in the development of Twitterspeciﬁc POS taggers parsers and other tweet understanding tools there has been little work on methods for generating tweets despite the utility this would have for users and organizations</example>
		<phraseLemma>while there have be np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we study &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we study&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the generation of the particular class of tweets that contain a link to an external web page that is composed primarily of text</example>
		<phraseLemma>in this paper we study np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in our data set</phrase>
		<frequency>10</frequency>
		<example>This class of tweets which we call indicative tweets represents a large subset of tweets overall constituting more than half of the tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our data set&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in we datum set</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; behind &lt;NP&gt; was &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One of the original motivations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;behind extractive summarization was&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the observation that human summary writers tended to extract snippets of key phrases from the source text</example>
		<phraseLemma>np behind np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>One of the original motivations behind extractive summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was the observation that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human summary writers tended to extract snippets of key phrases from the source text</example>
		<phraseLemma>np be np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be viewed as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We begin to address the above issues through a study that examines to what extent tweet generation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be viewed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an extractive summarization problem</example>
		<phraseLemma>np can be view as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be found in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We used this data and applied unigram bigram and LCS matching techniques inspired by ROUGE to determine what proportion of tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be found in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the linked article</example>
		<phraseLemma>np can be find in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Ghosh classiﬁed the retweeting activity of users &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on time intervals between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; retweets of a single user and frequency of retweets from unique users</example>
		<phraseLemma>np base on np between np</phraseLemma>
	</can>
	<can>
		<phrase>Were used to generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Summarization systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were used to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences lesser than characters in length by summarizing documents which could then be taken to be tweets</example>
		<phraseLemma>np be use to generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Cheung and Penn &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the news genre extractive summarization systems that are optimized for centrality—that is getting the core parts of the text into the summary— cannot perform well when compared to model summaries since the model summaries are abstracted from the document to a large extent</example>
		<phraseLemma>np show that for np</phraseLemma>
	</can>
	<can>
		<phrase>However none of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However none of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the datasets in those studies focused on tweets and related articles linked to these tweets</example>
		<phraseLemma>however none of np</phraseLemma>
	</can>
	<can>
		<phrase>The dataset of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The dataset of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Lloret and Palomar is an exception as it contains tweets and the news articles they link to but it only contains English tweetarticle pairs</example>
		<phraseLemma>the dataset of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contained &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We extracted about 1 tweets of which more than half or around 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contained URLs to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an external news article photo on photo sharing sites or video</example>
		<phraseLemma>np contain np to np</phraseLemma>
	</can>
	<can>
		<phrase>The data from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The data from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tweets was cleaned by removing the tweets that were not in English as well as the retweets ie republications of a tweet by a different user</example>
		<phraseLemma>the datum from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that were not in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The data from the tweets was cleaned by removing the tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that were not in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English as well as the retweets ie republications of a tweet by a different user</example>
		<phraseLemma>np that be not in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was used to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The newspaper package &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was used to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extract article text and the title from the web page</example>
		<phraseLemma>np be use to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This includes the URL itself and the text extracted from the article &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as some extracted information such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence boundaries POS tags for tokens parse trees and dependency trees</example>
		<phraseLemma>np as well as np such as np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows an example of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows an example of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an entry in the dataset</example>
		<phraseLemma>table 1 show a example of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we performed &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the unigram analysis we performed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a variant of the analysis in which we computed the overlap within threesentence windows in the source article</example>
		<phraseLemma>for np we perform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is extracted from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These results give an indication of the degree to which the tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document text</example>
		<phraseLemma>np be extract from np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; out of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In 1 cases out of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these the tweet text matched the title of the article which our preprocessing tool did not correctly separate from the body of the article</example>
		<phraseLemma>in np out of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; appears in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the other cases the text of the tweet appears in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its entirety inside the body of the article</example>
		<phraseLemma>in np appear in np</phraseLemma>
	</can>
	<can>
		<phrase>The mean of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The mean of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this distribution shows that the number of matched unigrams from a tweet in the article are fairly low</example>
		<phraseLemma>the mean of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was converted into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The text of the tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was converted into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bigrams and we then looked for those bigrams in the article text</example>
		<phraseLemma>np be convert into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Again the ﬁnal result for the article &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was the window in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the maximum percentage was recorded among all windows</example>
		<phraseLemma>np be np in which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was correlated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This formality score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was correlated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the longest common subsequence measure that we deﬁned above</example>
		<phraseLemma>np be correlate with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by training &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>They calculate formality scores for words and sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by training a model on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a large corpus based on the appearance of words in speciﬁc documents</example>
		<phraseLemma>np by training np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>They calculate formality scores for words and sentences by training a model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a large corpus based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the appearance of words in speciﬁc documents</example>
		<phraseLemma>np on np base on np</phraseLemma>
	</can>
	<can>
		<phrase>We would like to thank the anonymous reviewers for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We would like to thank the anonymous reviewers for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their comments and suggestions and Julian Brooke for the formality lexicon used in a part of this study</example>
		<phraseLemma>we would like to thank the anonymous reviewer for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; obtaining &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>By applying features from a convolutional neural network we obtain stateoftheart performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a standard dataset obtaining&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a 1 relative improvement over previous work which uses bags of visual words based on SIFT features</example>
		<phraseLemma>np on np obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that rely on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Consequently monolingual approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; comparable instead of parallel corpora have been developed</example>
		<phraseLemma>np that rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by using &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It has recently been shown however that much better performance can be achieved on semantic similarity and relatedness tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by using visual representations from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deep convolutional neural networks instead of BOVW features</example>
		<phraseLemma>np by use np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To obtain a translation of a word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a source language we ﬁnd the nearest neighbours from words in the target language where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words in both languages reside in a shared visual space made up of CNNbased features</example>
		<phraseLemma>np in np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; made up of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To obtain a translation of a word in a source language we ﬁnd the nearest neighbours from words in the target language where words in both languages reside in a shared visual space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;made up of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CNNbased features</example>
		<phraseLemma>np make up of np</phraseLemma>
	</can>
	<can>
		<phrase>Finally we show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Finally we show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the visual approach outperforms the linguistic approaches on one of the three language pairs on a standard dataset</example>
		<phraseLemma>finally we show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; typically rely on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However these models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;typically rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the availability of bilingual seed lexicons to produce shared bilingual spaces as well as large repositories of comparable data</example>
		<phraseLemma>np typically rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has led to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although the idea of transferring CNN features is not new the simultaneous availability of massive amounts of data and cheap GPUs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has led to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; considerable advances in computer vision similar in scale to those witnessed with SIFT and HOG descriptors a decade ago</example>
		<phraseLemma>np have lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is motivated by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Multimodal semantics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is motivated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parallels with human concept acquisition</example>
		<phraseLemma>np be motivate by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relies heavily on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It has been found that semantic knowledge from a very early age &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relies heavily on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; perceptual information and there exists substantial evidence that many concepts are grounded in the perceptual system</example>
		<phraseLemma>np rely heavily on np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to extract &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use Google Images to extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top n ranked images for each lexical item in the evaluation datasets</example>
		<phraseLemma>we use np to extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; yield &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It has been shown that images from Google &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;yield higher quality representations than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; comparable sources such as Flickr and that Googlederived datasets are competitive with hand prepared datasets”</example>
		<phraseLemma>np yield np than np</phraseLemma>
	</can>
	<can>
		<phrase>See Figure 1 for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;See Figure 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a simple diagram illustrating the approach</example>
		<phraseLemma>see figure 1 for np</phraseLemma>
	</can>
	<can>
		<phrase>Each of &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Each of the two words has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; n images associated with it – the top n as returned by Google image search using bicycle and ﬁets as separate query terms</example>
		<phraseLemma>each of np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that use &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To avoid confusion we will refer to the CNNbased models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that use these metrics as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CNNAVGMAX and CNNMAXMAX</example>
		<phraseLemma>np that use np as np</phraseLemma>
	</can>
	<can>
		<phrase>In order to get &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to get&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁve hundred lexical items they ﬁrst rank nouns by the conditional probability of them occurring in the pattern of ” in the webscale Google Ngram corpus and take the top ﬁve hundred words as their English lexicon</example>
		<phraseLemma>in order to get np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was created in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given the way that the BERGSMA 1 dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was created in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; particular the use of the pattern described above it contains largely concrete linguistic concepts</example>
		<phraseLemma>np be create in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which combines &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We report their bestperforming visualonly system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which combines SIFTbased descriptors with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; color histograms as well as their bestperforming overall system which combines the visual approach with normalized edit distance</example>
		<phraseLemma>np which combine np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We report their bestperforming visualonly system which combines SIFTbased descriptors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with color histograms as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their bestperforming overall system which combines the visual approach with normalized edit distance</example>
		<phraseLemma>np with np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>It is clear that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is clear that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the perimage similarity metrics perform better on genuine similarity as measured by SimLex 1 than on relatedness as measured by MEN</example>
		<phraseLemma>it be clear that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; perform better on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It is clear that the perimage similarity metrics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;perform better on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; genuine similarity as measured by SimLex 1 than on relatedness as measured by MEN</example>
		<phraseLemma>np perform better on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; works best for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In fact the aggressive” CNNMAXMAX method which picks out a single pair of images to represent a linguistic pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;works best for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SimLex 1 indicating how stringently it focuses on genuine similarity</example>
		<phraseLemma>np work best for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; on the other</phrase>
		<frequency>10</frequency>
		<example>This sheds light &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a question raised by Kiela and Bottou where they speculate that certain errors are a result of whether their visual similarity metric measures genuine similarity on the one hand or relatedness on the other&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np on the other</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; obtains &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This model which we call BOOTSTRAP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;obtains the current best results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the evaluation dataset</example>
		<phraseLemma>np obtain np on np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows the results for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the language pairs in the VULIC 1 dataset</example>
		<phraseLemma>table 1 show the result for np</phraseLemma>
	</can>
	<can>
		<phrase>This can be explained by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This can be explained by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the observation that Vulic´ and Moenss NLEN training data for the BOOTSTRAP model is less abundant 1 times fewer Wikipedia articles and of lower quality than the data for their ESEN and ITEN models</example>
		<phraseLemma>this can be explain by np</phraseLemma>
	</can>
	<can>
		<phrase>In light of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In light of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our ﬁndings concerning the difference between genuine similarity and relatedness this also gives rise to the question of whether the additional layers might be useful for similarity or relatedness or both</example>
		<phraseLemma>in light of np</phraseLemma>
	</can>
	<can>
		<phrase>We combined &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We combined CNN layers with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each other by concatenating the normalized layers</example>
		<phraseLemma>we combine np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shown in Table 1</phrase>
		<frequency>10</frequency>
		<example>The average image dispersions for the two datasets broken down by language &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shown in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be show in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that has &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Using an image resource like Google Images &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that has full coverage for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; almost any word means that we can retrieve what we might call associated” images as opposed to extensional” images</example>
		<phraseLemma>np that have np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; outperforming &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This explains why we still obtain good performance on the more abstract VULIC 1 dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in some cases outperforming&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; linguistic methods</example>
		<phraseLemma>np in np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>We thank &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We thank Marco Baroni for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; useful feedback and the anonymous reviewers for their helpful comments</example>
		<phraseLemma>we thank np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we use the Recursive Autoencoder architecture to develop a Cross Lingual Sentiment Analysis tool &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using sentence aligned corpora between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a pair of resource rich and resource poor language</example>
		<phraseLemma>np use np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is based on &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is based on the assumption that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic similarity between different phrases also implies sentiment similarity in majority of sentences</example>
		<phraseLemma>np be base on np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; especially when &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It is shown that our approach significantly outperforms state of the art systems for Sentiment Analysis &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;especially when&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; labeled data is scarce</example>
		<phraseLemma>np especially when np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; but &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However in recent years there has been emergence of increasing amount of text in Hindi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on electronic sources but&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NLP Frameworks to process this data is sadly miniscule</example>
		<phraseLemma>np on np but np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First We develop a new Rating scale &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based Movie Review Dataset for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Hindi</example>
		<phraseLemma>np base np for np</phraseLemma>
	</can>
	<can>
		<phrase>In recent years there have been &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In recent years there have been&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; emergence of works on Sentiment Analysis for Hindi</example>
		<phraseLemma>in recent year there have be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieving &lt;NP&gt; of 1</phrase>
		<frequency>10</frequency>
		<example>provided a comparative analysis of Unigram based Inlanguage MT based Cross Lingual and WordNet based Sentiment classifier &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieving highest accuracy of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np achieve np of 1</phraseLemma>
	</can>
	<can>
		<phrase>Were then used to train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were then used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a linear SVM to predict positive or negative polarity on a tourism review dataset</example>
		<phraseLemma>np be then use to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; behind &lt;NP&gt; is that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The core idea &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;behind BRAE is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a phrase and its correct translation should share the same semantic meaning</example>
		<phraseLemma>np behind np be that np</phraseLemma>
	</can>
	<can>
		<phrase>In this model &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each word wk in the vocabulary V of given language corresponds to a vector xk 1 Rn and stacked into a single word embedding matrix L 1 Rn jV j</example>
		<phraseLemma>in this model np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learned using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This matrix &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learned using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; DNN and serves as input to further stages of RAE</example>
		<phraseLemma>np be learn use np</phraseLemma>
	</can>
	<can>
		<phrase>We show &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show a node in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; modified architecture in Fig</example>
		<phraseLemma>we show np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; then &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To compute this gradient we 1 greedily construct all trees and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;then derivatives for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these trees are computed efficiently via backpropagation through structure</example>
		<phraseLemma>np then np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; available at &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In Supervised Phase I we used IMDB 1 dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;available at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; http</example>
		<phraseLemma>np available at np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to obtain &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used MOSES Toolkit to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; high quality bilingual phrase pairs from HindEnCorp to train our BRAE model</example>
		<phraseLemma>we use np to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we sample &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each size we sample&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the data times with replacement and trained the model</example>
		<phraseLemma>for np we sample np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we calculated &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each sample we calculated&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fold cross validation accuracy as described above</example>
		<phraseLemma>for np we calculate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was selected as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Parameter λp &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was selected as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 λS as 1 and λT as 1 after selection in range 1 1 in steps of 1</example>
		<phraseLemma>np be select as np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we show &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Fig 1 we show&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the variation in accuracy of the classifiers with amount of sentiment labeled Training data used</example>
		<phraseLemma>in np we show np</phraseLemma>
	</can>
	<can>
		<phrase>We also observed that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also observed that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model which restricts modification to transformation weights during supervised phase II does better than the one which allows the modification at all dataset sizes</example>
		<phraseLemma>we also observe that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also observed that the model which restricts modification to transformation weights during supervised phase II &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the one which allows the modification at all dataset sizes</example>
		<phraseLemma>np do better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; making &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Analyzing these aspects and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;making correct predictions on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such examples needs further work</example>
		<phraseLemma>np make np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learns &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To achieve this our model 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learns phrase embeddings for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two languages using Standard RAE then fine tune these embeddings using Cross Training procedure</example>
		<phraseLemma>np learn np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To achieve this our model 1 learns phrase embeddings for two languages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using Standard RAE then fine tune these embeddings using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Cross Training procedure</example>
		<phraseLemma>np use np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Due to the explosive growth of data ﬁne grained sentiment analysis &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as summarization&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on the whole chunk of data can be a very timeconsuming task</example>
		<phraseLemma>np as well as np on np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; movie reviews for opinion summarization task as they often have the following parts</example>
		<phraseLemma>in this paper we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; generate &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In contrast conventional twostage approach Pang and Lee which ﬁrst &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;generate candidate subjective sentences using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mincut and then selects top subjective sentences within budget to generate a summary have less computational complexity than joint models</example>
		<phraseLemma>np generate np use np</phraseLemma>
	</can>
	<can>
		<phrase>Automatically generating &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Automatically generating opinion summaries from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large review text corpora has long been studied in both information retrieval and natural language processing</example>
		<phraseLemma>automatically generate np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; either in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since many authors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;either in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; summarization or opinion summarization have used functions similar to submodular functions as objective we can take this fact as testament to the value of submodular functions for opinion summarization</example>
		<phraseLemma>np either in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are added to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Monotonicity As more sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are added to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; opinion summary subjectivity increases along with information content as opinionated words are being added</example>
		<phraseLemma>np be add to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the effect of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Diminishing Return If multiple sentences of varying intensity are added to opinion summary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the effect of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a lower intensity polarity bearing sentence is diluted in the presence of a higher intensity one</example>
		<phraseLemma>np the effect of np</phraseLemma>
	</can>
	<can>
		<phrase>To show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; opinion summarization inherently follow the diminishing return property consider the following sentences with positive polarity</example>
		<phraseLemma>to show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to select &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The task of extractive opinion summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to select&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a subset S ∈ V to represent the entirety</example>
		<phraseLemma>np be to select np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is calculated using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The subjective score sj &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is calculated using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiwordnet as sum of the positive score ∈ and negative score ∈</example>
		<phraseLemma>np be calculate use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;intuition behind the combination of sentiment and aspect coverage in same function A is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; opinion polarity holds submodular property of diminishing return only if the set of sentences talk about common aspect of the same entity as discussed in previous section</example>
		<phraseLemma>np in np be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as discussed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The intuition behind the combination of sentiment and aspect coverage in same function A is that opinion polarity holds submodular property of diminishing return only if the set of sentences talk about common aspect of the same entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as discussed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; previous section</example>
		<phraseLemma>np as discuss in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is modeled by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If each aspect chooses the sentences with the highest value the total value provided to all aspects &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is modeled by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this set function</example>
		<phraseLemma>np be model by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These summaries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used as gold standard for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; estimating ROUGE scores of system generated summaries</example>
		<phraseLemma>np be use as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the experiment the partial enumeration based greedy algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; summary generation of test documents within budget of words</example>
		<phraseLemma>np be use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponding to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the algorithm the sentences are clustered in different partitions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponding to different aspects in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ontology tree using the clue words</example>
		<phraseLemma>np correspond to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is set as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The linear combination parameter β &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is set as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 α to bring out the tradeoff between relevance and subjective coverage of aspects and α is varied from 1 to 1 with step size 1 to ﬁnd optimal α</example>
		<phraseLemma>np be set as np</phraseLemma>
	</can>
	<can>
		<phrase>The measure of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The measure of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiment preservation is calculated as Pearson correlation between the sentiment score of the document and the corresponding summary sentiment both calculated by the Naive Bayes sentiment classifier while the measure of coverage of information content is given by ROUGE 1 and ROUGE 1 fscores</example>
		<phraseLemma>the measure of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is calculated as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The measure of sentiment preservation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is calculated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Pearson correlation between the sentiment score of the document and the corresponding summary sentiment both calculated by the Naive Bayes sentiment classifier while the measure of coverage of information content is given by ROUGE 1 and ROUGE 1 fscores</example>
		<phraseLemma>np be calculate as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are proportional to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Further this optimal tradeoff between relevance and subjectivity can be used to design an evaluation framework for opinion summarization task as both part of the objective functions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are proportional to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ROUGE and Sentiment Correlation respectively which are widely used evaluation measures</example>
		<phraseLemma>np be proportional to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; annotated in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>There are several sentiment expressions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;annotated in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MPQA</example>
		<phraseLemma>np annotated in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is negative toward &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Imam &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is negative toward&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Salman Rushdie and the insulting event as revealed by the expression issued the fatwa against</example>
		<phraseLemma>np be negative toward np</phraseLemma>
	</can>
	<can>
		<phrase>Previous work in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Previous work in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiment analysis mainly focuses on detecting explicit opinions</example>
		<phraseLemma>previous work in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is critical for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Joint prediction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is critical for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our task because it involves multiple mutually constraining ambiguities</example>
		<phraseLemma>np be critical for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is evaluated against &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The target in a spanbased system is evaluated by measuring the overlapping proportion of an extracted span against the gold standard phrase while the eTarget in an entity/eventlevel system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is evaluated against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the exact word in the gold standard</example>
		<phraseLemma>np be evaluate against np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are limited to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>That work assumes that the source is only the writer and the targets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are limited to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entities that participate in /effect events</example>
		<phraseLemma>np be limit to np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we introduce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we introduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the deﬁnition of the entity/eventlevel sentiment analysis task followed by a description of the gold standard corpus</example>
		<phraseLemma>in this section we introduce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; since it is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It is called an eTarget &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;since it is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an entity or an event</example>
		<phraseLemma>np since it be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A PSL model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of atoms to be grounded and a set of weighted ifthen rules expressed in ﬁrstorder logic</example>
		<phraseLemma>np be deﬁned use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is associated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each rule &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a weight indicating the importance of this rule in the whole rule set</example>
		<phraseLemma>np be associate with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is created with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus for an opinion y if the source s is assigned by a ground atom SOURCE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is created with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; score 1</example>
		<phraseLemma>np be create with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is created for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For each opinion y a ground atom ETARGET &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is created for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each eTarget candidate t ET 1 considers all the nouns and verbs in the sentence to provide a full recall of eTargets</example>
		<phraseLemma>np be create for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; considers &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For each opinion y a ground atom ETARGET is created for each eTarget candidate t ET 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;considers all the nouns and verbs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence to provide a full recall of eTargets</example>
		<phraseLemma>np consider np in np</phraseLemma>
	</can>
	<can>
		<phrase>In addition for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the eTarget candidate set extracted by ET 1 or ET 1 we run the Stanford coreference system to expand the set in two ways</example>
		<phraseLemma>in addition for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are deﬁned in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally to support the new inferences more groundings of ETARGET &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are deﬁned in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PSL 1</example>
		<phraseLemma>np be deﬁned in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in total</phrase>
		<frequency>10</frequency>
		<example>Currently there are documents 1 sentences and 1 DS and ESEs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in total&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in total</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as described in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Though the PSL inference does not need supervision and the SVM classiﬁer for agents and themes in Section 1 is trained on a separate corpus we still have to train the eTarget SVM classiﬁer to assign local scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np as describe in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is carried out on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The trained classiﬁer is then run on the test set and PSL inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is carried out on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the test set</example>
		<phraseLemma>np be carry out on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; regards &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since each noun and verb may be an eTarget the ﬁrst baseline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;regards all the nouns and verbs as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; eTargets</example>
		<phraseLemma>np regard np as np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that our sentiment analysis system has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the capability through inference to recognize positive and negative pairs even if corresponding opinion expressions are not extracted</example>
		<phraseLemma>note that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even if &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Note that our sentiment analysis system has the capability through inference to recognize positive and negative pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; corresponding opinion expressions are not extracted</example>
		<phraseLemma>np even if np</phraseLemma>
	</can>
	<can>
		<phrase>According to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;According to the gold standard in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 each opinion has a set of eTargets</example>
		<phraseLemma>accord to np in np</phraseLemma>
	</can>
	<can>
		<phrase>But not &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;But not&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all eTargets are equally important</example>
		<phraseLemma>but not np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; captures &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;introduced in Section 1 a spanbased target annotation of an opinion in MPQA 1 captures&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most important target this opinion is expressed toward</example>
		<phraseLemma>np in np capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Correctly recognizing all the eTargets is difﬁcult but all the PSL models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline</example>
		<phraseLemma>np be better than np</phraseLemma>
	</can>
	<can>
		<phrase>As shown in Table 1 &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As shown in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the low accuracy of baseline All NP/VP shows that entity/eventlevel sentiment analysis is a difﬁcult task</example>
		<phraseLemma>as show in table 1 np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shows that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As shown in Table 1 the low accuracy of baseline All NP/VP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows that entity/eventlevel sentiment analysis&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; is a difﬁcult task</example>
		<phraseLemma>np show that np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is a form of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A simile &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is a form of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁgurative language that compares two essentially unlike things such as Jane swims like a dolphin”</example>
		<phraseLemma>np be a form of np</phraseLemma>
	</can>
	<can>
		<phrase>Our work is also related to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our work is also related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiment analysis in general</example>
		<phraseLemma>we work be also relate to np</phraseLemma>
	</can>
	<can>
		<phrase>We removed &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We removed tweets with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; exact duplicate content and tweets containing a retweet token</example>
		<phraseLemma>we remove np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; containing &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We removed tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with exact duplicate content and tweets containing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a retweet token</example>
		<phraseLemma>np with np contain np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to identify &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used the UIUC Chunker to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrase sequences with the syntax of similes</example>
		<phraseLemma>we use np to identify np</phraseLemma>
	</can>
	<can>
		<phrase>We kept &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We kept the entire noun phrase for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vehicle since vehicles like ice box” and gift box” may represent two different concepts with different polarities in similes</example>
		<phraseLemma>we keep np for np</phraseLemma>
	</can>
	<can>
		<phrase>We replaced &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We replaced personal pronouns with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a general PERSON token and other pronouns with a general IT token</example>
		<phraseLemma>we replace np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is created using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our ﬁrst training data set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is created using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the AFINN sentiment lexicon containing 1 manually labeled words with integer values ranging from to 1</example>
		<phraseLemma>np be create use np</phraseLemma>
	</can>
	<can>
		<phrase>We applied &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We applied the CMU partofspeech tagger for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets to match the MPQA partsofspeech for each word</example>
		<phraseLemma>we apply np for np</phraseLemma>
	</can>
	<can>
		<phrase>We assign &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We assign positive/negative polarity to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similes with more positive/negative lexicon words</example>
		<phraseLemma>we assign np to np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a Java implementation of SVM from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LIBLINEAR with the original parameter values used by the NRC Canada system</example>
		<phraseLemma>we use np from np</phraseLemma>
	</can>
	<can>
		<phrase>We obtain &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We obtain up to two levels of hypernym classes for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each simile component head using WordNet</example>
		<phraseLemma>we obtain np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are obtained for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Once the hypernym classes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are obtained for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a word we no longer keep the level information and use a binary feature to represent each hypernym class</example>
		<phraseLemma>np be obtain for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; to represent &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Once the hypernym classes are obtained for a word we no longer keep the level information and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use a binary feature to represent&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each hypernym class</example>
		<phraseLemma>np use np to represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our intuition is that groups of similar words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different similes with the same affective polarity</example>
		<phraseLemma>np can be use in np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to represent &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use 1 binary features to represent&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the label that the NRCCanada Sentiment Classiﬁer assigns to a simile</example>
		<phraseLemma>we use np to represent np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train two binary classiﬁers one for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive and one for negative polarity</example>
		<phraseLemma>we train np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; labeled &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For positive polarity we use similes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;labeled positive as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive training instances and similes labeled negative or neutral as the negative training instances</example>
		<phraseLemma>np label np as np</phraseLemma>
	</can>
	<can>
		<phrase>We also used &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also used the connotation lexicon the same way as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MPQA sentiment lexicon to compare as an additional baseline</example>
		<phraseLemma>we also use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which indicates that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The Neutral class has extremely low precision &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which indicates that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many unrecognized positive and negative similes are being classiﬁed as Neutral</example>
		<phraseLemma>np which indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; yields &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally Row shows that adding the sentiment features along with all the other features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;yields a precision gain for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive polarity and a recall gain for negative polarity</example>
		<phraseLemma>np yield np for np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁers when they are trained with automatically acquired training instances</example>
		<phraseLemma>table 1 show the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; provide &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We observe a substantial recall gain which validates our hypothesis that similes obtained by recognizing sentiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in their surrounding words provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁer with a more diverse set of training examples</example>
		<phraseLemma>np in np provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provide &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We observe a substantial recall gain which validates our hypothesis that similes obtained by recognizing sentiment in their surrounding words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provide the classiﬁer with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a more diverse set of training examples</example>
		<phraseLemma>np provide np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the size of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also generated learning curves to determine how much &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the size of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training set matters</example>
		<phraseLemma>np the size of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Figures 1 and 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classiﬁers trained using varying amounts of manually annotated data</example>
		<phraseLemma>np show the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These ﬁgures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that the classiﬁers with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unigram features hit a plateau at about training instances</example>
		<phraseLemma>np show that np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; appeared</phrase>
		<frequency>10</frequency>
		<example>For this analysis we looked at the simile component triples as well as the context of ten tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which the simile appeared&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in which np appear</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows examples of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows examples of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some similes that we identiﬁed as being ﬁgurative literal or both depending on context</example>
		<phraseLemma>table 1 show example of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; was 1</phrase>
		<frequency>10</frequency>
		<example>Even &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with Master Turkers a qualiﬁcation task ﬁltering annotators by gold standard items and collapsing scalar 1 values to literal and 1 values to ﬁgurative the interannotator agreement with Fleiss κ was 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np be 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; while &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Similarly a baby is often positive as a vehicle but smelling and sounding like a baby may not be positive depending &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the circumstances while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the positive or negative interpretation of sounding like a pirate and looking like a pirate might also be context dependent</example>
		<phraseLemma>np on np while np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we focus on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a new problem of event coreference resolution across television news videos</example>
		<phraseLemma>in this paper we focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; lead to &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These properties &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;lead to needs for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; automatic methods to cluster information and remove redundancy</example>
		<phraseLemma>np lead to np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tend to use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moreover diverse anchors reporters and TV channels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tend to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similar or identical video contents to describe the same story even though they usually use different words and phrases</example>
		<phraseLemma>np tend to use np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we adopt &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we adopt&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart techniques and that train robust convolutional neural networks over millions of web images to detect 1 semantic categories deﬁned in ImageNet from each image</example>
		<phraseLemma>in this work we adopt np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be considered as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 nd to the last layer features from such deep network &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be considered as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; highlevel visual representation that can be used to discriminate various semantic classes</example>
		<phraseLemma>np can be consider as np</phraseLemma>
	</can>
	<can>
		<phrase>To compute &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To compute the similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; videos associated with two candidate event mentions we sample multiple frames from each video and aggregate the similarity scores of the few most similar image pairs between the videos</example>
		<phraseLemma>to compute np between np</phraseLemma>
	</can>
	<can>
		<phrase>In order to detect &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to detect&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; anchor frames automatically a face detector is applied to all Iframes of a video</example>
		<phraseLemma>in order to detect np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is applied to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to detect anchor frames automatically a face detector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all Iframes of a video</example>
		<phraseLemma>np be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is conducted on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A 1 fold crossvalidation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is conducted on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training set and the average fscore is 1</example>
		<phraseLemma>np be conduct on np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we take &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we take&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a step back to document this moment in time making a record of the major available corpora that are driving the ﬁeld</example>
		<phraseLemma>in this paper we take np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; The quality of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Vision Datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The quality of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dataset is highly dependent on the sampling and scraping techniques used early in the data collection process</example>
		<phraseLemma>np the quality of np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we propose &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of such metrics that characterize vision &amp; language datasets</example>
		<phraseLemma>in this section we propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is aligned with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>De´ja Images Dataset consists of 1 K unique usergenerated captions associated with 1 M Flickr images where one caption &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is aligned with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple images</example>
		<phraseLemma>np be align with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In total this dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains photos of basic object types with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 million labeled instances in 1 k images each paired with 1 captions</example>
		<phraseLemma>np contain np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; gave rise to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;gave rise to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CVPR 1 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research</example>
		<phraseLemma>np give rise to np</phraseLemma>
	</can>
	<can>
		<phrase>To get &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To get a rough estimate of the reporting bias in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; image captioning we determined the percentage of toplevel objects that are mentioned in the captions for this dataset out of all the objects that are annotated</example>
		<phraseLemma>to get np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has demonstrated that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Recent work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has demonstrated that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ngram language modeling paired with scenelevel understanding of an image trained on large enough datasets can result in reasonable automatically generated captions</example>
		<phraseLemma>np have demonstrate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can result in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Recent work has demonstrated that ngram language modeling paired with scenelevel understanding of an image trained on large enough datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can result in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reasonable automatically generated captions</example>
		<phraseLemma>np can result in np</phraseLemma>
	</can>
	<can>
		<phrase>However &lt;NP&gt; suffers from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However the CQA dataset suffers from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a high perplexity against a background corpus relative to the other datasets at odds with relatively short sentence lengths</example>
		<phraseLemma>however np suffer from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is in line with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In contrast the COCO and Flickr 1 K dataset’s relatively high syntactic complexity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is in line with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their relatively high sentence length</example>
		<phraseLemma>np be in line with np</phraseLemma>
	</can>
	<can>
		<phrase>As shown in Figure 1 &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As shown in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the COCO dataset is balanced across POS tags most similarly to the balanced Brown corpus</example>
		<phraseLemma>as show in figure 1 np</phraseLemma>
	</can>
	<can>
		<phrase>We explore &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We explore the prediction of prepositions for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a pair of entities both in the case when the labels of such entities are known and unknown</example>
		<phraseLemma>we explore np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is important in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is important in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that such detailed annotations are more informative and discriminative compared to isolated textual labels and are essential for improved text and image retrieval</example>
		<phraseLemma>np be important in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are essential for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The task is important in that such detailed annotations are more informative and discriminative compared to isolated textual labels and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are essential for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; improved text and image retrieval</example>
		<phraseLemma>np be essential for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; map &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On the other hand work that does consider the image content for generating prepositions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;map geometric relations to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a limited set of prepositions using manually deﬁned rules not as humans would naturally use them with a richer vocabulary</example>
		<phraseLemma>np map np to np</phraseLemma>
	</can>
	<can>
		<phrase>Our hypothesis is that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our hypothesis is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the combination of geometric textual and visual features can help with the task of predicting the most appropriate preposition since incorporating geometric and visual information should help generate a relation that is consistent with the image content whilst incorporating textual information should help generate a description that is consistent with natural language</example>
		<phraseLemma>we hypothesis be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is consistent with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our hypothesis is that the combination of geometric textual and visual features can help with the task of predicting the most appropriate preposition since incorporating geometric and visual information should help generate a relation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the image content whilst incorporating textual information should help generate a description &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural language</example>
		<phraseLemma>np that be consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>We base &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We base the preposition&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prediction task on two largescale image datasets with human authored descriptions namely MSCOCO and Flickr 1 k</example>
		<phraseLemma>we base np on np</phraseLemma>
	</can>
	<can>
		<phrase>To extract &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To extract instances of triples from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; image descriptions we used the Neural Network transitionbased dependency parser of Chen and Manning as implemented in Stanford CoreNLP</example>
		<phraseLemma>to extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are available in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Details on how the triples were extracted from captions and matched to instances in images &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are available in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the supplementary material</example>
		<phraseLemma>np be available in np</phraseLemma>
	</can>
	<can>
		<phrase>We chose to use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We chose to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; continuous features as we felt these may be more powerful and expressive compared to discrete binned features</example>
		<phraseLemma>we choose to use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; during training</phrase>
		<frequency>10</frequency>
		<example>This allows information to be transferred across semantically related terms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;during training&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np during training</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we extracted &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For this scenario we extracted&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; visual representations from the ﬁnal layer of a Convolutional Neural Network trained on ImageNet and used them as representations for entity instances in place of textual features</example>
		<phraseLemma>for np we extract np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we extracted &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For this scenario we extracted visual representations from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁnal layer of a Convolutional Neural Network trained on ImageNet and used them as representations for entity instances in place of textual features</example>
		<phraseLemma>for np we extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used them as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For this scenario we extracted visual representations from the ﬁnal layer of a Convolutional Neural Network trained on ImageNet and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used them as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; representations for entity instances in place of textual features</example>
		<phraseLemma>np use they as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; providing &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This simulates the scenario of a vision detector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;providing a ﬁrm decision&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on the concept label for the detected entities</example>
		<phraseLemma>np provide np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus we also evaluated our models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a balanced subset where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each preposition is limited to a maximum of random test samples</example>
		<phraseLemma>np on np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The training samples are weighted according to their class frequency &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nonbiased classiﬁers to predict this balanced test set</example>
		<phraseLemma>np in order to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In general geometric features perform better than the baseline and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when combined with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text features further improve the results</example>
		<phraseLemma>np when np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; plays &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We found that the landmark &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;plays a larger role in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; preposition prediction as omitting the trajector produces 1 1 better results than omitting the landmark</example>
		<phraseLemma>np play np in np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows the results of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the structured model used to predict the most likely combination</example>
		<phraseLemma>table 1 show the result of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; adding &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Future work could include nonprepositional terms like verbs having prepositions modify verbs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;adding word 1 vec embeddings to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the structured prediction model and providing stronger features – whether textual visual or geometric</example>
		<phraseLemma>np add np to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the current pipeline these wordbased models are the seeds for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more sophisticated models which need alignment tableaus to start their optimization procedure</example>
		<phraseLemma>in np be np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is given in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given these deﬁnitions the IBM Model 1 optimization problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is given in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fig 1 and for example</example>
		<phraseLemma>np be give in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the same for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In particular although the objective cost &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the same for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any optimal solution the translation quality of the solutions is not ﬁxed and will still depend on the initialization of the model</example>
		<phraseLemma>np be the same for np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we used &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For our alignment experiments we used a subset of the Canadian Hansards bilingual corpus with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 EnglishFrench sentence pairs as training data 1 sentences of development data and sentences of test data</example>
		<phraseLemma>for np we use np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not lead to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moreover we note that dice &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; quality β exponents and that unfortunately combining methods as in column 1 = does not necessarily lead to additive gains in AER and FMeasure performance</example>
		<phraseLemma>np do not lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that allows for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We looked at a speciﬁc member within the studied optimization family &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that allows for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an easy EM algorithm</example>
		<phraseLemma>np that allow for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; provide &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We then replicate the 1 score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the TOEFL test and provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; additional stateoftheart scores for the BLESS test</example>
		<phraseLemma>np on np provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; represents &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In other words each cell fij in the matrix representation F represents&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a cooccurrence count between a word wi and a context cj</example>
		<phraseLemma>np in np represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represents &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In other words each cell fij in the matrix representation F &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represents a cooccurrence count between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a word wi and a context cj</example>
		<phraseLemma>np represent np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Bullinaria and Levy further corroborate Carons result and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that the optimum exponent parameter p for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; DSMs is with strong statistical signiﬁcance p &amp;lt 1</example>
		<phraseLemma>np show that np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was split into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was split into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different sizes to test the statistical signiﬁcance of the weight redistribution effect</example>
		<phraseLemma>np be split into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are based on &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The cosine distributions Θi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are based on the best matches for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each category i and normalized by the total mean and variance amongst all categories Θ i = Θiµ</example>
		<phraseLemma>np be base on np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contain &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The results in Figure 1 indicate that the top PCs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contain a higher level of cohyponymy rela tions than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the lower removing the top PCs gives a violin shape that resembles the inverse of the plot for the top PCs</example>
		<phraseLemma>np contain np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is for &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the PC removal the optimal number of removed PCs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is for TOEFL 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLESS and for SimLex 1 while the optimal number for the Caron ptransform is 1 for TOEFL 1 for BLESS and 1 for SimLex 1</example>
		<phraseLemma>np be for np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the PC removal the optimal number of removed PCs is for TOEFL 1 for BLESS and for SimLex 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while the optimal number for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Caron ptransform is 1 for TOEFL 1 for BLESS and 1 for SimLex 1</example>
		<phraseLemma>np while np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is known as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This convention &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is known as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Pareto principle and generally gives a good tradeoff between compression and precision</example>
		<phraseLemma>np be know as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used in the experiments</phrase>
		<frequency>10</frequency>
		<example>The method signiﬁcantly outperforms the baseline Skipgram model on all tests &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used in the experiments&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use in the experiment</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we explore &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we explore&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a POS tagging application of neural architectures that can infer word representations from the raw character stream</example>
		<phraseLemma>in this paper we explore np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is considered as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In these models the observed word form is considered as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the elementary unit while its morphological properties remain neglected</example>
		<phraseLemma>in np be consider as np</phraseLemma>
	</can>
	<can>
		<phrase>As a result &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a result&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vocabulary observed on training data heavily restricts the generalization power of lexicalized models</example>
		<phraseLemma>as a result np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This allows a better processing of morphologically rich languages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a combinatorial explosion of word forms most of which are not observed during training</example>
		<phraseLemma>np in which there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; combines &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Using a maximum after a sliding convolution window ensures that the embedding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;combines local features from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the whole word and selects the more useful ones</example>
		<phraseLemma>np combine np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To compute the probability of tagging the nth word in the sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with tag ti we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a window of dw word embeddings centered around the word w followed by a hidden and output layers</example>
		<phraseLemma>np with np we use np</phraseLemma>
	</can>
	<can>
		<phrase>To infer &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To infer the tag sequence from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sequence of output layers deﬁned by equations 1 or 1 we explore two strategies</example>
		<phraseLemma>to infer np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; directly from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Here the character level allows the model to ignore the definition a priori of a vocabulary and let the model build its own representation of a sentence or a document &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;directly from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the character level</example>
		<phraseLemma>np directly from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; to model &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>also descends lower than the word level &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using a dictionary of morphemes and recursive neural networks to model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the structure of the words</example>
		<phraseLemma>np use np to model np</phraseLemma>
	</can>
	<can>
		<phrase>We consider &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We consider the two tagging tasks with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁrst a coarse tagset tags and then a morphosyntactical rich tagset items observed on the the training set</example>
		<phraseLemma>we consider np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outperforms &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally the model that uses the concatenation of both the character and wordlevel embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outperforms the stateoftheart system on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the more difﬁcult task without any feature engineering</example>
		<phraseLemma>np outperform np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Surprisingly this architecture performs poorly with simple inference but clearly improves when predicting a structured output using the Viterbi algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training and testing</example>
		<phraseLemma>np both for np</phraseLemma>
	</can>
	<can>
		<phrase>Our results showed that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our results showed that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; characterlevel encoding can address the unknown words problem for morphologically complex languages</example>
		<phraseLemma>we result show that np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance preliminary experiments show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bidirectional recurrent network can achieve very competitive and promising results</example>
		<phraseLemma>for np show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; inherent in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One of the main advantages of these methods is the ability to learn smooth vector representations for words thereby reducing the sparsity problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inherent in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any natural language dataset</example>
		<phraseLemma>np inherent in np</phraseLemma>
	</can>
	<can>
		<phrase>We refer to this as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We refer to this as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a version of online learning as gradient descent is used to optimise the vector even during testing</example>
		<phraseLemma>we refer to this as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to achieve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our results indicate that by exchanging some existing model parameters for a component using online learning the system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to achieve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lower perplexity while also reducing the necessary computation</example>
		<phraseLemma>np be able to achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shown in Figure 1</phrase>
		<frequency>10</frequency>
		<example>We base our implementation of the RNNLM on Mikolov &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shown in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np show in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is trained using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The network &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is trained using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; gradient descent and backpropagation through time</example>
		<phraseLemma>np be train use np</phraseLemma>
	</can>
	<can>
		<phrase>In addition we introduce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition we introduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a special vector to use as the hidden vector at the start of each sentence</example>
		<phraseLemma>in addition we introduce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are treated as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The values in this vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are treated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parameters and optimised during training</example>
		<phraseLemma>np be treat as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>During testing the values in the document vector are continuously modiﬁed depending on the error derivatives being backpropagated from the output layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while all other parameters in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model stay constant</example>
		<phraseLemma>np while np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; include &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>They construct a feedforward language model and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;include a paragraph vector as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an additional vector in the input layer</example>
		<phraseLemma>np include np as np</phraseLemma>
	</can>
	<can>
		<phrase>They use &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They use these vectors as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input to a logistic regression classiﬁer and achieve stateoftheart performance on sentiment classiﬁcation of movie reviews</example>
		<phraseLemma>they use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; directly on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However they did not consider the effect of this model modiﬁcation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;directly on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of language modelling</example>
		<phraseLemma>np directly on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; directly to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Therefore we attach the document vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;directly to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the output layer in parallel with the recurrent hidden component</example>
		<phraseLemma>np directly to np</phraseLemma>
	</can>
	<can>
		<phrase>We constructed &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We constructed a dataset from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English Wikipedia to evaluate language modelling performance over individual sentences</example>
		<phraseLemma>we construct np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is measured using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Model performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is measured using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; perplexity therefore lower values indicate a model which is able to better predict the data</example>
		<phraseLemma>np be measure use np</phraseLemma>
	</can>
	<can>
		<phrase>When &lt;NP&gt; is added to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When the same number of elements is added to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; M or D our results show consistently better performance when using the document vector</example>
		<phraseLemma>when np be add to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; leads to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Table 1 contains the additional values for the experiments showing that replacing some hidden vector parameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the actively learned document vector leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fewer total parameters and fewer operations along with lower perplexity</example>
		<phraseLemma>np with np lead to np</phraseLemma>
	</can>
	<can>
		<phrase>The graph of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The graph of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; perplexity with respect to additional operations in the model also has a very similar shape</example>
		<phraseLemma>the graph of np</phraseLemma>
	</can>
	<can>
		<phrase>The idea of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The idea of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; continuous training and adaptation is natural and also established in biological learning processes yet it is not widely used due to computational complexity</example>
		<phraseLemma>the idea of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with and without &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>After mapping the utterance into a vector space the model exploits the structure of the output labels by mapping each label to a hyperplane that separates utterances &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with and without&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that label</example>
		<phraseLemma>np with and without np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which were not in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both these mappings are initialised with unsupervised word embeddings so they can be computed even for words or concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which were not in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SLU training data</example>
		<phraseLemma>np which be not in np</phraseLemma>
	</can>
	<can>
		<phrase>For this we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For this we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an English Wikipedia dump as our unlabelled training corpus which is a diverse broadcoverage corpus</example>
		<phraseLemma>for this we use np</phraseLemma>
	</can>
	<can>
		<phrase>We split &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For our ﬁrst experiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we split each dataset into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; about 1 for the testing set and 1 for the training set</example>
		<phraseLemma>np we split np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One natural approach to this task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one binary classiﬁer for each possible label to decide whether or not to include it in the output</example>
		<phraseLemma>np be to train np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we build &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our alternative approach we build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the representation of the utterance and the representation of the label from their constituent words then we check if these representations match or not</example>
		<phraseLemma>in np we build np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; as well</phrase>
		<frequency>10</frequency>
		<example>Following the success &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in transfer learning from parsing to understanding tasks we use dependency parse bigrams in our features as well&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np as well</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; by using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We learn to build a local representation at each word position &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the utterance by using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the word representation adjacent word representations and the head word representation</example>
		<phraseLemma>np in np by use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been shown to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Large margin classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been shown to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an effective tool for this task</example>
		<phraseLemma>np have be show to be np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to learn &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the same idea of binary classiﬁers to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one hyperplane per label which separates the utterances with this label from all other utterances with a large margin</example>
		<phraseLemma>we use np to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is independent of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the standard way of building the classiﬁer each labels hyperplane &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is independent of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other labels</example>
		<phraseLemma>np be independent of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by assuming that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>We exploit the structure of labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by assuming that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each hyperplane representation is a composition of representations of the labels constituent components namely dialogue action attribute and attribute value</example>
		<phraseLemma>np by assume that np</phraseLemma>
	</can>
	<can>
		<phrase>The score of &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The score of each local representation vector is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its distance from this label hyperplane which is computed as the dot product of the local vector φ with the normal vector W</example>
		<phraseLemma>the score of np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; less than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To train a large margin classiﬁer we train all the parameters such that the score of an utterance is bigger than a margin for its labels and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;less than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the negative margin for all other labels</example>
		<phraseLemma>np less than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We report as baselines &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Phoenix system and a binary linear SVM trained on the same data</example>
		<phraseLemma>np the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we describe &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a new SLU model that is designed for improved domain adaptation</example>
		<phraseLemma>in this paper we describe np</phraseLemma>
	</can>
	<can>
		<phrase>We tested &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We tested this SLU model on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; datasets where the number of attribute types and values is increased and show much better results than the baselines especially in recall</example>
		<phraseLemma>we test np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; especially in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We tested this SLU model on datasets where the number of attribute types and values is increased and show much better results than the baselines &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;especially in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recall</example>
		<phraseLemma>np especially in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a form of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we address the latter case framed as modeling continuous interarrival times under a logGaussian Cox process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a form of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inhomogeneous Poisson process which captures the varying rate at which the tweets arrive over time</example>
		<phraseLemma>np a form of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we address the latter case framed as modeling continuous interarrival times under a logGaussian Cox process a form of inhomogeneous Poisson process which captures the varying rate &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tweets arrive over time</example>
		<phraseLemma>np at which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which models &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We propose to address interarrival time prediction problem with logGaussian Cox process an inhomogeneous Poisson process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which models&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets to be generated by an underlying intensity function which varies across time</example>
		<phraseLemma>np which model np</phraseLemma>
	</can>
	<can>
		<phrase>Even though &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Even though the central application is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rumours one could apply the proposed approaches to model the arrival times of tweets corresponding to other types of memes eg discussions about politics</example>
		<phraseLemma>even though np be np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we observe &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our setting we observe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; posts over a target rumour i for one hour and over reference rumours for two hours</example>
		<phraseLemma>in np we observe np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In IPP the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets y occurring in an interval is Poisson Re distributed with rate λdt</example>
		<phraseLemma>in np the number of np</phraseLemma>
	</can>
	<can>
		<phrase>We associate &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We associate a distinct intensity function λ = exp with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each rumour Ei as they have varying temporal proﬁles</example>
		<phraseLemma>we associate np with np</phraseLemma>
	</can>
	<can>
		<phrase>The likelihood of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The likelihood of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; posts in the rumour data is obtained by taking the product of the likelihoods over individual rumours</example>
		<phraseLemma>the likelihood of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to obtain &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The distribution of the posterior p is intractable and a Laplace approximation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the posterior</example>
		<phraseLemma>np be use to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is obtained using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The predictive distribution fi at time ti &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is obtained using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the approximated posterior</example>
		<phraseLemma>np be obtain use np</phraseLemma>
	</can>
	<can>
		<phrase>We set &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We set the rate parameter of this exponential distribution to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 which generates points with a mean value around 1</example>
		<phraseLemma>we set np to np</phraseLemma>
	</can>
	<can>
		<phrase>We introduce &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We introduce two metrics based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; root mean squared error for evaluating predicted interarrival times</example>
		<phraseLemma>we introduce np base on np</phraseLemma>
	</can>
	<can>
		<phrase>We consider &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We consider a homogeneous Poisson process as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline which results in exponentially distributed interarrival times with rate λ</example>
		<phraseLemma>we consider np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which results in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We consider a homogeneous Poisson process as a baseline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; exponentially distributed interarrival times with rate λ</example>
		<phraseLemma>np which result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is set to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The rate parameter &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is set to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the maximum likelihood estimate the reciprocal of the mean of the interarrival times in the training data</example>
		<phraseLemma>np be set to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is modeled as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 baseline is a GP with a linear kernel where the interarrival time &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is modeled as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a function of time of occurrence of last tweet</example>
		<phraseLemma>np be model as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We consider this baseline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the singletask setting where reference rumours are not considered</example>
		<phraseLemma>np only in np</phraseLemma>
	</can>
	<can>
		<phrase>In the case of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the case of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LGCP the model parameters of the intensity function associated with a rumour are learnt from the observed interarrival times from that rumour alone</example>
		<phraseLemma>in the case of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; predicts &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However LGCP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;predicts a large number of arrivals in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this interval making it a bad model compared to LGCPTXT which predicts only 1 points</example>
		<phraseLemma>np predict np in np</phraseLemma>
	</can>
	<can>
		<phrase>According to &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;According to PRMSE LGCPTXT is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most successful method signiﬁcantly outperforming all other according to Wilcoxon signed rank test</example>
		<phraseLemma>accord to np be np</phraseLemma>
	</can>
	<can>
		<phrase>We also note that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also note that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; GPLIN performs very poorly according to PRMSE</example>
		<phraseLemma>we also note that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when compared to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On a large scale language modeling task this architecture achieves a 1 x speedup at runtime and a signiﬁcant reduction in perplexity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a standard multilayer network</example>
		<phraseLemma>np when compare to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; made &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>To mitigate this Devlin &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;made the observation that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; for 1 layer NNLMs the dot product between each embeddingposition pair and the ﬁrst hidden layer can be precomputed after training is complete which allows the matrixvector multiplication to be replaced by a handful of vector additions</example>
		<phraseLemma>np make np that np</phraseLemma>
	</can>
	<can>
		<phrase>This results in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This results in 1 dimensional vectors for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each word that can be stored in a lookup table</example>
		<phraseLemma>this result in np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is identical to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Note that this is not an approximation and the resulting output vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is identical to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original matrixvector product</example>
		<phraseLemma>np be identical to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are combined using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The output of these lateral layers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are combined using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an inexpensive elementwise function and fed into the output layer</example>
		<phraseLemma>np be combine use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be thought of as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Mathematically this &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be thought of as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a generalization of maxout networks where different elementwise combination functions are explored rather than just the max function</example>
		<phraseLemma>np can be think of as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are mapped into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In a standard feedforward embeddingbased neural network the input tokens &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are mapped into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a continuous vector using an embedding table and this embedding vector is fed into the ﬁrst hidden layer</example>
		<phraseLemma>np be map into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is fed into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In a standard feedforward embeddingbased neural network the input tokens are mapped into a continuous vector using an embedding table and this embedding vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is fed into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst hidden layer</example>
		<phraseLemma>np be feed into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that takes &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Where C is a combination function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that takes two or more kdimensional vectors as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inputs and produces as kdimensional vector as output</example>
		<phraseLemma>np that take np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be trained using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The vocabulary is limited to 1 k words so that the output layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be trained using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a basic softmax with selfnormalization</example>
		<phraseLemma>np can be train use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Consistent with Schwenk &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using additional hidden layers to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stacked network results in 1 perplexity improvements on top of the 1 layer model</example>
		<phraseLemma>np use np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs as well as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Perhaps most surprisingly the additive function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the max function despite the fact that it provides no additional modeling power compared to a 1 layer network</example>
		<phraseLemma>np perform as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are similar to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although there is a large absolute improvement over the 1 gram LM the relative improvement between the 1 layer 1 stacked and 1 lateral systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 gram scenario</example>
		<phraseLemma>np be similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consists of &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The tuning set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consists of utterances from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; conversational and newswire data and the test set consists of sentences of collected conversational data</example>
		<phraseLemma>np consist of np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can see &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that perplexity improvements are similar to what is seen in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the English NYT data and that improvements in BLEU over a 1 layer model are small but consistent</example>
		<phraseLemma>np that np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which allows for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we explored an alternate architecture for embeddingbased neural network models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which allows for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a fully precomputable network with multiple hidden layers</example>
		<phraseLemma>np which allow for np</phraseLemma>
	</can>
	<can>
		<phrase>We introduce &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We introduce a topic model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; link prediction based on the intuition that linked documents will tend to have similar topic distributions integrating a maxmargin learning criterion and lexical term weights in the loss function</example>
		<phraseLemma>we introduce np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is inﬂuenced by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The new topic model we propose therefore takes association links into account so that a documents topic distribution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is inﬂuenced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the topic distributions of its neighbors</example>
		<phraseLemma>np be inﬂuenced by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are likely to have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Speciﬁcally we propose a joint model that uses link structure to deﬁne clusters of documents and following the intuition that documents in the same cluster &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are likely to have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similar topic distributions assigns each cluster its own separate Dirichlet prior over the clusters topic distribution</example>
		<phraseLemma>np be likely to have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is collected from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is collected from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Sina Weibo with three types of links between documents</example>
		<phraseLemma>np be collect from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We cluster the most frequent words word 1 vec representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into K wordclusters using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the kmeans algorithm based on the training corpus</example>
		<phraseLemma>np into np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We cluster the most frequent words word 1 vec representations into K wordclusters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the kmeans algorithm based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training corpus</example>
		<phraseLemma>np use np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is drawn from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Documents are generated as in LDA where each documents topic distribution θ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is drawn from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the clusters topic prior and each words topic assignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is drawn from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the documents topic distribution</example>
		<phraseLemma>np be draw from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the number of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Following Polson and Scott by introducing an auxiliary variable λdd 1 we derive the conditional probability of a topic assignment where Nkv denotes the count of word v assigned to topic k Ndk &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the number of tokens in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document d that are assigned to topic k Marginal counts are denoted by · dn denotes that the count excludes token n in document d d 1 denotes the indexes of documents which are linked to document d πdn is estimated based on the maximal ld k path assumption</example>
		<phraseLemma>np be the number of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not depend on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In contrast hinge loss and lexical term weights &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not depend on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; metadata availability and generally produce improvements in link prediction performance</example>
		<phraseLemma>np do not depend on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to improve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This is consistent with the role of lexical terms in the model their purpose &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; link prediction performance not improve topic quality</example>
		<phraseLemma>np be to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which means that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The higher the ratio the more linked document pairs differ from unlinked pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which means that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; linked documents are easier to distinguish</example>
		<phraseLemma>np which mean that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that takes advantage of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We introduce a new topic model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that takes advantage of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document links incorporating link information straightforwardly by deriving clusters from the link graph and assigning each cluster a separate Dirichlet prior</example>
		<phraseLemma>np that take advantage of np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we propose &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we propose a new alignment model based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text descriptions of entities without dependency on anchors</example>
		<phraseLemma>in this paper we propose np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; so that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Wang combines knowledge embedding and word embedding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a joint framework so that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the entities/relations and words are in the same vector space and hence operators like inner product between them are meaningful</example>
		<phraseLemma>np in np so that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are the same as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our focus is on a new alignment model LA while the knowledge model LK and text model LT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are the same as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the counterparts in</example>
		<phraseLemma>np be the same as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to calculate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Negative sampling &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to calculate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the normalization items over large vocabularies</example>
		<phraseLemma>np be use to calculate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be mapped to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As many Wikipedia articles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be mapped to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Freebase entities we regard a Wikipedia article as the description for the corresponding entity in Freebase</example>
		<phraseLemma>np can be map to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to extract &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; facts from plain text</example>
		<phraseLemma>np be to extract np</phraseLemma>
	</can>
	<can>
		<phrase>The above &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The above&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 experiments are consistent in results</example>
		<phraseLemma>the above np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which was based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Algorithm 1 gives pseudocode for our implementation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which was based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Dyer</example>
		<phraseLemma>np which be base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; comparing &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since the model decomposes across mentions we train by treating them as independent predictions with multiple gold outputs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;comparing the inferred link with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the gold link that is scored highest under the current model</example>
		<phraseLemma>np compare np with np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used the ﬁrstorder MST parser in two modes Eisners algorithm for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; projective trees and the ChuLiuEdmonds algorithm for nonprojective trees</example>
		<phraseLemma>we use np in np for np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we tuned &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each method we tuned&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hyperparameters by considering a grid of values and measuring dev set performance over ﬁve training iterations except for constituency parsing where we took ﬁve measurements 1 k instances apart</example>
		<phraseLemma>for np we tune np</phraseLemma>
	</can>
	<can>
		<phrase>We can see that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>In the models we consider that have realvalued features summarization and parsing with a neural model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; perceptron methods indeed have difﬁculty 1 All of the margin based methods and gradient descent on likelihood require tuning of a regularization constant and a step size</example>
		<phraseLemma>np we can see that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; All of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the models we consider that have realvalued features summarization and parsing with a neural model we can see that perceptron methods indeed have difﬁculty 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;All of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the margin based methods and gradient descent on likelihood require tuning of a regularization constant and a step size</example>
		<phraseLemma>np all of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was crucial for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our implementation of sparse updates for AdaGrad &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was crucial for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; highspeed performance decreasing time by an order of magnitude on tasks with many sparse features such as NER and dependency parsing</example>
		<phraseLemma>np be crucial for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that represents &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Standard neural network architectures for inducing embeddings have an input layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that represents each word as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a onehot vector Collobert Mikolov</example>
		<phraseLemma>np that represent np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; replacing &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Based on we preprocess the corpus by removing sentences that are less than 1 lowercase lowercasing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;replacing URLs email addresses and digits by special tokens tokenization replacing words with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; frequency 1 with &amp;ltunk&amp;gt and adding endofsentence tokens</example>
		<phraseLemma>np replace np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that does not occur in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We exclude from the evaluation the pairs in RW that contain a word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that does not occur in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our corpus</example>
		<phraseLemma>np that do not occur in np</phraseLemma>
	</can>
	<can>
		<phrase>Note that we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; θ for two different purposes</example>
		<phraseLemma>note that we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a total of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For 1 values of the frequency threshold θ ∈ we train word 1 vec models with onehot initialization and with the 1 combinations of weighting and distributional initialization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a total of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 × = 1 models</example>
		<phraseLemma>np a total of np</phraseLemma>
	</can>
	<can>
		<phrase>Our results indicate that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our results indicate that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distributional initialization is beneﬁcial for very rare words – those that occur no more than times in the corpus</example>
		<phraseLemma>we result indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is beneﬁcial for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our results indicate that distributional initialization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is beneﬁcial for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; very rare words – those that occur no more than times in the corpus</example>
		<phraseLemma>np be beneﬁcial for np</phraseLemma>
	</can>
	<can>
		<phrase>Our results for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; medium rare words – those that occur between and times – are less clear</example>
		<phraseLemma>we result for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is helpful for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our experiments show that distributional representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is helpful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rare words</example>
		<phraseLemma>np be helpful for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are focused on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Most of these works &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are focused on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; modeling single relationships and hence do not take full advantage of the graph structure of KBs</example>
		<phraseLemma>np be focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is augmented with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In our approach called RTRANSE the training set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is augmented with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relevant examples of such compositions by performing constrained walks in the knowledge graph and training so that sequences of translations lead to the desired result</example>
		<phraseLemma>np be augmented with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are mapped to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In TRANSE entities and relationships of a KB &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are mapped to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; low dimensional vectors called embeddings</example>
		<phraseLemma>np be map to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These embeddings are learnt so that for each fact &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the KB we have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; h ≈ t in the embedding space</example>
		<phraseLemma>np in np we have np</phraseLemma>
	</can>
	<can>
		<phrase>In our experiments &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our experiments&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the paths created for training only consider the training subset of facts</example>
		<phraseLemma>in we experiment np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is organized in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This dataset is artiﬁcial and each family &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is organized in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a layered tree structure where each layer refers to a generation</example>
		<phraseLemma>np be organize in np</phraseLemma>
	</can>
	<can>
		<phrase>Experiments on &lt;NP&gt; show &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Experiments on FAMILY show&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a quantitative improvement of the performance of RTRANSE</example>
		<phraseLemma>experiment on np show np</phraseLemma>
	</can>
	<can>
		<phrase>We performed &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to better understand the gains of RTRANSE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we performed a detailed evaluation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on FB 1 K by classifying the test triples along two axes</example>
		<phraseLemma>np we perform np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are connected by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A test triple is easy if its head and tail &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are connected by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a triple in the training set ie if either or is seen in train for some relationship 1</example>
		<phraseLemma>np be connect by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is affected by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On the other side TRANSE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is affected by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the cascading error since the ranking loss does not guarantee that the distance between h l 1 and phil lamarr is small so when summing it eventually ends up closer to Ireland rather than USA</example>
		<phraseLemma>np be affect by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improves &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can see that learning on paths &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improves performances on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both metrics with a gain of 1 in terms of H@ 1 and an important gain of about in mean rank which corresponds to a relative improvement of about 1</example>
		<phraseLemma>np improve np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which corresponds to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can see that learning on paths improves performances on both metrics with a gain of 1 in terms of H@ 1 and an important gain of about in mean rank &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which corresponds to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a relative improvement of about 1</example>
		<phraseLemma>np which correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was funded by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This work was carried out in the framework of the Labex MS 1 T and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was funded by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the French National Agency for Research</example>
		<phraseLemma>np be fund by np</phraseLemma>
	</can>
	<can>
		<phrase>In order to reduce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to reduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; noise in training data most natural language crowdsourcing annotation tasks gather redundant labels and aggregate them into an integrated label which is provided to the classiﬁer</example>
		<phraseLemma>in order to reduce np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁnd &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ﬁnd a statistically signiﬁcant beneﬁt from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; low item agreement training ﬁltering in 1 of our ﬁve tasks and no systematic beneﬁt from soft labeling</example>
		<phraseLemma>we ﬁnd np from np</phraseLemma>
	</can>
	<can>
		<phrase>Previous work has shown that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Previous work has shown that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training strategy may affect Hard and Easy Case test instances differently</example>
		<phraseLemma>previous work have show that np</phraseLemma>
	</can>
	<can>
		<phrase>We construct &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We construct classiﬁers for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Biased Text Detection Stemming Classiﬁcation Recognizing Textual Entailment Twitter POS Tagging and Affect Recognition and evaluate the effect of our different training strategies on the accuracy of each task</example>
		<phraseLemma>we construct np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; resulted in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Beigman Klebanov and Beigman observed that on a task classifying text as semantically old or new the &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inclusion of Hard Cases in training data resulted in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reduced classiﬁer performance on Easy Cases</example>
		<phraseLemma>np in np result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; equal to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For any given corpus we could not use a cutoff value &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;equal to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no ﬁltering or that eliminated a class</example>
		<phraseLemma>np equal to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is expressed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The agreement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is expressed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the range 1 is perfect agreement</example>
		<phraseLemma>np be express in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; to predict &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We built an SVM regression task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using unigrams to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the numerical amount of bias</example>
		<phraseLemma>np use np to predict np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is generated for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One training instance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is generated for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each label from a text and weighted by how many times that label occurred with the text</example>
		<phraseLemma>np be generate for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; including &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Features used are combinations of the characters after the removal of the longest common substring between the word pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;including 1 additional characters from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the substring word boundaries are marked</example>
		<phraseLemma>np include np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; representing &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then s is given a weight &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;representing the average item agreement for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all t ∈ s</example>
		<phraseLemma>np represent np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are based on &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our Affect Recognition experiments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are based on the affective text annotation task in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Strapparava and Mihalcea using the Sadness dataset</example>
		<phraseLemma>np be base on np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be seen in Table 1</phrase>
		<frequency>10</frequency>
		<example>Our results on all ﬁve tasks using each of the training strategies and variously evaluating on all Easy or Hard Cases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be seen in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np can be see in table 1</phraseLemma>
	</can>
	<can>
		<phrase>Our results on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all ﬁve tasks using each of the training strategies and variously evaluating on all Easy or Hard Cases can be seen in Table 1</example>
		<phraseLemma>we result on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the lack of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As a higher cutoff is used for HighAgree &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the lack of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training data results in a worse model this can be seen in the downward curves of Figures 1 – 1 where the curved line is HighAgree and the matching pattern straight line is Integrated</example>
		<phraseLemma>np the lack of np</phraseLemma>
	</can>
	<can>
		<phrase>We also found that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also found that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; soft labeling was not beneﬁcial compared to aggregation</example>
		<phraseLemma>we also find that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Different evaluations result in different orderings of embedding methods calling into question the common assumption &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one single optimal vector representation</example>
		<phraseLemma>np that there be np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we explore &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we explore&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several approaches to measuring the quality of neural word embeddings</example>
		<phraseLemma>in this work we explore np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we use &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In extrinsic evaluation we use word embeddings as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input features to a downstream task and measure changes in performance metrics speciﬁc to that task</example>
		<phraseLemma>in np we use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which we refer to as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These tasks typically involve a preselected set of query terms and semantically related target words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which we refer to as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a query inventory</example>
		<phraseLemma>np which we refer to as np</phraseLemma>
	</can>
	<can>
		<phrase>We show that using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show that using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different criteria results in different relative orderings of embeddings</example>
		<phraseLemma>we show that use np</phraseLemma>
	</can>
	<can>
		<phrase>These results indicate that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;These results indicate that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; embedding methods should be compared in the context of a speciﬁc task eg linguistic insight or good downstream performance</example>
		<phraseLemma>these result indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to ﬁnd &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to ﬁnd a term x for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a given term y so that x</example>
		<phraseLemma>np be to ﬁnd np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; many of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While we focus mainly on challenges that arise in the relatedness evaluation task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;many of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the questions discussed also apply to other scenarios</example>
		<phraseLemma>np many of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; not in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We excluded examples from datasets that contained words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;not in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our vocabulary</example>
		<phraseLemma>np not in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; give &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In comparative evaluation users &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;give direct feedback on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embeddings themselves so we do not have to deﬁne a metric that compares scored word pairs</example>
		<phraseLemma>np give np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that referred to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We then chose an equal number of categories that mostly contained abstract concepts and categories &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that referred to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; concrete concepts</example>
		<phraseLemma>np that refer to np</phraseLemma>
	</can>
	<can>
		<phrase>For each of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the query words in the dataset the nearest neighbors at ranks k ∈ for the six embeddings were retrieved</example>
		<phraseLemma>for each of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show &lt;NP&gt; across &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While most embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show relatively homogeneous behaviour across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 classes GloVe suffers disproportionally on adverbs</example>
		<phraseLemma>np show np across np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned as the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The evaluation measure is microaveraged precision for an embedding across query words where peritem precision &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned as the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; raters that discovered the intruder divided the total number of raters of item i Random guessing would achieve an average precision of 1</example>
		<phraseLemma>np be deﬁned as the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However the best performing embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at this task are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TSCCA CBOW and GloVe while TSCCA attains greater precision in relation to C&amp;W HPCA and random projection embeddings</example>
		<phraseLemma>np at np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; seem to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Pairwise similarities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;seem to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only part of the information that is encoded in word embeddings so looking at more global measures is necessary for a better understanding of differences between embeddings</example>
		<phraseLemma>np seem to be np</phraseLemma>
	</can>
	<can>
		<phrase>We trained &lt;NP&gt; to predict &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We trained a logistic regression model to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word frequency categories based on word vectors</example>
		<phraseLemma>we train np to predict np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We report the mean accuracy and standard deviation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using ﬁvefold crossvalidation at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each threshold frequency in Figure 1</example>
		<phraseLemma>np use np at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be divided into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Previous work in evaluation for word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be divided into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; intrinsic and extrinsic evaluations</example>
		<phraseLemma>np can be divide into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are done on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However all these evaluations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are done on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precollected inventories and mostly limited to local metrics like relatedness</example>
		<phraseLemma>np be do on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improve the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Extrinsic evaluations use embeddings as features in models for other tasks such as semantic role labeling or partofspeech tagging and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improve the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; existing systems</example>
		<phraseLemma>np improve the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>We present &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present a novel evaluation framework based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; direct comparisons between embeddings that provides more ﬁnegrained analysis and supports simple crowdsourced relevance judgments</example>
		<phraseLemma>we present np base on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is associated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In LDA each document d is associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a multinomial distribution over topics θd</example>
		<phraseLemma>in np be associate with np</phraseLemma>
	</can>
	<can>
		<phrase>We represent &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We represent the document collection D as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sequence of words w and topic assignments as z</example>
		<phraseLemma>we represent np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; but &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use symmetric priors α and β &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the model and experiment but&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; asymmetric priors are easily encoded in the models</example>
		<phraseLemma>np in np but np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If m is knowledge about document d then fm applies to all topics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document d</example>
		<phraseLemma>np that be in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the following sections we show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural sparse prior knowledge representations are possible</example>
		<phraseLemma>in np we show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is represented as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In previous work word correlation prior knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is represented as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word mustlink constraints and cannotlink constraints</example>
		<phraseLemma>np be represent as np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; trained on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In a topic model trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a larger dataset like the New York Times News 1 of word types have fewer than topics with nonzero counts</example>
		<phraseLemma>in np train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are associated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then for words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior knowledge we update s r q with an additional potential term</example>
		<phraseLemma>np that be associate with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; to obtain &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Hu and BoydGraber &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use WordNet 1 to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; synsets for word types and then if a synset is also in the vocabulary they add a mustlink correlation between the word type and the synset</example>
		<phraseLemma>np use np to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is higher than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If the synset is also in the vocabulary and the similarity between the synset and the word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is higher than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a threshold which in our experiment is 1 we generate a mustlink between thee words</example>
		<phraseLemma>np be higher than np</phraseLemma>
	</can>
	<can>
		<phrase>We also use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also use Mallets SparseLDA implementation for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vanilla LDA in the topic coherence experiment</example>
		<phraseLemma>we also use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the average of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We train a topic model on the NIPS dataset with different methods and compare the average topic coherence score and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the average of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top twenty topic coherence scores</example>
		<phraseLemma>np the average of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; apply &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Because document labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;apply sparsity to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the documenttopic counts the average running time per iteration decreases as the number of labeled document increases</example>
		<phraseLemma>np apply np to np</phraseLemma>
	</can>
	<can>
		<phrase>However for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large numbers of topics Gibbs sampling can become unwieldy</example>
		<phraseLemma>however for np</phraseLemma>
	</can>
	<can>
		<phrase>In contrast to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these upstream models Zhu and Nguyen improve inference of downstream models</example>
		<phraseLemma>in contrast to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We would like q to approximately represent the set q in the sense &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; every e q M is larger than the values for e q</example>
		<phraseLemma>np that for np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are many possible candidates for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; T and M For example T could be ones favorite neural network mapping from Rd to Rd</example>
		<phraseLemma>there be np for np</phraseLemma>
	</can>
	<can>
		<phrase>Here we focus on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Here we focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two composable models that were both recently shown to achieve stateoftheart performance on knowledge base completion</example>
		<phraseLemma>here we focus on np</phraseLemma>
	</can>
	<can>
		<phrase>When training on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When training on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; path queries we explicitly parameterize inverse relations</example>
		<phraseLemma>when training on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Section 1 we generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; path query datasets from these base datasets</example>
		<phraseLemma>in np we generate np</phraseLemma>
	</can>
	<can>
		<phrase>We generate &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we generate path query datasets from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these base datasets</example>
		<phraseLemma>np we generate np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are presented in Table 1</phrase>
		<frequency>10</frequency>
		<example>The statistics for the path query datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are presented in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be present in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; leads to &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On both tasks we show that the compositional training strategy proposed in Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;leads to substantial performance gains over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard singleedge training</example>
		<phraseLemma>np lead to np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In terms of hits at the best model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on WordNet is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Bilinear while the best model on Freebase is TransE</example>
		<phraseLemma>np on np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that result from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It is tempting to think that if SINGLE has accurately modeled individual edges in a graph it should accurately model the paths &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that result from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; those edges</example>
		<phraseLemma>np that result from np</phraseLemma>
	</can>
	<can>
		<phrase>These results suggest that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;These results suggest that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; compositional training is a more effective way to combat cascading errors</example>
		<phraseLemma>these result suggest that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; leads to &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our experiments show that compositional training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;leads to stateoftheart performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both path query answering and knowledge base completion</example>
		<phraseLemma>np lead to np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are available on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our code data and experiments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are available on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CodaLab platform at https</example>
		<phraseLemma>np be available on np</phraseLemma>
	</can>
	<can>
		<phrase>We would like to thank &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We would like to thank Gabor Angeli for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fruitful discussions and the anonymous reviewers for their valuable feedback</example>
		<phraseLemma>we would like to thank np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the anonymous reviewers for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We would like to thank Gabor Angeli for fruitful discussions and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the anonymous reviewers for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their valuable feedback</example>
		<phraseLemma>np the anonymous reviewer for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that makes use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our key contributions are to show the utility of dense projected structures when training the target language parser and to introduce a novel learning algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that makes use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dense structures</example>
		<phraseLemma>np that make use of np</phraseLemma>
	</can>
	<can>
		<phrase>This paper describes &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This paper describes novel methods for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the transfer of syntactic information between languages</example>
		<phraseLemma>this paper describe np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; give &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;give empirical evidence that dense structures give particularly high accuracy for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their projected dependencies</example>
		<phraseLemma>np give np for np</phraseLemma>
	</can>
	<can>
		<phrase>In experiments on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; transfer from a single source language to a single target language our average dependency accuracy is 1</example>
		<phraseLemma>in experiment on np</phraseLemma>
	</can>
	<can>
		<phrase>This is &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is a 1 absolute improvement over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the previous best results reported on this data set 1 for the approach of</example>
		<phraseLemma>this be np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This prior work involves various innovations such as &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; posterior regularization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entropy regularization and parallel guidance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a simple method to transfer delexicalized parsers across languages and a method for training on partial annotations that are projected from source to target language</example>
		<phraseLemma>np the use of np</phraseLemma>
	</can>
	<can>
		<phrase>We describe &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We describe the generalization to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than two languages in 1</example>
		<phraseLemma>we describe np to np</phraseLemma>
	</can>
	<can>
		<phrase>In our experiments we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our experiments we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; intersected alignments from GIZA to provide the Akj values</example>
		<phraseLemma>in we experiment we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The hard match deﬁnition harms performance presumably because it reduces the number of sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model</example>
		<phraseLemma>np use to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used in our experiments</phrase>
		<frequency>10</frequency>
		<example>We now describe the training procedure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used in our experiments&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use in we experiment</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; similar to that of</phrase>
		<frequency>10</frequency>
		<example>We use a perceptrontrained shiftreduce parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;similar to that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np similar to that of</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; where for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It achieves this by constrained decoding of the sentences in P under the model θ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;where for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each sentence we use beam search to search for the highest scoring projective full tree that is consistent with the dependencies in P TOP takes as input a set of full trees D and a model θ</example>
		<phraseLemma>np where for np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; as &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the EuroParl data as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our parallel data and the Google universal treebank as our evaluation data and as our training data for the supervised sourcelanguage parsers</example>
		<phraseLemma>we use np as np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are present in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use seven languages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are present in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both Europarl and the Google universal treebank</example>
		<phraseLemma>np that be present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are removed from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Sentences with length greater than and singleword sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are removed from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parallel data</example>
		<phraseLemma>np be remove from np</phraseLemma>
	</can>
	<can>
		<phrase>We follow &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We follow common practice in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training Giza for both translation directions and taking the intersection of the two sets as our ﬁnal alignment</example>
		<phraseLemma>we follow np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; when &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The number of iterations over the training data is 1 when training model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in any setting and 1 1 and 1 when&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training models θ 1 θ 1 respectively</example>
		<phraseLemma>np in np when np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; across &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Compared to the results of and which are directly comparable there are clear improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;across all languages the highest accuracy 1 is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a 1 absolute improvement over the average accuracy for</example>
		<phraseLemma>np across np be np</phraseLemma>
	</can>
	<can>
		<phrase>In all cases &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In all cases&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accuracy reported is the percentage match to a supervised parser used to parse the same data</example>
		<phraseLemma>in all case np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; forms &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we formalize the dependency parsing task for a lowresource language as a domain adaptation task in which a target resourcepoor language treebank is treated as indomain while a much larger treebank in a highresource language forms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the outofdomain data</example>
		<phraseLemma>np in np form np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However a crucial requirement for domain adaptation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that the indomain and outofdomain data have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; compatible representations</example>
		<phraseLemma>np be that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that there are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The underlying hypothesis for the joint learning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that there are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some sharedstructures across languages that we can exploit</example>
		<phraseLemma>np be that there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in both &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our approach works by jointly training a neural network dependency parser to model the syntax &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a source and target language</example>
		<phraseLemma>np in both np</phraseLemma>
	</can>
	<can>
		<phrase>In this way &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this way&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the information can ﬂow back and forth between languages allowing for the learning of a compatible crosslingual syntactic representation while also allowing the parsers to mutually correct one anothers errors</example>
		<phraseLemma>in this way np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both in terms of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We demonstrate that these encode meaningful syntactic phenomena &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both in terms of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the observable clusters and through a verb classiﬁcation task</example>
		<phraseLemma>np both in term of np</phraseLemma>
	</can>
	<can>
		<phrase>We refer the reader to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We refer the reader to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Chen and Manning for a more detailed description</example>
		<phraseLemma>we refer the reader to np</phraseLemma>
	</can>
	<can>
		<phrase>We select &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>When selecting minibatches for online gradient updates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we select an equal number of classiﬁcation instances from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source and target languages</example>
		<phraseLemma>np we select np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Following Srivastava we randomly dropout &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input layer and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hidden layer</example>
		<phraseLemma>np 1 of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that if &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We seek to incorporate this dictionary as a part of model learning to encode the intuition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two lexical items are translations of one another the parser should treat them similarly</example>
		<phraseLemma>np that if np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; as described in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the extended model as described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; section 1 we also need a bilingual dictionary</example>
		<phraseLemma>for np as describe in np</phraseLemma>
	</can>
	<can>
		<phrase>We apply &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We apply the joint model and joint model with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dictionary constraints for each target language The results are reported in Table 1</example>
		<phraseLemma>we apply np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; had &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This was despite the fact that the cascaded model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;had the beneﬁt of tuning for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the regularization parameters on a development corpus while the joint model had no parameter tuning</example>
		<phraseLemma>np have np for np</phraseLemma>
	</can>
	<can>
		<phrase>This is &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is a beneﬁt for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; small datasets providing a smoothing function to limit overtraining</example>
		<phraseLemma>this be np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moreover translation entries exist between syntactically related word types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as semantically related pairs with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the latter potentially limiting the beneﬁcial effect of the dictionary</example>
		<phraseLemma>np as well as np with np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we used &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the above experiments we used the universal POS tagset for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the languages in the corpus</example>
		<phraseLemma>in np we use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; outperforms &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Beyond 1 k tokens the joint model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the language speciﬁc POS tagset outperforms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; UPOS</example>
		<phraseLemma>np use np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provides &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Using all the target data the language speciﬁc POS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provides a 1 gain over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; UPOS</example>
		<phraseLemma>np provide np over np</phraseLemma>
	</can>
	<can>
		<phrase>This is consistent with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁnding in Figure 1</example>
		<phraseLemma>this be consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our embeddings encode not just crosslingual correspondences but also capture dependency relations which we expect might be beneﬁcial for other NLP tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on dependency parsing eg crosslingual semantic role labelling where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; longdistance relationship can be captured by word embedding</example>
		<phraseLemma>np base on np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Interestingly some small but consistent gains are still realised by joint crosslingual training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large complete treebanks</example>
		<phraseLemma>np even on np</phraseLemma>
	</can>
	<can>
		<phrase>At the heart of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At the heart of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural language parsing is the challenge of representing the state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse</example>
		<phraseLemma>at the heart of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be understood as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Like most transitionbased parsers Dyer s parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be understood as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sequential manipulation of three data structures</example>
		<phraseLemma>np can be understand as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in 1</phrase>
		<frequency>10</frequency>
		<example>We discuss the inclusion of SWAP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are added in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As in the LSTM new inputs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are added in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the rightmost position but the stack pointer indicates which LSTM cell provides ct and ht for the computation of the next iterate</example>
		<phraseLemma>np be add in np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we present &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we present&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the standard word embeddings as in Dyer and the improvements we made generating word embeddings designed to capture morphology based on orthographic strings</example>
		<phraseLemma>in this section we present np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is represented with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The same process is also applied in reverse computing a similar continuousspace vector embedding starting from the last character ← and ﬁnishing at the ﬁrst again each character &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is represented with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an LSTM cell</example>
		<phraseLemma>np be represent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as provided by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For all the corpora of the SPMRL Shared Task we used predicted POS tags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the shared task organizers</example>
		<phraseLemma>np as provide by np</phraseLemma>
	</can>
	<can>
		<phrase>For English we used &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For English we used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Stanford Dependency representation of the Penn Treebank 1</example>
		<phraseLemma>for english we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is on par with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On average Chars &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is on par with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Words POS and the best average of labeled attachment scores is achieved with Chars POS</example>
		<phraseLemma>np be on par with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is achieved with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On average Chars is on par with Words POS and the best average of labeled attachment scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is achieved with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Chars POS</example>
		<phraseLemma>np be achieve with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; proposed &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Similarly Chen &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;proposed joint learning of character and word embeddings for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Chinese claiming that characters contain rich information</example>
		<phraseLemma>np propose np for np</phraseLemma>
	</can>
	<can>
		<phrase>To the best of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To the best of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our knowledge previous work has not used characterbased embeddings to improve dependency parsers as done in this paper</example>
		<phraseLemma>to the best of np</phraseLemma>
	</can>
	<can>
		<phrase>This research was supported by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This research was supported by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the US Army Research Laboratory and the US Army Research Ofﬁce under contract/grant number W 1 NF 1 and NSF IIS 1</example>
		<phraseLemma>this research be support by np</phraseLemma>
	</can>
	<can>
		<phrase>This research was supported by &lt;NP&gt; under &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This research was supported by the US Army Research Laboratory and the US Army Research Ofﬁce under&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; contract/grant number W 1 NF 1 and NSF IIS 1</example>
		<phraseLemma>this research be support by np under np</phraseLemma>
	</can>
	<can>
		<phrase>Thanks to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Thanks to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Joakim Nivre Bernd Bohnet Fei Liu and Swabha Swayamdipta for useful comments</example>
		<phraseLemma>thanks to np</phraseLemma>
	</can>
	<can>
		<phrase>We present &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present an LSTM approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deletionbased sentence compression where the task is to translate a sentence into a sequence of zeros and ones corresponding to token deletion decisions</example>
		<phraseLemma>we present np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; outperforms &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In an experiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with human raters the LSTMbased model outperforms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline achieving 1 in readability and 1 in informativeness</example>
		<phraseLemma>np with np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Sentence compression is a standard NLP task where the goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a shorter paraphrase of a sentence</example>
		<phraseLemma>np be to generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Unfortunately this makes such systems vulnerable to error propagation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no way to recover from an incorrect parse tree</example>
		<phraseLemma>np as there be np</phraseLemma>
	</can>
	<can>
		<phrase>To our knowledge there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To our knowledge there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no competitive compression system so far which does not require any linguistic preprocessing but tokenization</example>
		<phraseLemma>to we knowledge there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; other than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However in this paper our goal is to demonstrate that a simple but robust deletionbased system can be built without using any linguistic features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;other than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; token boundaries</example>
		<phraseLemma>np other than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was related to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The ﬁrst change &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the learning procedure and the 1 one to the family of features used</example>
		<phraseLemma>np be relate to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is equivalent to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This optimization step &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is equivalent to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Quadratic Programming problem if K &amp;gt 1 which is timecostly to solve and therefore not adequate for the large amount of data we used for training the model</example>
		<phraseLemma>np be equivalent to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is set to 1</phrase>
		<frequency>10</frequency>
		<example>Consequently and for the sake of being able to successfully train the model with largescale data the learning procedure is implemented as a distributed structured perceptron with iterative parameter mixing where each shard is processed with MIRA and K &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is set to 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be set to 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is implemented as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Consequently and for the sake of being able to successfully train the model with largescale data the learning procedure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is implemented as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a distributed structured perceptron with iterative parameter mixing where each shard is processed with MIRA and K is set to 1</example>
		<phraseLemma>np be implement as np</phraseLemma>
	</can>
	<can>
		<phrase>The performance on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the development set plateaus when getting close to the last epoch</example>
		<phraseLemma>the performance on np</phraseLemma>
	</can>
	<can>
		<phrase>In particular we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a RNN based on the Long Short Term Memory unit designed to avoid vanishing gradients and to remember some longdistance dependences from the input sequence</example>
		<phraseLemma>in particular we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; be &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Let xt ht and mt &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;be the input control state and memory state at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; timestep t</example>
		<phraseLemma>np be np at np</phraseLemma>
	</can>
	<can>
		<phrase>We collect &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Following the method of Filippova &amp; Altun &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we collect a much larger corpus of about two million parallel sentencecompression instances from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the news where every compression is a subsequence of tokens from the input</example>
		<phraseLemma>np we collect np from np</phraseLemma>
	</can>
	<can>
		<phrase>We take &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We take the ﬁrst 1 sentences from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this set for the manual evaluation with human raters and the ﬁrst 1 sentences for the automatic evaluation</example>
		<phraseLemma>we take np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The raters were asked to rate readability and informativeness of compressions given the input &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are the standard evaluation metrics for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; compression</example>
		<phraseLemma>np which be np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>More than 1 of golden compressions could be fully regenerated by the LSTM systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sharp contrast with the 1 of MIRA</example>
		<phraseLemma>np which be in np</phraseLemma>
	</can>
	<can>
		<phrase>The differences in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The differences in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fscore between the three versions of LSTM are not signiﬁcant all scores are close to 1</example>
		<phraseLemma>the difference in np</phraseLemma>
	</can>
	<can>
		<phrase>The results indicate that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results indicate that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the LSTM models produce more readable and more informative compressions</example>
		<phraseLemma>the result indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>And sentences with quotes are challenging for parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; turn provide important signals for most compression systems</example>
		<phraseLemma>np which in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our understanding of why the extended model performed worse in the human evlauation than the base model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the absence of syntactic features the basic LSTM learned a model of syntax useful for compression while LSTM which was given syntactic information learned to optimize for the particular way the ”golden” set was created</example>
		<phraseLemma>np be that in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; there was &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While the automatic evaluation penalized all deviations from the single golden variant &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in human evals there was&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no penalty for readable alternatives</example>
		<phraseLemma>np in np there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can beneﬁt from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both of syntactic and Ngram models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can beneﬁt from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; largescale raw text</example>
		<phraseLemma>np can beneﬁt from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are trained from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Such models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are trained from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; largescale raw text capturing distributions of local word Ngrams which can be used to improve the ﬂuency of synthesized text</example>
		<phraseLemma>np be train from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been used as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>More recently syntactic language models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a complement or alternative to Ngram language models for machine translation syntactic analysis and tree linearization</example>
		<phraseLemma>np have be use as np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we make &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we make&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an empirical comparison between syntactic and Ngram language models on the task of word ordering which is to order a set of input words into a grammatical and ﬂuent sentence</example>
		<phraseLemma>in this paper we make np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be regarded as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be regarded as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an abstract language modeling problem although methods have been explored extending it for tree linearization broader text generation and machine translation</example>
		<phraseLemma>np can be regard as np</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare the output quality of the two models on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different scales of training data and also on different amounts of training time</example>
		<phraseLemma>we compare np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which leads to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Transitionbased syntactic word ordering can be modelled as an extension to transitionbased parsing with the main difference be ing that the order of words is not given in the input &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a much larger search space</example>
		<phraseLemma>np which lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; returns &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally the algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;returns the highestscore state best in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the agenda</example>
		<phraseLemma>np return np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; respectively in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As shown in Figure 1 from the hypotheses produced in steps 1 and 1 the features T om ← likes ” and likes &amp;gt potatoes ” are extracted which corresponds to P and P &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;respectively in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dependency language model of Chen</example>
		<phraseLemma>np respectively in np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train Ngram language models from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; raw text using modiﬁed KneserNey smoothing without pruning</example>
		<phraseLemma>we train np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; without &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We train Ngram language models from raw text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using modiﬁed KneserNey smoothing without&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pruning</example>
		<phraseLemma>np use np without np</phraseLemma>
	</can>
	<can>
		<phrase>In order to obtain &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; datasets with different parsing accuracies we randomly sample a small number of sentences from each training subset as shown in Table 1</example>
		<phraseLemma>in order to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are derived from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The dependency trees of each set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these bracketed sentences using Penn 1 Malt after base noun phrase are extracted as a single word</example>
		<phraseLemma>np be derive from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are extracted as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The dependency trees of each set are derived from these bracketed sentences using Penn 1 Malt after base noun phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are extracted as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single word</example>
		<phraseLemma>np be extract as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to measure &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The WPB test data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to measure&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; indomain performance and the SANCL blog data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to measure&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; crossdomain performance</example>
		<phraseLemma>np be use to measure np</phraseLemma>
	</can>
	<can>
		<phrase>A comparison between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;A comparison between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the grey dots and the dashed lines shows that the syntactic model trained on the WSJ data perform better than the syntactic model trained on similar amounts of AFP data</example>
		<phraseLemma>a comparison between np</phraseLemma>
	</can>
	<can>
		<phrase>As can be seen from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As can be seen from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁgure the syntactic model is much slower to train</example>
		<phraseLemma>as can be see from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shown in Figure 1</phrase>
		<frequency>10</frequency>
		<example>The BLEU and METEOR scores of the two systems on various sentence lengths &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shown in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be show in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>In contrast for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; shorter sentences the syntactic structure is relatively simple and therefore the Ngram model can give better performance based on string patterns which form smaller search spaces</example>
		<phraseLemma>in contrast for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs better in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However the Ngram model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs better in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WHPP SBARQ and WHNP</example>
		<phraseLemma>np perform better in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>In the test data WHPP SBARQ and WHNP are much less than PP NP VP ADJP ADVP and CONJP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the syntactic model gives better recalls</example>
		<phraseLemma>np on which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shown in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The experimental results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Tables 1 and 1</example>
		<phraseLemma>np be show in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is larger than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The running time of the combined system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is larger than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pure syntactic system because of Ngram probability computation</example>
		<phraseLemma>np be larger than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as compared with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However as the size of training data increases syntactic language models can become intolerantly slow to train making them beneﬁt less from the scale of training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as compared with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Ngram models</example>
		<phraseLemma>np as compare with np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; to produce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast abstractive summarization attempts to produce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a bottomup summary aspects of which may not appear as part of the original</example>
		<phraseLemma>in np to produce np</phraseLemma>
	</can>
	<can>
		<phrase>This approach to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; summarization which we call AttentionBased Summarization incorporates less linguistic structure than comparable abstractive summarization approaches but can easily scale to train on a large amount of data</example>
		<phraseLemma>this approach to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; including &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To test the effectiveness of this approach we run extensive comparisons &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with multiple abstractive and extractive baselines including&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; traditional syntaxbased systems integer linear programconstrained systems informationretrieval style approaches as well as statistical phrasebased machine translation</example>
		<phraseLemma>np with np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; including &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To test the effectiveness of this approach we run extensive comparisons with multiple abstractive and extractive baselines &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;including traditional syntaxbased systems integer linear programconstrained systems informationretrieval style approaches as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; statistical phrasebased machine translation</example>
		<phraseLemma>np include np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that take into account &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this work we focus on factored scoring functions s &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that take into account&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a ﬁxed window of previous words</example>
		<phraseLemma>np that take into account np</phraseLemma>
	</can>
	<can>
		<phrase>The core of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The core of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our parameterization is a language model for estimating the contextual probability of the next word</example>
		<phraseLemma>the core of np</phraseLemma>
	</can>
	<can>
		<phrase>The core of &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The core of our parameterization is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a language model for estimating the contextual probability of the next word</example>
		<phraseLemma>the core of np be np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows an example of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows an example of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this distribution p as a summary is generated</example>
		<phraseLemma>figure 1 show a example of np</phraseLemma>
	</can>
	<can>
		<phrase>For instance if &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current context aligns well with position i then the words x x are highly weighted by the encoder</example>
		<phraseLemma>for instance if np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the summary</phrase>
		<frequency>10</frequency>
		<example>The negative loglikelihood conveniently factors into a term for each token &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the summary&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the summary</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; Because there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As with Viterbi this beam search algorithm is much simpler than beam search for phrasebased MT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Because there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no explicit constraint that each source word be used exactly once there is no need to maintain a bit set and we can simply move from lefttoright generating words</example>
		<phraseLemma>np because there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; learning &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>They extract tree transduction rules from aligned parsed texts and learn weights on transfomations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using a maxmargin learning&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; algorithm</example>
		<phraseLemma>np use np learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; utilize &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Most of these models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;utilize recurrent neural networks for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; generation as opposed to feedforward models</example>
		<phraseLemma>np utilize np for np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we describe &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corpora used for this task the baseline methods we compare with and implementation details of our approach</example>
		<phraseLemma>in this section we describe np</phraseLemma>
	</can>
	<can>
		<phrase>The data for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The data for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task consists of news articles from the New York Times and Associated Press Wire services each paired with 1 different humangenerated reference summaries capped at bytes</example>
		<phraseLemma>the datum for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is closer to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is closer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task the model is trained for and it allows us to use a bigger evaluation set which we will include in our code release</example>
		<phraseLemma>np be closer to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consists of &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The headline vocabulary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consists of million tokens and 1 K word types with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the average title of length 1 words</example>
		<phraseLemma>np consist of np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as REFERENCE</phrase>
		<frequency>10</frequency>
		<example>We report the average interannotater agreement score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as REFERENCE&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as reference</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that have &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also include several baselines &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that have access to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same training data as our system</example>
		<phraseLemma>np that have np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are combined with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The syntax and language model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are combined with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of linguistic constraints and decoding is performed with an ILP solver</example>
		<phraseLemma>np be combine with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The syntax and language model are combined with a set of linguistic constraints and decoding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an ILP solver</example>
		<phraseLemma>np be perform with np</phraseLemma>
	</can>
	<can>
		<phrase>To improve &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To improve the baseline for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task we augment the phrase table with deletion” rules mapping each article word to include an additional deletion feature for these rules and allow for an inﬁnite distortion limit</example>
		<phraseLemma>to improve np for np</phraseLemma>
	</can>
	<can>
		<phrase>For training we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For training we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; minibatch stochastic gradient descent to minimize negative loglikelihood</example>
		<phraseLemma>for training we use np</phraseLemma>
	</can>
	<can>
		<phrase>We refer to &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We refer to the main model as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ABS and the tuned model as ABS</example>
		<phraseLemma>we refer to np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shown in Table 1</phrase>
		<frequency>10</frequency>
		<example>We also consider model and decoding ablations on the main summary model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shown in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np show in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in terms of &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We show that our proposed method effectively improves over existing summarization approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of ROUGE scores on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TAC 1 scientiﬁc summarization dataset</example>
		<phraseLemma>np in term of np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is formed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The ﬁnal summary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is formed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; maximizing both novelty and informativeness of the sentences in the summary</example>
		<phraseLemma>np be form by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is estimated using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In these approaches the content/topic distribution in the ﬁnal summary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is estimated using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a graphical probabilistic model</example>
		<phraseLemma>np be estimate use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to ﬁnd &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In these approaches the goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to ﬁnd the most central sentences in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document by constructing a graph in which nodes are sentences and edges are similarity between these sentences</example>
		<phraseLemma>np be to ﬁnd np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In these approaches the goal is to ﬁnd the most central sentences in the document by constructing a graph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which nodes are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences and edges are similarity between these sentences</example>
		<phraseLemma>np in which np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; requires &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Training the discourse parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;requires large amount of training data in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the RST framework</example>
		<phraseLemma>np require np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are useful for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While citations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are useful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; summarization relying solely on them might not accurately capture the original context of the referenced paper</example>
		<phraseLemma>np be useful for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; under which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>That is the generated summary lacks the appropriate evidence to reﬂect the content of the original paper such as circumstances data and assumptions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;under which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; certain ﬁndings were obtained</example>
		<phraseLemma>np under which np</phraseLemma>
	</can>
	<can>
		<phrase>We assume that &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We assume that the citation text in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each citing article is already known</example>
		<phraseLemma>we assume that np in np</phraseLemma>
	</can>
	<can>
		<phrase>As for &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As for the similarity function we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; cosine similarity between tfidf vectors of the sentences</example>
		<phraseLemma>as for np we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is captured by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Such quality &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is captured by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the modularity measure of the graph</example>
		<phraseLemma>np be capture by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned as follows</phrase>
		<frequency>10</frequency>
		<example>Graph modularity quantiﬁes the denseness of the subgraphs in comparison with denseness of the graph of randomly distributed edges and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be deﬁned as follow</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are built in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Speciﬁcally communities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are built in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hierarchical fashion</example>
		<phraseLemma>np be build in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are assigned to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then nodes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are assigned to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; new communities if there is a positive gain in modularity</example>
		<phraseLemma>np be assign to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are selected based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We employ a greedy strategy similar to MMR in which sentences from each group &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are selected based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the following scoring formula</example>
		<phraseLemma>np be select base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are selected from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The top singular vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are selected from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; VT iteratively until length of the summary reaches a predeﬁned threshold</example>
		<phraseLemma>np be select from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In Maximal Marginal Relevance sentences are greedily ranked according to a score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on their relevance to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document and the amount of redundant information they carry</example>
		<phraseLemma>np base on np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are then used to generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These clusters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are then used to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁnal summary by selecting the top central sentences from each cluster in a roundrobin fashion</example>
		<phraseLemma>np be then use to generate np</phraseLemma>
	</can>
	<can>
		<phrase>While &lt;NP&gt; focuses on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While ROUGEN focuses on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ngram overlaps ROUGEL uses the longest common subsequence to measure the quality of the summary</example>
		<phraseLemma>while np focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; described in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We generated two sets of summaries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the methods and baselines described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; previous sections</example>
		<phraseLemma>np use np describe in np</phraseLemma>
	</can>
	<can>
		<phrase>As far as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As far as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words summaries since we did not have gold summaries of that length we considered the ﬁrst 1 words from each gold summary</example>
		<phraseLemma>as far as np</phraseLemma>
	</can>
	<can>
		<phrase>In recent years &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In recent years&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of recommending hashtags for microblogs has been given increasing attention</example>
		<phraseLemma>in recent year np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we introduce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we introduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a novel nonparametric Bayesian method for this task</example>
		<phraseLemma>in this paper we introduce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>By taking these aspects into consideration the relative improvement of the proposed method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over the stateoftheart methods is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; around 1 in score</example>
		<phraseLemma>np over np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is used as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While the aim of hashtag #BREAKING &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the example 1 is used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a label of the microblog</example>
		<phraseLemma>np in np be use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are calculated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>All counters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are calculated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current microblog d excluded</example>
		<phraseLemma>np be calculate with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is one of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use a dataset collected from Sina Weibo which provides the Twitterlike service and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is one of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most popular one in China to evaluate the proposed approach and alternative methods</example>
		<phraseLemma>np be one of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is one of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use a dataset collected from Sina Weibo which provides the Twitterlike service and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is one of the most popular one in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; China to evaluate the proposed approach and alternative methods</example>
		<phraseLemma>np be one of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is calculated based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Precision &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is calculated based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the percentage of hashtags truly assigned” among hashtags assigned by system”</example>
		<phraseLemma>np be calculate base on np</phraseLemma>
	</can>
	<can>
		<phrase>We formulate &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We formulate hashtag recommendation as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a binary classiﬁcation task and apply NB to model the posterior probability of each hashtag given a microblog</example>
		<phraseLemma>we formulate np as np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the comparisons of the proposed method with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart methods on the constructed evaluation dataset</example>
		<phraseLemma>table 1 show np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are generated from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>NHR 1 ” is a model in which we consider all the hashtags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are generated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distribution ϕ</example>
		<phraseLemma>np be generate from np</phraseLemma>
	</can>
	<can>
		<phrase>Comparing the results of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Comparing the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the method CNHR with the methods NHR 1 and NHR 1 which do not take the types of hashtags into consideration we can see that the proposed method beneﬁts a lot from incorporating the types of hashtags</example>
		<phraseLemma>compare the result of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; that is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In curves the curve that is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the highest of the graph indicates the best performance</example>
		<phraseLemma>in np that be np</phraseLemma>
	</can>
	<can>
		<phrase>To handle &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To handle the vocabulary problem in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; keyphrase w extraction task Liu proposed a topical ord trigger model which treated the keyphrase xtraction problem as a translation process with tent topics</example>
		<phraseLemma>to handle np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; proposed &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To handle the vocabulary problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in keyphrase w extraction task Liu proposed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a topical ord trigger model which treated the keyphrase xtraction problem as a translation process with tent topics</example>
		<phraseLemma>np in np propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tend to have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Based on the the observation that similar web pages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tend to have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same tags Lu proposed a method taking both tag information and page content into account to achieve the task</example>
		<phraseLemma>np tend to have np</phraseLemma>
	</can>
	<can>
		<phrase>This work was partially funded by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This work was partially funded by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; National Natural Science Foundation of China the National High Technology Research and Development Program of China and Shanghai Science and Technology Development Funds</example>
		<phraseLemma>this work be partially fund by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Unlike the general bagofwords model which assumes words are independent our model correlates the words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on their similarities on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; readability</example>
		<phraseLemma>np base on np on np</phraseLemma>
	</can>
	<can>
		<phrase>Research on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Research on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; readability assessment starts from the early 1 th century</example>
		<phraseLemma>research on np</phraseLemma>
	</can>
	<can>
		<phrase>We perform experiments on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We perform experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; datasets of both English and Chinese</example>
		<phraseLemma>we perform experiment on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; focus on &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The wordbased methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;focus on words and their frequencies in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a document to assess its readability which mainly include the unigram/bigram/ngram models and the word acquisition model</example>
		<phraseLemma>np focus on np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is applied on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Method Graphbased label propagation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is applied on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a graph to propagate class labels from labeled nodes to unlabeled ones</example>
		<phraseLemma>np be apply on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compute &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>During the ﬁrst step a similarity function is required to build edges and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compute weights between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pairs of the nodes</example>
		<phraseLemma>np compute np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to compute &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The ﬁrst &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a reading score of a sentence by heuristic functions</example>
		<phraseLemma>np be to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is divided into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Let η denote the predetermined number of difﬁculty levels rmax and rmin denote the maximum and minimum reading score respectively of all the sentences in D To determine the difﬁculty level l ∈ of a sentence s the range &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is divided into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; η intervals so that each interval contains the reading scores of 1 η of all the sentences</example>
		<phraseLemma>np be divide into np</phraseLemma>
	</can>
	<can>
		<phrase>We employ &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We employ the coupled bagofwords model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; readability assessment under the graphbased classiﬁcation framework as described in the previous work</example>
		<phraseLemma>we employ np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will lead to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Refer to Section 1 the three coupled TFIDF matrices &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three different document graphs denoted as Gsur Glex and Gsyn respectively</example>
		<phraseLemma>np will lead to np</phraseLemma>
	</can>
	<can>
		<phrase>To take advantage of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To take advantage of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the three aspects at one time we need to merge the three graphs into one denoted as Gc</example>
		<phraseLemma>to take advantage of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are computed on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Here we also take into consideration the featurebased graph where similarities among documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are computed on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text features</example>
		<phraseLemma>np be compute on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are built from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are built from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; wellknown textbooks where documents are labeled as grade levels by credible educationists</example>
		<phraseLemma>np be build from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are averaged over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To reduce variability given certain labeling proportion 1 rounds of crossvalidation are performed and the validation results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are averaged over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the rounds</example>
		<phraseLemma>np be average over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; refer to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The values marked &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in bold in each row refer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the maximum measure gained by the methods</example>
		<phraseLemma>np in np refer to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; perform well on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The featurebased methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;perform well on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both ENCT and CPT which means both the text features developed and the classiﬁers trained are useful</example>
		<phraseLemma>np perform well on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is less than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both LR and SVM perform better than SUM but the performance is not good when the labeling proportion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is less than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 especially on the Chinese dataset</example>
		<phraseLemma>np be less than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; especially on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>From Figure 1 the three word coupling matrices greatly outperform the TFIDF matrix &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;especially on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Chinese dataset</example>
		<phraseLemma>np especially on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shown on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both the mean values and deviations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shown on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the line chart</example>
		<phraseLemma>np be show on np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we compare &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For RQ 1 we compare&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; graphs built on each singular word coupling matrix to the merged graph and the combined graph</example>
		<phraseLemma>for np we compare np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are outperformed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We show that many sophisticated features are not necessarily valuable for training a generalized model and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are outperformed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classic features such as plain wordngrams and characterngrams</example>
		<phraseLemma>np be outperform by np</phraseLemma>
	</can>
	<can>
		<phrase>To create &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To create generalized models for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; incident type classiﬁcation the most important step is an appropriate feature generation</example>
		<phraseLemma>to create np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; followed by &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In Section 1 we provide a description of our datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;followed by a comprehensive evaluation in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np follow by np in np</phraseLemma>
	</can>
	<can>
		<phrase>A review of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;A review of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; existing work on the classiﬁcation of social media content shows which features feature groups and algorithms are generally used</example>
		<phraseLemma>a review of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a variety of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The related approaches mostly use wordngrams and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a variety of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Twitterspeciﬁc features</example>
		<phraseLemma>np a variety of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we created &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For this purpose we created&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; datasets with more than 1 k labeled tweets to train and test models with respect to their generalization</example>
		<phraseLemma>for np we create np</phraseLemma>
	</can>
	<can>
		<phrase>Also for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Also for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all cities sufﬁciently many English tweets can be retrieved</example>
		<phraseLemma>also for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are part of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the next step we removed all redundant tweets as well as those with no textual content from the resulting sets as a couple of tweets contain keywords &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are part of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hashtags or @mentions but have no useful textual content</example>
		<phraseLemma>np that be part of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As the most simple approach and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all related works we represented tweets as a set of words and also as a set of characters with varying lengths</example>
		<phraseLemma>np as use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; make use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Various text classiﬁcation approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;make use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these eg for sentiment analysis</example>
		<phraseLemma>np make use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was used as &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Friedmans test &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was used as nonparametric alternative to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a repeatedmeasures oneway ANOVA and Nemenyis test was used posthoc as a replacement for Tukeys test</example>
		<phraseLemma>np be use as np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>When comparing the ten bestperforming groups the Friedman test &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed strong signiﬁcant differences between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the performances of the models 1 p &amp;lt 1 α = 1</example>
		<phraseLemma>np show np between np</phraseLemma>
	</can>
	<can>
		<phrase>As can be seen &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As can be seen&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the plain ngram approach can be improved further by 1</example>
		<phraseLemma>as can be see np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicated &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Though the Friedman test &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicated strong signiﬁcant differences between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the performances of the models = 1 p = 1 α = 1 the subsequent Nemenyi test did not indicate signiﬁcant pairwise differences</example>
		<phraseLemma>np indicate np between np</phraseLemma>
	</can>
	<can>
		<phrase>We propose &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose domain adaptation as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a solution to adapt an AES system from an initial prompt to a new prompt</example>
		<phraseLemma>we propose np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is included in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Essay writing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is included in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; highstakes assessments such as Test of English as a Foreign Language and Graduate Record Examination</example>
		<phraseLemma>np be include in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can divide the approaches of domain adaptation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into two categories based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the availability of labeled target data</example>
		<phraseLemma>np into np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which makes use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Daume´ III described a domain adaptation scheme called EasyAdapt &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which makes use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; feature augmentation</example>
		<phraseLemma>np which make use of np</phraseLemma>
	</can>
	<can>
		<phrase>We model &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We model the AES task as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a regression problem and use Bayesian linear ridge regression as our learning algorithm</example>
		<phraseLemma>we model np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; separately for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We perform the calculation of useful ngrams &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;separately for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; source and target domain essays and join them together using set union during the domain adaptation experiment</example>
		<phraseLemma>np separately for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; not included in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The POS tag sequences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;not included in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the correct POS tags are considered as bad POS</example>
		<phraseLemma>np not include in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in this experiment</phrase>
		<frequency>10</frequency>
		<example>We use scikitlearn version 1 NLTK version 1 b 1 and aspell version 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in this experiment&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in this experiment</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; where N is the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The weight entries are wij = 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;where N is the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; possible ratings</example>
		<phraseLemma>np where n be the number of np</phraseLemma>
	</can>
	<can>
		<phrase>To &lt;NP&gt; we perform &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To this end we perform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 fold cross validation by training and testing within each domain</example>
		<phraseLemma>to np we perform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; The results of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We see that the BLRR scores are close to the the human agreement scores for prompt 1 and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the domain adaptation experiments are tabulated in Table 1 where the best scores are boldfaced and the secondbest scores are underlined</example>
		<phraseLemma>np the result of np</phraseLemma>
	</can>
	<can>
		<phrase>We also see that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLRR is com instructive to understand why domain adaptation parable to linear SVM regression giving almost is important for AES</example>
		<phraseLemma>we also see that np</phraseLemma>
	</can>
	<can>
		<phrase>First we see that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;First we see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SharedHyper is a rather poor domain adaptation method for AES because it gives the lowest QWK score except for the case of using 1 and target essays in adapting from prompt 1 to prompt 1 where it is better than Concat</example>
		<phraseLemma>1 we see that np</phraseLemma>
	</can>
	<can>
		<phrase>Future work on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Future work on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; domain adaptation for AES can explore chosing the prior pρ on ρ to better reﬂect the nature of the essays involved</example>
		<phraseLemma>future work on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared to using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We have shown that domain adaptation can achieve better results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared to using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; just the small number of target domain data or just using a large amount of data from a different domain</example>
		<phraseLemma>np compare to use np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we propose &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of automatically detecting such evidences from unstructured text that support a given claim</example>
		<phraseLemma>in this work we propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to develop &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Part of this awakening is the The DebaterTM project whose goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to develop&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; technologies that will assist humans to debate and reason eg by automatically suggesting arguments relevant to an examined topic</example>
		<phraseLemma>np be to develop np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relevant to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Part of this awakening is the The DebaterTM project whose goal is to develop technologies that will assist humans to debate and reason eg by automatically suggesting arguments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relevant to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an examined topic</example>
		<phraseLemma>np relevant to np</phraseLemma>
	</can>
	<can>
		<phrase>On average for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On average for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a significant fraction of claims the proposed system succeeds to propose relevant CDE amongst its top 1 predictions and properly determines the evidence type</example>
		<phraseLemma>on average for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Furthermore ER is typically performed for factual assertions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CDED one may want to consider a wider range of claim types cf claim B in Table 1</example>
		<phraseLemma>np while in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; used &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition some works &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on machinelearning techniques used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same topic in training and testing relying on features from the topic itself in identifying arguments</example>
		<phraseLemma>np base on np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are presented to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Next in the conﬁrmation stage all the candidates suggested by the annotators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are presented to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; another set of ﬁve annotators which conﬁrm or reject each candidate and determine the type of accepted candidates</example>
		<phraseLemma>np be present to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are part of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In other words even for claims with at least one CDE of type Study on average only 1 of the sentences in the claims article &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are part of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such Study CDE</example>
		<phraseLemma>np be part of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; aims to identify &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally the fourth component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;aims to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a subset of the claims for which CDE will be proposed</example>
		<phraseLemma>np aim to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; reduces the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The signiﬁcant ﬁltering done after the contextfree stage &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;reduces the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; candidates for which we have to calculate these features</example>
		<phraseLemma>np reduce the number of np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we provide &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more technical details for each of the components in our architecture</example>
		<phraseLemma>in this section we provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; cover 1 of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This decision is based on the observation that such segments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;cover 1 of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CDE in the labeled data</example>
		<phraseLemma>np cover 1 of np</phraseLemma>
	</can>
	<can>
		<phrase>In addition we used &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition we used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the heldout data to automatically learn wider lexicons of words that are signiﬁcantly associated with each type</example>
		<phraseLemma>in addition we use np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used the Stanford NER to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extract named entities such as person and organization and an inhouse NER to extract more ﬁne grained categories such as ”educational organization” and ”leader”</example>
		<phraseLemma>we use np to np</phraseLemma>
	</can>
	<can>
		<phrase>The features for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The features for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁer used in this component can be conceptually divided into 1 types</example>
		<phraseLemma>the feature for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; where in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 relies on the average cosine similarity between the Word 1 Vec representation of all pairs of words in the two texts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;where in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each pair one word is taken from the ﬁrst text and the other word from the 1</example>
		<phraseLemma>np where in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is taken from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The 1 relies on the average cosine similarity between the Word 1 Vec representation of all pairs of words in the two texts where in each pair one word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is taken from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst text and the other word from the 1</example>
		<phraseLemma>np be take from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to rank &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The goal of this component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to rank&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all claims according to the probability that the claims article includes CDE of the relevant type associated with the claim</example>
		<phraseLemma>np be to rank np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisted of &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisted of all claims where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive examples included claims for which at least one CDE of the relevant type existed in the labeled data and negative examples included all remaining claims</example>
		<phraseLemma>np consist of np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to determine &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A thresholding mechanism on the component score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to determine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the claims for which candidates will be presented</example>
		<phraseLemma>np be use to determine np</phraseLemma>
	</can>
	<can>
		<phrase>The features used by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The features used by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this component exploited three types of information</example>
		<phraseLemma>the feature use by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; prior to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We start by assessing the proposed pipeline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;prior to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the claim selection component</example>
		<phraseLemma>np prior to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the results</phrase>
		<frequency>10</frequency>
		<example>Comparing the overlap MRR measure to the exact MRR highlights that identifying the correct segment boundaries is still a challenge and once we improve this aspect we can expect a signiﬁcant improvement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the results&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the result</phraseLemma>
	</can>
	<can>
		<phrase>In this work we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; user event logs from an ecommerce web site to fetch similar search query pairs within an active session</example>
		<phraseLemma>in this work we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is presented with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The user &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is presented with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a spelling correction and if she clicks on it they learn that the correction is valid</example>
		<phraseLemma>np be present with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Query spelling correction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; clickthrough data and uses a phrasebased error model is reported in</example>
		<phraseLemma>np that be base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is stored in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each action of a user visiting the site &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is stored in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a data warehouse and HDFS</example>
		<phraseLemma>np be store in np</phraseLemma>
	</can>
	<can>
		<phrase>As part of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As part of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; user event tracking all interactions on the site are stored in the database eg which search terms were entered which links were clicked and what actions resulted in this</example>
		<phraseLemma>as part of np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to determine &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use event timestamps to determine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; how long the users need between consecutive queries and discard similar query pairs if they are above a threshold of seconds</example>
		<phraseLemma>we use np to determine np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; trained on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use an English language model trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; publicly available corpora frequent queries and web content from the ecommerce domain to calculate loglikelihoods for each query and ﬁlter entries if the ratio is above zero ie log p = log p log p &amp;gt 1</example>
		<phraseLemma>we use np train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained on &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use an English language model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained on publicly available corpora frequent queries and web content from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ecommerce domain to calculate loglikelihoods for each query and ﬁlter entries if the ratio is above zero ie log p = log p log p &amp;gt 1</example>
		<phraseLemma>np train on np from np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; instead of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use bigram characters instead of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; single characters as suggested in in order to improve the statistical alignment models and make them more expressive</example>
		<phraseLemma>we use np instead of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is obtained by using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The character alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is obtained by using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; GIZA for 1 1 and 1 iterations of IBM Model 1 HMM and IBM Model 1 respectively</example>
		<phraseLemma>np be obtain by use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was reduced to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The initial size of 1 M query pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was reduced to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 k by exponential reservoir sampling after sorting by frequency</example>
		<phraseLemma>np be reduce to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be found in Table 1</phrase>
		<frequency>10</frequency>
		<example>Detailed statistics on the two sets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be found in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np can be find in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is based on &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The target character for substitutions and insertions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is based on a random Gaussian distribution with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a distance mean 1 and standard deviation of 1 around the selected character ie we target neighboring keys on the keyboard</example>
		<phraseLemma>np be base on np with np</phraseLemma>
	</can>
	<can>
		<phrase>We run &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We run this method times on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of 1 k queries and remove all duplicates resulting in additional 1 M search query pairs with around 1 M tokens for the training data</example>
		<phraseLemma>we run np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; resulting in &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We run this method times on a set of 1 k queries and remove all duplicates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;resulting in additional 1 M search query pairs with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; around 1 M tokens for the training data</example>
		<phraseLemma>np result in np with np</phraseLemma>
	</can>
	<can>
		<phrase>We compare the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our autocorrection system with that of three other online search engines both from the ecommerce as well as web search domain</example>
		<phraseLemma>we compare the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is more important than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We argue that recall in our setup &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is more important than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision because the goal is to correct as much as possible as autocorrection is usually invoked by the search backend as a result of low or null recall size</example>
		<phraseLemma>np be more important than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that focuses on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also investigated a system setup &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that focuses on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision over recall in order to be more in sync with the online systems that have been most likely optimized to a more cautious correction mode</example>
		<phraseLemma>np that focus on np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we presented &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we presented a powerful spelling correction system for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ecommerce domain using statistical phrasebased machine translation based on character bigram sequences</example>
		<phraseLemma>in this paper we present np for np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluated &lt;NP&gt; against &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluated our system against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; established online search sites both in the general and ecommerce domain and showed favorable results in terms of recall and retrieval rates</example>
		<phraseLemma>we evaluate np against np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; directly in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also plan to investigate the prototype &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;directly in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; query expansion as part of the search backend</example>
		<phraseLemma>np directly in np</phraseLemma>
	</can>
	<can>
		<phrase>The authors would like to thank &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The authors would like to thank&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Localization team at eBay for creating gold corrections for the evaluation sets and members of the HLT and Search teams for fruitful discussions as well as reading early drafts of this work and giving valuable feedback</example>
		<phraseLemma>the author would like to thank np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to evaluate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The produced rankings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to evaluate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard metrics for grammatical error correction in terms of correlation with human judgment</example>
		<phraseLemma>np be use to evaluate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; leads to &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The uncertainty about metrics quality &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;leads to proposals of new metrics with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Felice and Briscoe being a recent example</example>
		<phraseLemma>np lead to np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is calculated as follows</phrase>
		<frequency>10</frequency>
		<example>The probability pi for a set of outputs Oi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is calculated as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be calculate as follow</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we calculate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In a ﬁrst step we calculate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accuracy of the unclustered total orderings discarding ties</example>
		<phraseLemma>in np we calculate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; assigns &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In our case due to the large number of ties their method of tuning r is trapped in local maxima and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;assigns all systems to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single cluster</example>
		<phraseLemma>np assign np to np</phraseLemma>
	</can>
	<can>
		<phrase>The aim of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The aim of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the metrics task is to assess the quality of automatic evaluation metrics for MT in terms of correlation with the collected human judgments</example>
		<phraseLemma>the aim of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in this area</phrase>
		<frequency>10</frequency>
		<example>The inventory of evaluation metrics for GEC is signiﬁcantly smaller than for MT We hope that making our data available will fuel the interest &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in this area&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in this area</phraseLemma>
	</can>
	<can>
		<phrase>In order to use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CoNLL 1 gold standard with these metrics the editbased annotation has been converted into two plain text ﬁles one per annotator</example>
		<phraseLemma>in order to use np</phraseLemma>
	</can>
	<can>
		<phrase>Outside the scope of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Outside the scope of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the particular data we need to wonder if our results generalize to other shared tasks and other languages</example>
		<phraseLemma>outside the scope of np</phraseLemma>
	</can>
	<can>
		<phrase>The authors would like to thank &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The authors would like to thank the following judges for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their hard work on the ranking task</example>
		<phraseLemma>the author would like to thank np for np</phraseLemma>
	</can>
	<can>
		<phrase>We present &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present a novel ﬁnetuning algorithm in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a deep hybrid architecture for semisupervised text classiﬁcation</example>
		<phraseLemma>we present np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have relied on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However many of these deeper models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have relied on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; minibatch training on largescale labeled datasets either using unsupervised pretraining or improved architectural components</example>
		<phraseLemma>np have rely on np</phraseLemma>
	</can>
	<can>
		<phrase>While &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While incremental approaches such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; cotraining have been employed to help these models learn in a more updateable fashion neural architectures can naturally be trained in an online manner through the use of stochastic gradient descent</example>
		<phraseLemma>while np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; regardless of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Consider the case of a child learning to discriminate between object categories and mapping them to words given only a small amount of explicitly labeled data and a large portion of unsupervised learning where the child comprehends an adults speech or experiences positive feedback for his or her own utterances &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;regardless of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their correctness</example>
		<phraseLemma>np regardless of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when applied to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We investigate the performance of the constructed deep model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semisupervised text classiﬁcation problems and ﬁnd that our hybrid architecture outperforms all baselines</example>
		<phraseLemma>np when apply to np</phraseLemma>
	</can>
	<can>
		<phrase>In a variety of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In a variety of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; languagebased problems deep architectures have outperformed popular shallow models and classiﬁers</example>
		<phraseLemma>in a variety of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; is &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For textbased classiﬁcation a dominating model is the support vector machine with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many useful innovations to yet further improve its discriminative performance</example>
		<phraseLemma>for np be np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; described above</phrase>
		<frequency>10</frequency>
		<example>Data samples are propagated up the model to the layer targeted for layerwise training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the feedforward schema described above&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use np describe above</phraseLemma>
	</can>
	<can>
		<phrase>To calculate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To calculate the generative gradient for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an unlabeled sample u a pseudolabel must be obtained by using a layerwise HRBMs current estimate of p which can be viewed as a form of selftraining or Entropy Regularization</example>
		<phraseLemma>to calculate np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; independent of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although efﬁcient the bottomup procedure described above is greedy which means that the gradients are computed for each layerwise HRBM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;independent of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; gradient information from other layers of the model</example>
		<phraseLemma>np independent of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is incorporated into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We hypothesize that holistic ﬁnetuning ensures that discriminative information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is incorporated into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the generative features being constructed in the bottomup learning step</example>
		<phraseLemma>np be incorporate into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 and &lt;NP&gt; 1</phrase>
		<frequency>10</frequency>
		<example>Model performance was evaluated on the WebKB dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 and a smallscale version of the 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NewsGroup dataset 1</example>
		<phraseLemma>np 1 and np 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was evaluated on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Model performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was evaluated on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WebKB dataset 1 and a smallscale version of the 1 NewsGroup dataset 1</example>
		<phraseLemma>np be evaluate on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; since &lt;NP&gt; had &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For a dataset like the 1 NewsGroup which contained a number of unlabeled samples greater than training iterations we view our schema as simulating access to a data stream &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;since all models had&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; access to any given unlabeled example only once during a training run</example>
		<phraseLemma>np since np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; had &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For a dataset like the 1 NewsGroup which contained a number of unlabeled samples greater than training iterations we view our schema as simulating access to a data stream since all models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;had access to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any given unlabeled example only once during a training run</example>
		<phraseLemma>np have np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; comparing the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this setup we repeated the stratiﬁed crossfold scheme for each possible labeled data subset size &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;comparing the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SVM model against 1 SBENBU and 1 SBENBUTD</example>
		<phraseLemma>np compare the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; serves as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The additional topdown phase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;serves as a mechanism for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unifying the layerwise experts where error signals for both labeled and pseudolabeled examples increase agreement among all model layer experts</example>
		<phraseLemma>np serve as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outperformed &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We compared results against several baseline models and found that our hybrid architecture &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outperformed the others in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all settings investigated</example>
		<phraseLemma>np outperform np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improve &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We found that the SBEN especially when trained with the full BottomUpTopDown learning procedure could in some cases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improve classiﬁcation error by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; as much 1 over the Pegasos SVM and nearly 1 over the HRBM especially when data is in very limited supply</example>
		<phraseLemma>np improve np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is one of &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Sponsored search is a multibillion dollar market that makes most search engine revenue and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is one of the most successful ways for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; advertisers to reach their intended audiences</example>
		<phraseLemma>np be one of np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; cannot capture &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While this is true to a certain extent the use of simple lexical similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;cannot capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic information such as synonyms entities of the same type and strong relationships between entities</example>
		<phraseLemma>np can not capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to train &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The simplest way to harness click feedback &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to train conventional word embedding models on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a corpus that only includes clicked impressions where each sentence” is constructed by mixing the query and ad text</example>
		<phraseLemma>np be to train np on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we give &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Section 1 we give&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some background information on ad click prediction in sponsored search</example>
		<phraseLemma>in np we give np</phraseLemma>
	</can>
	<can>
		<phrase>To the best of our knowledge there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To the best of our knowledge there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no previous work adopting semanticlevel text features for the purpose of click prediction in particular word embeddings to measure queryad relevance</example>
		<phraseLemma>to the best of we knowledge there be np</phraseLemma>
	</can>
	<can>
		<phrase>On top of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On top of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis machine translation and semantic relatedness tasks</example>
		<phraseLemma>on top of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is formulated as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our baseline click prediction model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is formulated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a supervised learning problem</example>
		<phraseLemma>np be formulate as np</phraseLemma>
	</can>
	<can>
		<phrase>Due to &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Due to the signiﬁcant decrease of CTR depending on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ad position it has become common practice to use positionnormalized CTR</example>
		<phraseLemma>due to np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are needed for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However many impressions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are needed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these statistics to be reliable and therefore data for speciﬁc queryad pairs can be sparse and noisy</example>
		<phraseLemma>np be need for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the number of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These features include &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the number of overlapping words and characters in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; queryad URL queryad title and queryad description and the number of words and characters in the query</example>
		<phraseLemma>np the number of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are selected by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To model interactions among features some features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are selected by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; domain knowledge to be conjoined</example>
		<phraseLemma>np be select by np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to denote &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use bold letters qj tk dl to denote&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corresponding embedding representations of</example>
		<phraseLemma>we use np to denote np</phraseLemma>
	</can>
	<can>
		<phrase>Were used for training &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to circumvent the severe inﬂuence of ad position on the click prediction model only the impressions that are placed at the top position &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were used for training&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the click prediction model</example>
		<phraseLemma>np be use for training np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; which have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For commercial search engines which have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a very strong baseline AucLoss a reduction of 1 can be considered large</example>
		<phraseLemma>for np which have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; about how &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Eyeballing the most similar words to several queries in the vector space is often helpful for getting some sense &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;about how&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different methods inﬂuence the resulting vector spaces</example>
		<phraseLemma>np about how np</phraseLemma>
	</can>
	<can>
		<phrase>Since many of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since many of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the different sources contain their own unique information we might be able to obtain a much better understanding about the user state and intent through this rich joint embedding space</example>
		<phraseLemma>since many of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; annotated for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We present a new corpus of clinical narratives &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;annotated for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; temporal expressions and also use existing corpora in the newswire and historical domains</example>
		<phraseLemma>np annotated for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when compared with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Experimental results show that relying on word embeddings achieves a better performance on the task of extracting 1 types of relationships from a collection of newswire documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when compared with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline using TFIDF to ﬁnd similar relationships</example>
		<phraseLemma>np when compare with np</phraseLemma>
	</can>
	<can>
		<phrase>The objective of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The objective of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bootstrapping is thus to expand the seed set with new relationship instances while limiting the semantic drift ie the progressive deviation of the semantics for the extracted relationships from the semantics of the seed relationships</example>
		<phraseLemma>the objective of np</phraseLemma>
	</can>
	<can>
		<phrase>We implemented &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We implemented these ideas in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BREDS a bootstrapping system for RE based on word embeddings</example>
		<phraseLemma>we implement np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used as &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Instances with a conﬁdence above a threshold τt &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used as seeds in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the next iteration</example>
		<phraseLemma>np be use as np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is transformed into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each context &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is transformed into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single vector by a simple compositional function that starts by removing stopwords and adjectives and then sums the word embedding vectors of each individual word</example>
		<phraseLemma>np be transform into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by applying &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As Snowball BREDS generates extraction patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by applying a singlepass clustering algorithm to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the relationship instances gathered in the previous step</example>
		<phraseLemma>np by apply np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; lower than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If all the clusters have a similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;lower than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a threshold τsim a new cluster Cm is created containing the instance in</example>
		<phraseLemma>np lower than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which contains &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>FreebaseEasy is a processed version of Freebase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which contains a unique meaningful name for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; every entity together with canonical binary relations</example>
		<phraseLemma>np which contain np for np</phraseLemma>
	</can>
	<can>
		<phrase>We adopted &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We adopted a previously proposed framework for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the evaluation of largescale RE systems by Bronzi to estimate precision and recall using FreebaseEasy as the knowledge base</example>
		<phraseLemma>we adopt np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to select &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Table 1 a shows the results for the BREDS system while Table 1 b shows the results for Snowball a modiﬁed Snowball in which a relational pattern based on ReVerb &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to select&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the words for the BET context</example>
		<phraseLemma>np be use to select np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; instead of using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The performance results of Snowball and Snowball suggest that selecting words based on a relational pattern to represent the BET context &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;instead of using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the words works better for TFIDF representations</example>
		<phraseLemma>np instead of use np</phraseLemma>
	</can>
	<can>
		<phrase>For instance for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the founderof relationship BREDS learns patterns based on words such as founder cofounder cofounders or founded while Snowball only learns patterns that have the word founder like CEO and founder or founder and chairman</example>
		<phraseLemma>for instance for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance for the founderof relationship BREDS learns patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on words such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; founder cofounder cofounders or founded while Snowball only learns patterns that have the word founder like CEO and founder or founder and chairman</example>
		<phraseLemma>np base on np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; containing &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Variables tend to be complex rather than atomic entities and expressed as noun phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;containing multiple modiﬁers eg oxygen depletion in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the upper m of the ocean or timing and magnitude of surface temperature evolution in the Southern Hemisphere in deglacial proxy records</example>
		<phraseLemma>np contain np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are presented as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Text mining results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are presented as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a browsable variable hierarchy which allows users to inspect all mentions of a particular variable type in the text as well as any generalisations or specialisations</example>
		<phraseLemma>np be present as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Text mining results are presented as a browsable variable hierarchy which allows users to &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inspect all mentions of a particular variable type in the text as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any generalisations or specialisations</example>
		<phraseLemma>np in np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; turns out to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In contrast deﬁning the entities of interest in marine science &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;turns out to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; much harder</example>
		<phraseLemma>np turn out to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is 1</phrase>
		<frequency>10</frequency>
		<example>The total number of matched variables &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the corpus is 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np be 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; whereas &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance the pattern for Change in Table 1 matches the NP the annual Milankovitch and continuum temperature variability &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;whereas the actual variable is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the annual Milankovitch and continuum temperature</example>
		<phraseLemma>np whereas np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are presented in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Text mining results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are presented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an innovative way as a browsable hierarchy ranging from most general to most speciﬁc variables with links to their textual instances</example>
		<phraseLemma>np be present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Reporting on our ongoing work now future work will include an evaluation by asking domain exports to judge the correctness of extracted variables &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as their generalisations in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the given context</example>
		<phraseLemma>np as well as np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Pairs of change events – causally or otherwise associated – obtained from different publications can be chained together possibly in combination with domain knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; new hypotheses as pioneered in the work on literaturebased knowledge discovery</example>
		<phraseLemma>np in order to generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A KB tag is a canonical name &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is an identiﬁer in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a KB and an entity type</example>
		<phraseLemma>np that be np in np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are many sources of KB tags such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; manual entity indexing for news stories or data extracted from personalised knowledge stores</example>
		<phraseLemma>there be np such as np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to build &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a documents KB tags to build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a documentspeciﬁc gazetteers which we use in addition to standard features for a conditional random ﬁeld model</example>
		<phraseLemma>we use np to build np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be incorporated into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These are usually drawn from widecoverage sources like Wikipedia and census lists and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be incorporated into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sequence models by designing binary features that indicate whether a token appears in a gazetteer entry</example>
		<phraseLemma>np can be incorporate into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that indicate whether &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These are usually drawn from widecoverage sources like Wikipedia and census lists and can be incorporated into sequence models by designing binary features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that indicate whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a token appears in a gazetteer entry</example>
		<phraseLemma>np that indicate whether np</phraseLemma>
	</can>
	<can>
		<phrase>To create &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To create gazetteers from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a documents KB tags we preprocess the canonical name from each KB tag tokenising by underscore lowercasing and removing parenthesised sufﬁxes becomes chris lewis</example>
		<phraseLemma>to create np from np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; using &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train a CRF model using CRFsuite with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a standard set of features that encode lexical context token shape but no external knowledge features such as gazetteers</example>
		<phraseLemma>we train np use np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; drawn from &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>There are gazetteers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;drawn from many sources with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; approximately 1 million entries</example>
		<phraseLemma>np draw from np with np</phraseLemma>
	</can>
	<can>
		<phrase>As with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CRFWIDE MISC entities remain hard to tag correctly</example>
		<phraseLemma>as with np</phraseLemma>
	</can>
	<can>
		<phrase>We see &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We see similar trends in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TESTB except KB tags and CRFWIDE are complementary</example>
		<phraseLemma>we see np in np</phraseLemma>
	</can>
	<can>
		<phrase>At the same time &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At the same time&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the article text may be updated with verbs either being added or deleted to reﬂect the changes made to the infobox</example>
		<phraseLemma>at the same time np</phraseLemma>
	</can>
	<can>
		<phrase>In summary &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In summary&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our contributions are as follows</example>
		<phraseLemma>in summary np</phraseLemma>
	</can>
	<can>
		<phrase>We obtain &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We obtain Wikipedia URLs of this set of entities P from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; YAGO and crawl their articles revision history</example>
		<phraseLemma>we obtain np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We ﬁnd that 1 out of the 1 labeled documents contain verb edits but only 1 contain verb edits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with two arguments where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one argument matches the entity and another matches the value of the infobox change</example>
		<phraseLemma>np with np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; test on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use 1 of our labeled documents that have verb edits as features as training data and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;test on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the remaining 1</example>
		<phraseLemma>np test on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that can be used as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>There is no previous approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that can be used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; baseline therefore we have compared our structured prediction using MIP and MAXENT with a majority class baseline</example>
		<phraseLemma>np that can be use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A popular task in this regard &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Wikipedia edit history categorization</example>
		<phraseLemma>np be that of np</phraseLemma>
	</can>
	<can>
		<phrase>Because of &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Because of polysemy distant labeling for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; information extraction leads to noisy training data</example>
		<phraseLemma>because of np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even when &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We show that this labeling approach leads to good performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even when&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; offtheshelf classiﬁers are used on the distantlylabeled data</example>
		<phraseLemma>np even when np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; might lead to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance matching the KB above &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;might lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; labeling passage 1 from Table 1 as support for the fact adverseEffectOf</example>
		<phraseLemma>np might lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>From the dependence parse we also ﬁnd the verb &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is the closest ancestor of the head of the NP all modiﬁers of this verb and the path to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this verb</example>
		<phraseLemma>np which be np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will be used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The validating set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will be used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the next subsection to validate different parameter settings and the training set is used in this experiment as MRW seeds and the distant supervision of DSbaseline</example>
		<phraseLemma>np will be use in np</phraseLemma>
	</can>
	<can>
		<phrase>We report the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; runs and each run has its own randomly generated training set and heldout set</example>
		<phraseLemma>we report the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are given in Table 1</phrase>
		<frequency>10</frequency>
		<example>The results for individual types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are given in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be give in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which shows that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>LPbaseline achieves an encouraging recall for symptom &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; coordinate lists are very helpful for disambiguating those symptom mentions</example>
		<phraseLemma>np which show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on the results</phrase>
		<frequency>10</frequency>
		<example>We present another experiment to examine the precision of the systems and investigate the effect of training size and top N numbers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the results&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on the result</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Speciﬁcally for DIEL and LPbaseline the evaluation data is prepared &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the top lists of each type as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; scored by MRW with the validating instances as seeds</example>
		<phraseLemma>np with np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is given in Figure 1</phrase>
		<frequency>10</frequency>
		<example>F 1 values of DIEL with different settings are given in Figure 1 and comparison of three systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is given in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be give in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are obtained with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The reported results of DIEL in the previous experiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are obtained with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; top 1 examples from 1 seeds as training data since this setting achieves the highest value as shown in Figure 1</example>
		<phraseLemma>np be obtain with np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the DSbaseline the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training NPs obtained with different portions of the training set is given in the penultimate row</example>
		<phraseLemma>for np the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; having &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It is because with the training seeds MRW cannot effectively walk to testing lists that are generated with the validating set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;having no intersection with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training set</example>
		<phraseLemma>np have np with np</phraseLemma>
	</can>
	<can>
		<phrase>When &lt;NP&gt; is used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When such a KG is used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an application one is often interested in known facts for a given entity and not necessarily the overall size of the KG</example>
		<phraseLemma>when np be use in np</phraseLemma>
	</can>
	<can>
		<phrase>In order to make &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to make&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the query speciﬁc especially in case of ambiguous entities a few keywords are also added to the query</example>
		<phraseLemma>in order to make np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are linked to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Secondly if both noun phrases in the triple &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are linked to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the knowledge graph then it makes the triple more likely to become a representative tuple of the cluster</example>
		<phraseLemma>np be link to np</phraseLemma>
	</can>
	<can>
		<phrase>In case of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In case of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no match reﬁnements like dropping of adjectives considering only noun phrases are done to for rematching</example>
		<phraseLemma>in case of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depending on whether &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If such categorysignature based mapping is not possible then the predicate is listed as a new relation and the corresponding triple marked to belong to either NRKE or NENE extraction class &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depending on whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target entity is already present in the KG or not</example>
		<phraseLemma>np depend on whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was evaluated using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For each category a random subset of extractions in that category &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was evaluated using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Mechanical Turk</example>
		<phraseLemma>np be evaluate use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We ﬁnd that ENTICE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signiﬁcantly increase NELLs knowledge density by a factor of 1 at 1 accuracy</example>
		<phraseLemma>np be able to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicates &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Note that the order of the predicates on the path &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicates the proper assignments of subjects and objects for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that relation</example>
		<phraseLemma>np indicate np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by looking up &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the lookup table step each node in the dependency path is transformed into a vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by looking up&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embedding matrix We ∈ R where d is the dimension of a vector and V is a set of all nodes we consider</example>
		<phraseLemma>np by look up np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to capture &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We therefore perform a max pooling over Z to produce a global feature vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most useful local features produced by the convolutional layer which has a ﬁxed size of n independent of the dependency path length</example>
		<phraseLemma>np in order to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to predict &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The softmax classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Kclass distribution d where K is the size of all possible relation types and the transformation matrix is ∈ R</example>
		<phraseLemma>np be use to predict np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate our model on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our model on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SemEval 1 Task 1 which contains 1 annotated examples including 1 instances for training and 1 for test</example>
		<phraseLemma>we evaluate we model on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; including &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We evaluate our model on the SemEval 1 Task 1 which contains 1 annotated examples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;including 1 instances for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training and 1 for test</example>
		<phraseLemma>np include np for np</phraseLemma>
	</can>
	<can>
		<phrase>We randomly sampled &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We randomly sampled 1 samples from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data for validation</example>
		<phraseLemma>we randomly sample np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only if &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We choose the relation other if and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both predictions are other</example>
		<phraseLemma>np only if np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We evaluate our models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by macroaveraged using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ofﬁcial evaluation script</example>
		<phraseLemma>np by np use np</phraseLemma>
	</can>
	<can>
		<phrase>We can see that &lt;NP&gt; outperforms &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can see that the both of our depCNN and depLCNN outperforms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MVRNN and CNN by at least 1 indicating that our treatment is better than previous conventions in capturing syntactic structures for relation extraction</example>
		<phraseLemma>we can see that np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outperforms &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can see that the both of our depCNN and depLCNN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outperforms MVRNN and CNN by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; at least 1 indicating that our treatment is better than previous conventions in capturing syntactic structures for relation extraction</example>
		<phraseLemma>np outperform np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the world</phrase>
		<frequency>10</frequency>
		<example>In this paper we present an approach to extend HeidelTime to all languages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the world&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the world</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; different from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While in the meantime the range of annotated corpora covers several languages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;different from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English eg the French Portuguese Italian and Romanian TimeBank corpora most approaches to temporal tagging focus on processing English text eg DANTE and SUTime</example>
		<phraseLemma>np different from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; results in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This strategy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;results in promising temporal tagging quality for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many languages although our evaluations show that the quality of a manually extended temporal tagger is not reached and that there are open issues that need to be addressed for speciﬁc language characteristics</example>
		<phraseLemma>np result in np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally for the 1 types of temporal expressions a rule ﬁle &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains all rules for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; respective expressions</example>
		<phraseLemma>np contain np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; part of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition partofspeech constraints can be speciﬁed negative rules can be formulated and offset information can be set eg if parts of the extracted patterns are required context information during the extraction process but not &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;part of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the temporal expression itself</example>
		<phraseLemma>np part of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in detail</phrase>
		<frequency>10</frequency>
		<example>In this section we present the steps of our automatic extension approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in detail&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in detail</phraseLemma>
	</can>
	<can>
		<phrase>The main goal of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The main goal of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the simpliﬁed English resources is to make HeidelTimes original English resources amenable to automatic translation</example>
		<phraseLemma>the main goal of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to make &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The main goal of the simpliﬁed English resources &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to make&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; HeidelTimes original English resources amenable to automatic translation</example>
		<phraseLemma>np be to make np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are necessary for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While this makes the resources longer these simpliﬁed resources &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are necessary for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a smooth translation process</example>
		<phraseLemma>np be necessary for np</phraseLemma>
	</can>
	<can>
		<phrase>In Table 1 we compare &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Table 1 we compare&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; HeidelTimes original English resources with the simpliﬁed resources</example>
		<phraseLemma>in table 1 we compare np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; for &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast recall for Chinese Croatian and Romanian are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; quite low precision is low for Romanian and normalization accuracy for Chinese Vietnamese and Romanian</example>
		<phraseLemma>in np for np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; introduces &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As is the case for other languages social media informality &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;introduces numerous problems for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NLP systems such as spelling errors novel words and ungrammatical constructions</example>
		<phraseLemma>np introduce np for np</phraseLemma>
	</can>
	<can>
		<phrase>We take &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Similarly our implementation yields results on SIGHAN 1 similar to those reported in Zhang 1 Overall &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we take this tagger as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; representative of stateoftheart for Chinese NER</example>
		<phraseLemma>np we take np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Chinese &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not word segmented so embeddings for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each word cannot be trained on a raw corpus</example>
		<phraseLemma>np be not np for np</phraseLemma>
	</can>
	<can>
		<phrase>We learn &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We learn an embedding for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each character in the training corpus</example>
		<phraseLemma>we learn np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; obtain &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To use these embeddings as features we segment the NER text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;obtain position tags for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each character and add features for the corresponding embedding</example>
		<phraseLemma>np obtain np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allows &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Jointly training the embeddings with the multipart objectives &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allows the ﬁnetuned embeddings to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; further inﬂuence other embeddings even those that do not appear in the labeled training data</example>
		<phraseLemma>np allow np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that do not appear in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Jointly training the embeddings with the multipart objectives allows the ﬁnetuned embeddings to further inﬂuence other embeddings even those &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that do not appear in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the labeled training data</example>
		<phraseLemma>np that do not appear in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>This differs from a traditional CRF &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the feature function depends on the additional variables ew which are the embeddings</example>
		<phraseLemma>np in that np</phraseLemma>
	</can>
	<can>
		<phrase>We followed &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We followed the DEFT ERE 1 annotation guidelines for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entities which includes 1 major semantic types</example>
		<phraseLemma>we follow np for np</phraseLemma>
	</can>
	<can>
		<phrase>We divided &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We divided the corpus into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 folds each with messages where each message corresponds to a single instance</example>
		<phraseLemma>we divide np into np</phraseLemma>
	</can>
	<can>
		<phrase>We randomly selected &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We randomly selected 1 messages from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same time period as above</example>
		<phraseLemma>we randomly select np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; showing &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First we observe that the results for the baseline are significantly below those for SIGHAN shared tasks as well as the reported results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on Twitter NER showing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the difﬁculty of this task</example>
		<phraseLemma>np on np show np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to produce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our system collects 1 binary relations from ReVerb and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to produce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topranking relation schemas with a mean reciprocal rank of 1</example>
		<phraseLemma>np be able to produce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is denoted by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The set of argument pairs sharing the same relation pattern rel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is denoted by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Srel</example>
		<phraseLemma>np be denote by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; larger than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to control the quality of candidate entities for an argument having m words we only keep entities that have at least one alias matching m − 1 words in the argument and have a similarity score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;larger than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a threshold τ</example>
		<phraseLemma>np larger than np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we selected &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each relation we selected&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; top type pairs with the largest support as what we evaluated</example>
		<phraseLemma>for np we select np</phraseLemma>
	</can>
	<can>
		<phrase>We selected &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For each relation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we selected top type pairs with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the largest support as what we evaluated</example>
		<phraseLemma>np we select np with np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We deﬁne the association score between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relation and type pair as</example>
		<phraseLemma>we deﬁne np between np</phraseLemma>
	</can>
	<can>
		<phrase>We introduce &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We introduce an intermediate latent layer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; model latent document types and deﬁne a joint distribution over the documententity pairs and latent documenttypes on the observation data</example>
		<phraseLemma>we introduce np to np</phraseLemma>
	</can>
	<can>
		<phrase>In comparison to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In comparison to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baselines listed in the 1 nd block of Table 1 our mixture models achieve higher or competitive precision and accuracy in both scenarios considerably</example>
		<phraseLemma>in comparison to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; improves &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However even if sourcebased feature vector holds a few dimensions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our experiments src LDT improves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the precision on the basis of GM</example>
		<phraseLemma>np in np improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are reported in Table 1</phrase>
		<frequency>10</frequency>
		<example>For all variant of the LDTM the number of latent types determined by AIC &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are reported in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be report in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in previous work</phrase>
		<frequency>10</frequency>
		<example>There are three kinds of approaches developed for CCR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in previous work&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in previous work</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to build &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A compromised solution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a global discriminative model with all features indifferently</example>
		<phraseLemma>np be to build np</phraseLemma>
	</can>
	<can>
		<phrase>This work is funded by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This work is funded by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the National Program on Key Basic Research Project Program Grant No 1 CB 1 National Natural Science Foundation of China and Beijing Higher Education Young Elite Teacher Project Grant No</example>
		<phraseLemma>this work be fund by np</phraseLemma>
	</can>
	<can>
		<phrase>The data of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The data of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst run in used old topics and judgments which proved to be problematic due to the small percentage of relevant sentences</example>
		<phraseLemma>the datum of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use it to measure the distance between context C and sentence S The intuition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that the more distant the distributions are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the more likely it is that the sentence is novel</example>
		<phraseLemma>np be that np be np</phraseLemma>
	</can>
	<can>
		<phrase>On &lt;NP&gt; there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On the negative side there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no guarantee about the quality of the answers as people of very different background knowledge and with different motivation contribute answers to a given question</example>
		<phraseLemma>on np there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to learn &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In a followup work Zhou included a longshort term memory in their convolution neural network to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁcation sequence for the thread</example>
		<phraseLemma>np in np to learn np</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare lemmata and POS grams using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Jaccard containment and cosine as well as using some similarities from DKPro such as longest common substring and greedy string tiling</example>
		<phraseLemma>we compare np use np</phraseLemma>
	</can>
	<can>
		<phrase>We report results on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ofﬁcial SemEval test set for all methods</example>
		<phraseLemma>we report result on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; works better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can see that the twoclass MaxEnt 1 C classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;works better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the threeclass MaxEnt 1 C</example>
		<phraseLemma>np work better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is worse than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The CRF model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is worse than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MaxEnt on all measures which suggests that the sequential information does not help</example>
		<phraseLemma>np be worse than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which suggests that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The CRF model is worse than MaxEnt on all measures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which suggests that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sequential information does not help</example>
		<phraseLemma>np which suggest that np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that the devtesttuned values of λ for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; graphcut and ILP put much lower weight to the SamevsDifferent component</example>
		<phraseLemma>note that np for np</phraseLemma>
	</can>
	<can>
		<phrase>In Figure 1 we show &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Figure 1 we show&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁcation decisions of our local and global classiﬁers along with the human annotations for an excerpt of a thread</example>
		<phraseLemma>in figure 1 we show np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; along with &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In Figure 1 we show the classiﬁcation decisions of our local and global classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;along with the human annotations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an excerpt of a thread</example>
		<phraseLemma>np along with np for np</phraseLemma>
	</can>
	<can>
		<phrase>In this case &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this case&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the votes received by these answers from other Good answers in the thread for being in the same class won against the votes received from other Bad answers</example>
		<phraseLemma>in this case np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to improve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We probably need more informative features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pairwise classiﬁcation performance</example>
		<phraseLemma>np in order to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to provide &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One approach to lowering the barrier &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tailored patient education based on their own EHR notes</example>
		<phraseLemma>np be to provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are similar to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pseudorelevant documents according to a language model are also used to reduce query length</example>
		<phraseLemma>np that be similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is applied in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This framework &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is applied in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many information retrieval tasks and shown to be successful</example>
		<phraseLemma>np be apply in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are extracted using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>From these documents medical concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are extracted using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MetaMap</example>
		<phraseLemma>np be extract use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are annotated as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On average only 1 of the concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are annotated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; key phrases in each note</example>
		<phraseLemma>np be annotated as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; would lead to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We hypothesize that directly optimizing the key concept identiﬁer for retrieval &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;would lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; better performance</example>
		<phraseLemma>np would lead to np</phraseLemma>
	</can>
	<can>
		<phrase>This work was supported in part by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This work was supported in part by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Award 1 I 1 HX 1 from the United States Department of Veterans Affairs Health Services Research and Development Program Investigator Initiated Research</example>
		<phraseLemma>this work be support in part by np</phraseLemma>
	</can>
	<can>
		<phrase>Using &lt;NP&gt; in &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Using the images in imagetext documents of each language as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hub we derive a common semantic subspace bridging two languages by means of generalized canonical correlation analysis</example>
		<phraseLemma>use np in np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are written in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However because most documents on the Web &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are written in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one language it is not always easy to collect a sufﬁcient number of multilingual documents especially those involving minor languages</example>
		<phraseLemma>np be write in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has resulted in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In recent years however deep learning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has resulted in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a breakthrough in visual recognition and dramatically improved image recognition accuracy in generic domains which is rapidly approaching human recognition levels</example>
		<phraseLemma>np have result in np</phraseLemma>
	</can>
	<can>
		<phrase>The work by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The work by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Rupnik is probably the closest to ours</example>
		<phraseLemma>the work by np</phraseLemma>
	</can>
	<can>
		<phrase>An overview of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;An overview of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our system is depicted in Figure 1</example>
		<phraseLemma>a overview of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is depicted in Figure 1</phrase>
		<frequency>10</frequency>
		<example>An overview of our system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is depicted in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be depict in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is one of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This dataset was originally created for the study of sentence generation from images &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is one of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current hot topics in computer vision</example>
		<phraseLemma>np which be one of np</phraseLemma>
	</can>
	<can>
		<phrase>At the level of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At the level of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; individual banks predictions are relatively inaccurate</example>
		<phraseLemma>at the level of np</phraseLemma>
	</can>
	<can>
		<phrase>It turns out that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It turns out that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; aggregated risk sentiment in forwardlooking documents is a leading indicator for the actual risk ﬁgures so it can be used within predictive models</example>
		<phraseLemma>it turn out that np</phraseLemma>
	</can>
	<can>
		<phrase>A comparison of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;A comparison of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several quantitative risk indicators based on expert interviews revealed that only the Tier 1 Capital Ratio fulﬁlls all criteria</example>
		<phraseLemma>a comparison of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contain &lt;NP&gt; about &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition for this study the documents need to be written in the English language directly published by the bank and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contain forwardlooking and subjective information about&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the banks attitude and expectations towards risk</example>
		<phraseLemma>np contain np about np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by making use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Another way is to convert the PDF ﬁles already in the ﬁrst step and to extract the relevant sections &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by making use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; speciﬁc tokens</example>
		<phraseLemma>np by make use of np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 lists &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 lists some examples for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiment words in the ﬁnancial context</example>
		<phraseLemma>table 1 list np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the sum of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The document sentiment score scj &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the sum of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the term sentiment scores which belong to the document j and the sentiment class c</example>
		<phraseLemma>np be the sum of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which belong to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The document sentiment score scj is the sum of the term sentiment scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which belong to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document j and the sentiment class c</example>
		<phraseLemma>np which belong to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used in &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The ﬁrst one assumes that the sentiment words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used in the lexiconbased analysis are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the relevant features for this experiment</example>
		<phraseLemma>np use in np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consists of &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Approach The outcome of the lexiconbased approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consists of sentiment scores for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each document representing the degrees of uncertainty negativity and positivity</example>
		<phraseLemma>np consist of np for np</phraseLemma>
	</can>
	<can>
		<phrase>The evolution of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The evolution of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiment in outlook sections is not depicted but is very similar to that of CEO letters</example>
		<phraseLemma>the evolution of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; could lead to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Applying it on the data of individual banks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;could lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; incorrect conclusions</example>
		<phraseLemma>np could lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are trained with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Feature selection based on document frequency and information gain achieves better results than the ﬁrst one but only when the classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are trained with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CEO letter collection</example>
		<phraseLemma>np be train with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 1</phrase>
		<frequency>10</frequency>
		<example>Na¨ıve Bayes correctly classiﬁes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; of the instances and the optimized SVM yields &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np 1 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; demonstrate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These results are better than the baseline and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;demonstrate a noticeable potential for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervised classiﬁcation even at the level of individual bank disclosures</example>
		<phraseLemma>np demonstrate np for np</phraseLemma>
	</can>
	<can>
		<phrase>However due to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their focus on contentbased features many sentiment analysis approaches are effective only for reviews from those domains they have been speciﬁcally modeled for</example>
		<phraseLemma>however due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; require &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Existing domain adaptation techniques for sentiment analysis &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;require a few training texts from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each target domain or a few domainindependent pivot features to align domainspeciﬁc features</example>
		<phraseLemma>np require np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was introduced by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The idea of modeling sentiment ﬂow &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was introduced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Mao and Lebanon who classify local sentiment based on neighboring local sentiment in a review</example>
		<phraseLemma>np be introduce by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; whereas in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>And 1 in some domains even single sentences often contain mixed sentiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;whereas in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; others opinions tend to be laid out across sentences</example>
		<phraseLemma>np whereas in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>We propose a fairly simple argumentation model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the observation that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many web reviews are organized sequentially</example>
		<phraseLemma>np base on np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represents &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As we assume that the overall argumentation of a web review &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represents global sentiment in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst place we fully abstract from the content of the facts and opinions that serve as arguments</example>
		<phraseLemma>np represent np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that serve as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As we assume that the overall argumentation of a web review represents global sentiment in the ﬁrst place we fully abstract from the content of the facts and opinions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that serve as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; arguments</example>
		<phraseLemma>np that serve as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as a whole</phrase>
		<frequency>10</frequency>
		<example>In we learn to infer global sentiment from the Manhattan distances between a sentiment ﬂow and a set of common ﬂows thereby analyzing the ﬂow &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as a whole&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as a whole</phraseLemma>
	</can>
	<can>
		<phrase>We map &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We map positive local sentiment to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the value 1 neutral to 1 and negative to 1</example>
		<phraseLemma>we map np to np</phraseLemma>
	</can>
	<can>
		<phrase>In accordance with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In accordance with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the product corpus we see score 1 as positive global sentiment 1 as neutral and 1 as negative</example>
		<phraseLemma>in accordance with np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; together with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In each review all main clauses together with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their subordinate clauses have been classiﬁed as being positive negative or neutral</example>
		<phraseLemma>in np together with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are not found in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Most signiﬁcantly the original ﬂows from the movie training set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are not found in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any test domain</example>
		<phraseLemma>np be not find in np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the recall and the sentiment distribution of each such ﬂow in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all evaluated domains exemplarily for three of the model variants discussed above</example>
		<phraseLemma>table 1 show np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Table 1 &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The small size of the given corpus explains the limited indomain accuracy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; even some outofdomain classiﬁers perform better on the product reviews</example>
		<phraseLemma>np in table 1 np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is analogous to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The task jointly extracts entities and sentiment classes and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is analogous to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; joint entity and relation extraction in that both are information extraction tasks with multilabel outputs</example>
		<phraseLemma>np be analogous to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are represented as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the ﬁgures the input features are represented as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; black and white circles indicating that they take 1 / 1 binary values</example>
		<phraseLemma>in np be represent as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We apply Viterbi decoding for all tasks and training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a maxmargin objective which is discussed in Section 1</example>
		<phraseLemma>np be perform use np</phraseLemma>
	</can>
	<can>
		<phrase>We extend &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We extend the discrete baseline system with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two salient changes which are illustrated in Figure 1</example>
		<phraseLemma>we extend np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are trained using &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>English and Spanish word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are trained using the word 1 vec tool with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; respective corpora of minion random tweets crawled by tweet API 1</example>
		<phraseLemma>np be train use np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For English there are 1 unique words for which 1 are out of word embedding vocabulary words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Spanish there are 1 unique words for which 1 are OOE words</example>
		<phraseLemma>np while for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; give &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As can be seen from the table the neural models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;give higher Fscores than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the discrete CRF models on the English dataset while comparable overall Fscores on the Spanish dataset</example>
		<phraseLemma>np give np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are tuned with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However it can make the embeddings of OOV words less useful to the model because the hidden layers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are tuned with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; adjusted embeddings</example>
		<phraseLemma>np be tune with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not rely on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On the other hand thanks to the rich discrete features in parameter space the integrated model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁnetuning of word embeddings which even caused slight overﬁtting and reduced the performances</example>
		<phraseLemma>np do not rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with both &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As shown in Table 1 the overall results are similar to those of Mitchell &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the neural and the integrated models demonstrating the same trends as the discrete baselines</example>
		<phraseLemma>np with both np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; involve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a broad sense opinion mining refers to a process to discover useful knowledge latent in a corpus of opinionated texts fundamental issues involve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; modeling an unit of opinions and searching the corpus for those units each of which typically comprises the evaluation by an author for a target object from an aspect</example>
		<phraseLemma>np in np involve np</phraseLemma>
	</can>
	<can>
		<phrase>To produce &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To produce a practical model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CFOs it is important to investigate them from a grammar point of view</example>
		<phraseLemma>to produce np for np</phraseLemma>
	</can>
	<can>
		<phrase>The method proposed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The method proposed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; OMahony and Smyth determines the helpfulness of a product review independent of the user proﬁle and thus cannot recommend reviews based on userrelated attributes</example>
		<phraseLemma>the method propose by np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; is not &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If the distinction of UCFOs is not&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; important the above sufﬁxes can be omitted</example>
		<phraseLemma>if np be not np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; calculate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use a combination of unigram and bigram models and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;calculate the conditional probability p for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; linearchain CRF by Equation</example>
		<phraseLemma>np calculate np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is more likely to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus there should be a pass of dependencies between a Condphrase and the opinion word and a phrase that leads to the opinion word via a smaller number of dependency arrows &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is more likely to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Condphrase</example>
		<phraseLemma>np be more likely to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in Figure 1</phrase>
		<frequency>10</frequency>
		<example>A CFO usually consists of a sequence of Condphrases where each phrase modiﬁes the next phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is represented in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each entry &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is represented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hierarchy structure with nine abstraction levels</example>
		<phraseLemma>np be represent in np</phraseLemma>
	</can>
	<can>
		<phrase>The likelihood that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The likelihood that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a phrase in question is a Condphrases partially depends on the part of speech for the head in that phrase</example>
		<phraseLemma>the likelihood that np</phraseLemma>
	</can>
	<can>
		<phrase>From &lt;NP&gt; we selected &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From this dataset we selected&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reviews and manually identiﬁed elements for opinion units</example>
		<phraseLemma>from np we select np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outperformed &lt;NP&gt; in terms of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Looking at Table 1 one can see that CRF &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outperformed the other methods in terms of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fmeasure and accuracy for both partial and exact matches</example>
		<phraseLemma>np outperform np in term of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; irrespective of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We used the twotailed paired ttest for statistical testing and found that the differences of CRF and each of the other methods in Fmeasure and accuracy were statistically signiﬁcant at the 1 level &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;irrespective of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the conﬁguration</example>
		<phraseLemma>np irrespective of np</phraseLemma>
	</can>
	<can>
		<phrase>Were not included in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Typically low frequency words and words related to miscellaneous activities during a travel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were not included in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our dictionary</example>
		<phraseLemma>np be not include in np</phraseLemma>
	</can>
	<can>
		<phrase>Errors were due to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Errors were due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependency analysis which often mistakenly recognizes sentence boundaries in an informal writing style and dependency relations in a sentence comprising a phrase such as the best location for fully enjoying Asakusa”</example>
		<phraseLemma>error be due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that consist of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>UCFOs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that consist of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a large number of phrases were often not extracted due to such as This hotel is acceptable for one night to take the train at the Chuo station next morning”</example>
		<phraseLemma>np that consist of np</phraseLemma>
	</can>
	<can>
		<phrase>Due to &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Due to the sparseness problem for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; restrictive words in the training data UCFOs and CFOs were not correctly distinguished</example>
		<phraseLemma>due to np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a collection of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To address this this paper introduces the Stanford Natural Language Inference corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a collection of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence pairs labeled for entailment contradiction and semantic independence</example>
		<phraseLemma>np a collection of np</phraseLemma>
	</can>
	<can>
		<phrase>This kind of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This kind of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; indeterminacy of label can be resolved only once the questions of coreference are resolved</example>
		<phraseLemma>this kind of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we used &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the premises we used captions from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Flickr 1 k corpus a collection of approximately 1 k captions collected in an earlier crowdsourced effort</example>
		<phraseLemma>for np we use np from np</phraseLemma>
	</can>
	<can>
		<phrase>Were &lt;NP&gt; rather than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also observed that the 1 1 1 bulk of the sentences from both sources &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were syn 1 1 1 tactically complete rather than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fragments and the 1 1 1 frequency with which the parser produces a parse 1 1 1 1 1 1 rooted with an S node attests to this</example>
		<phraseLemma>np be np rather than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>We also observed that the 1 1 1 bulk of the sentences from both sources were syn 1 1 1 tactically complete rather than fragments and the 1 1 1 frequency &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parser produces a parse 1 1 1 1 1 1 rooted with an S node attests to this</example>
		<phraseLemma>np with which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which occurred in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If there was no such consensus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which occurred in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; about 1 of cases we assigned the placeholder label</example>
		<phraseLemma>np which occur in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are summarized in Table 1</phrase>
		<frequency>10</frequency>
		<example>The results of this validation process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are summarized in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be summarize in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are likely to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The Fleiss κ scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are likely to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; conservative given our large and unevenly distributed pool of annotators but they still provide insights about the levels of disagreement across the three semantic classes</example>
		<phraseLemma>np be likely to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This yields a mostfrequentclass baseline accuracy of &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SNLI and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SICK</example>
		<phraseLemma>np 1 on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are initialized with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The word embeddings for all of the models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are initialized with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 d reference GloVe vectors and ﬁnetuned as part of training</example>
		<phraseLemma>np be initialize with np</phraseLemma>
	</can>
	<can>
		<phrase>Of &lt;NP&gt; to learn &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Of the two RNN models the LSTMs more robust ability to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; longterm dependencies serves it well giving it a substantial advantage over the plain RNN and resulting in performance that is essentially equivalent to the lexicalized classiﬁer on the test set</example>
		<phraseLemma>of np to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; common to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>There are revealing patterns in the errors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;common to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the models considered here</example>
		<phraseLemma>np common to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; achieve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this section we evaluate on the SICK entailment task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using a simple transfer learning method and achieve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; competitive results</example>
		<phraseLemma>np use np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To perform transfer we take the parameters of the LSTM RNN model trained on SNLI and use them to initialize a new model which is trained from that point &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training portion of SICK</example>
		<phraseLemma>np only on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are used by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We additionally transfer the accumulators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are used by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; AdaDelta to set the learning rates</example>
		<phraseLemma>np that be use by np</phraseLemma>
	</can>
	<can>
		<phrase>Further research on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Further research on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; effective transfer learning on small data sets with neural models might facilitate improvements here</example>
		<phraseLemma>further research on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; presents &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We hope that SNLI &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;presents valuable training data and a challenging testbed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the continued application of machine learning to semantic representation</example>
		<phraseLemma>np present np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; under grant no</phrase>
		<frequency>10</frequency>
		<example>IIS 1 and the Department of the Navy Ofﬁce of Naval Research &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;under grant no&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np under grant no</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; expressed in &lt;NP&gt; are those of the authors and do not necessarily reﬂect &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Any opinions ﬁndings and conclusions or recommendations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;expressed in this material are those of the authors and do not necessarily reﬂect&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the views of Google Bloomberg LP DARPA AFRL NSF ONR or the US government</example>
		<phraseLemma>np express in np be those of the author and do not necessarily reﬂect np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to represent &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This paper introduces the task of questionanswer driven semantic role labeling where questionanswer pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to represent&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predicateargument structure</example>
		<phraseLemma>np be use to represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while &lt;NP&gt; uses &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However this intuition is difﬁcult to formalize and fundamental aspects of the task vary across efforts for example FrameNet models a large set of interpretable thematic roles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while PropBank uses&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a small set of verbspeciﬁc roles</example>
		<phraseLemma>np while np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that occurred in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the PP ﬁeld the candidates are all the prepositions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that occurred in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence s and some frequentlyused prepositions by to for with and about</example>
		<phraseLemma>np that occur in np</phraseLemma>
	</can>
	<can>
		<phrase>We sampled &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the newswire domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we sampled sentences from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the English training data of CoNLL 1 shared task excluding questions and sentences with fewer than words</example>
		<phraseLemma>np we sample np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in our approach</phrase>
		<frequency>10</frequency>
		<example>In essence questions play the part of semantic roles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our approach&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in we approach</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; listed in Table 1</phrase>
		<frequency>10</frequency>
		<example>We then normalize the annotated questions by mapping its ﬁelds WH SBJ OBJ 1 and OBJ 1 to the roles r ∈ R using a small set of rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;listed in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np list in table 1</phraseLemma>
	</can>
	<can>
		<phrase>The results in &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results in Table 1 show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our system is able to produce questions which are both grammatical and answerable</example>
		<phraseLemma>the result in np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to predict &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The goal of the answer identiﬁcation task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an answer a given sentence s target verb v and a question q</example>
		<phraseLemma>np be to predict np</phraseLemma>
	</can>
	<can>
		<phrase>In each of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In each of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two domains we train the binary classiﬁers on the training set of that domain</example>
		<phraseLemma>in each of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Most obviously the annotation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training questionanswering systems as it directly encodes questionanswer pairs</example>
		<phraseLemma>np can be use for np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we propose &lt;NP&gt; called &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we propose a graphbased model called&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TremenRank to collectively identify target entities in short texts given a name list only</example>
		<phraseLemma>in this paper we propose np call np</phraseLemma>
	</can>
	<can>
		<phrase>The context in which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The context in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a target entity occurs plays an important role in disambiguation</example>
		<phraseLemma>the context in which np</phraseLemma>
	</can>
	<can>
		<phrase>To address &lt;NP&gt; we propose &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To address these challenges we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a collective method called TremenRank to disambiguate the target entities simultaneously</example>
		<phraseLemma>to address np we propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at a time</phrase>
		<frequency>10</frequency>
		<example>The majority of applications identify a set of entities in one domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at a time&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np at a time</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outside of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As shown in Figure 1 there are many different meanings for these names &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outside of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target domain such as plant bank media and animal</example>
		<phraseLemma>np outside of np</phraseLemma>
	</can>
	<can>
		<phrase>At the beginning of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At the beginning of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the propagation we initialize the trust scores of the documents with the prior estimation</example>
		<phraseLemma>at the beginning of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Equation 1</phrase>
		<frequency>10</frequency>
		<example>True mentions receive high scores because they are likely to connect with more trustworthy documents and thus receive more trust through the ﬁrst term &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Equation 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in equation 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; higher than 1</phrase>
		<frequency>10</frequency>
		<example>We use the results of the patternbased method as our seeds which have an accuracy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;higher than 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np higher than 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is different from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>TremenRank &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is different from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the standard TrustRank and MentionRank in several respects</example>
		<phraseLemma>np be different from np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; uses &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition CEA uses the reference documents as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior knowledge to model the topiclevel biography of an entity and identifies the truly relevant documents from the candidates based on biographydocument relevance</example>
		<phraseLemma>in np use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are used as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In view of the abovementioned investigation we partition the string matching results into two parts exact and fuzzy ones &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reliable prior knowledge and unrefined prior knowledge respectively</example>
		<phraseLemma>np which be use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by using &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>CEA models the biography of an entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by using the topics in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the reference source in which each topic serves as the description of a slice of life of the target entity as shown in Figure 1</example>
		<phraseLemma>np by use np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>CEA models the biography of an entity by using the topics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the reference source in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each topic serves as the description of a slice of life of the target entity as shown in Figure 1</example>
		<phraseLemma>np in np in which np</phraseLemma>
	</can>
	<can>
		<phrase>By using &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By using a relevance threshold as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the discrimination factor CEA either preserves the document if it is rele vant or filters otherwise</example>
		<phraseLemma>by use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is measured with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For an entity in our case we generate RM on the reference source and regard it as the probabilistic model of a macrolevel allembracing biography B over the prior knowledge R For a candidate document D the biographydocument relevance r &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is measured with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Hellinger Distance between RM and DM</example>
		<phraseLemma>np be measure with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is defined as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A popular slice &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is defined as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the slice of greater concern which is normally frequently mentioned in the reference source such as the slice of the career of George W Bush as the President” versus his childhood”</example>
		<phraseLemma>np be define as np</phraseLemma>
	</can>
	<can>
		<phrase>We employ &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We employ the toolkit GibbsLDA 1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic modeling which is an implementation of LDA using Gibbs sampling</example>
		<phraseLemma>we employ np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We employ the toolkit GibbsLDA 1 in topic modeling &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is an implementation of LDA using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Gibbs sampling</example>
		<phraseLemma>np which be np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Table 1 shows the operating parameters what we set in experiments where the ones were set as the default values &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while the iterative number num is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an empirical value</example>
		<phraseLemma>np while np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are closely related to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Different from the cases in reference the vocabulary VC obtained from the contexts in candidate &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are closely related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; diverse entities or other objects with the same name</example>
		<phraseLemma>np be closely related to np</phraseLemma>
	</can>
	<can>
		<phrase>In the light of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the light of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the relevance scores CEA ranks the documents and sets a clear threshold θ to cut off the long tail in the ranking list in other words filtering the documents that have a relevance score lower than θ</example>
		<phraseLemma>in the light of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; perform much better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Overall the proposed CEA methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;perform much better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the string matching based entity archiving methods</example>
		<phraseLemma>np perform much better than np</phraseLemma>
	</can>
	<can>
		<phrase>In order to deal with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to deal with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the high computation complexity associated with the large number of permutations Cao proposed a Topk approach which clusters the permutations by the ﬁrst k objects so the number of distinct probabilities that need to evaluate in model training reduces from n!</example>
		<phraseLemma>in order to deal with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this approach training instances are selected randomly and for each query &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; object pairs are sampled from the object list</example>
		<phraseLemma>np a number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are sampled from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this approach training instances are selected randomly and for each query a number of object pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are sampled from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the object list</example>
		<phraseLemma>np be sample from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was selected in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To let the algorithm practical k &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was selected in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; as well as the public toolkit RankLib</example>
		<phraseLemma>np be select in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Training the Topk model based on this subset greatly reduces the computation cost &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a large k</example>
		<phraseLemma>np even with np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that the network outputs are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural measures of object relevance based on the present ranking model</example>
		<phraseLemma>note that np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is tested on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The proposed stochastic Topk ListNet method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is tested on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document retrieval task based on the MQ 1 dataset of LETOR 1</example>
		<phraseLemma>np be test on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with either &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>From these results we ﬁrst observe that stochastic ListNet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with either&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁxed or adaptive distribution sampling tends to outperform the conventional ListNet approach particularly with a large k</example>
		<phraseLemma>np with either np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs slightly better than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It is also seen that the adaptive distribution sampling &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs slightly better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁxed distribution sampling</example>
		<phraseLemma>np perform slightly better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This indicates that for the conventional ListNet the Top model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not the only choice in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sense of computation complexity but also the best choice in the sense of P@ 1 performance</example>
		<phraseLemma>np be not np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that maps &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A possible way to solve this problem is to learn a scoring function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that maps humanlabelled scores to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more reasonable measures of document relevance though we took a different way that employs the network outputs as the relevance measures which is what the adaptive distribution sampling method does</example>
		<phraseLemma>np that map np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are labelled by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this dataset documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are labelled by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only three values 1 1 } which is rather imprecise and the rank information is very limited</example>
		<phraseLemma>np be label by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not limited to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally we highlight that the stochastic approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not limited to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ListNet model but any model for listwise learning</example>
		<phraseLemma>np be not limit to np</phraseLemma>
	</can>
	<can>
		<phrase>It is well known that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is well known that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; listwise learning outperforms pairwise learning due to it is capability of learning full ranks</example>
		<phraseLemma>it be well know that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; exploit &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First we simply use the extracted science rules directly as MLN clauses and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;exploit the structure present in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hard constraints to improve tractability</example>
		<phraseLemma>np exploit np in np</phraseLemma>
	</can>
	<can>
		<phrase>We cast &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We cast QA as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a reasoning problem in weightedﬁrst order logic</example>
		<phraseLemma>we cast np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have shown that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Recently Beltagy and Beltagy and Mooney &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have shown that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MLNs can be used to reason with rules derived from natural language</example>
		<phraseLemma>np have show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; appear to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While MLNs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;appear to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a natural ﬁt it is a priori unclear how to effectively formulate the QA task as an MLN problem</example>
		<phraseLemma>np appear to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to determine whether &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The reasoning task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to determine whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the query is true given the setup and the input knowledge</example>
		<phraseLemma>np be to determine whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by focusing on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Existing techniques address large grounding size &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by focusing on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relevant atoms or grouping atoms into large classes of interchangeable atoms</example>
		<phraseLemma>np by focus on np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that the efﬁciency boost using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hard constraints is orthogonal to using prototypical constants and can be applied here as well</example>
		<phraseLemma>note that np use np</phraseLemma>
	</can>
	<can>
		<phrase>We create &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We create copies of these rules for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; edges with the same label r = s with a higher weight and for edges with different labels r 1 = s with a lower weight</example>
		<phraseLemma>we create np for np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to deﬁne &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use MLNs to deﬁne&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the inference procedure to prove the query using the alignments from aligns</example>
		<phraseLemma>we use np to deﬁne np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; referred to as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given a question we use a simple wordoverlap based matching algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;referred to as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the rule selector to retrieve the top matching sentences to be considered for the question</example>
		<phraseLemma>np refer to as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; did not result in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>ERMLN as expected &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;did not result in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any timeouts during grounding</example>
		<phraseLemma>np do not result in np</phraseLemma>
	</can>
	<can>
		<phrase>As Table 1 shows &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As Table 1 shows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Pralines accuracy drops upon removing either of these constraints highlighting their importance</example>
		<phraseLemma>as table 1 show np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; demonstrate &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We show our approach achieves stateoftheart English entity linking performance and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;demonstrate successful deployment in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a new language and two new domains</example>
		<phraseLemma>np demonstrate np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be represented as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A structured KB such as DBpedia &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be represented as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a weighted graph Gk that consists of a set of vertices representing the entities and a set of directed edges labeled with relations between entities</example>
		<phraseLemma>np can be represent as np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we add &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In each Gic we add&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an edge between two vertices if they are connected in Gk by some relation r and their mentions are connected in Gm</example>
		<phraseLemma>in np we add np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by comparing &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Intuitively we would like to measure this structure difference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by comparing each candidate graph Gic with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its mention context graph Gm</example>
		<phraseLemma>np by compare np with np</phraseLemma>
	</can>
	<can>
		<phrase>In Figure 1 &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Figure 1 A Reche Caldwell is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a former player of New England Patriots and in Figure 1 B Andre Caldwells Wikipedia article includes a hyperlink pointing to New England Patriots</example>
		<phraseLemma>in figure 1 np be np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we ﬁrst &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we ﬁrst&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; show QCVs performance on generic English corpora and compare it with our baseline together with other stateoftheart EL systems</example>
		<phraseLemma>in this section we ﬁrst np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compare it with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this section we ﬁrst show QCVs performance on generic English corpora and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compare it with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our baseline together with other stateoftheart EL systems</example>
		<phraseLemma>np compare it with np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 presents the results of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 presents the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; QCV our baseline system as well as the top 1 supervised participant systemsand the top 1 unsupervised participant systems of the TACKBP 1 EL track</example>
		<phraseLemma>table 1 present the result of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; correspond to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Table 1 SR CS and CV correspond to the Salience Ranking the Context Similarity Ranking and the Collective Validation in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our QCV algorithm respectively</example>
		<phraseLemma>in np correspond to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is achieved by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As shown in Table 1 the best performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is achieved by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fahrni a supervised system using over ﬁnetuned features and many linguistic resources</example>
		<phraseLemma>np be achieve by np</phraseLemma>
	</can>
	<can>
		<phrase>We built &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We built our KB with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; over 1 domain ontologies downloaded from BioPortal</example>
		<phraseLemma>we build np with np</phraseLemma>
	</can>
	<can>
		<phrase>As a consequence &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a consequence&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the collective validation step in QCV does not take much effect since the weights of the involved relations are quite close to one another</example>
		<phraseLemma>as a consequence np</phraseLemma>
	</can>
	<can>
		<phrase>In order to create &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to create&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an evaluation dataset our domain expert selected three scientiﬁc papers about Early Triassic discovery Global Stratotype Section and Triassic crisis which are three different aspects of Earth Science related discovery and then identiﬁed 1 mentions that can be linked to DBpedia entities</example>
		<phraseLemma>in order to create np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contained in &lt;NP&gt; are those of the authors and should not be interpreted as representing &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The views and conclusions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contained in this document are those of the authors and should not be interpreted as representing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ofﬁcial policies either expressed or implied of the US Government</example>
		<phraseLemma>np contain in np be those of the author and should not be interpret as represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponds to &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The basic idea behind TransE is that the relationship between two entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponds to a translation between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embeddings of the entities that is h r ≈ t when the triple holds</example>
		<phraseLemma>np correspond to np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; takes &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As compared with TransE PTransE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;takes rich relation paths in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KBs for learning</example>
		<phraseLemma>np take np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is composed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As illustrated in Figure 1 the path embedding p &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is composed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embeddings of BorninCity CityInState and StateInCountry</example>
		<phraseLemma>np be compose by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is composed of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>That is the set of invalid triples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is composed of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original valid triple with one of three components replaced</example>
		<phraseLemma>np be compose of np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we adopt &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we adopt&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two datasets extracted from Freebase ie FB 1 K and FB 1 K</example>
		<phraseLemma>in this paper we adopt np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PTransE and other baselines by predicting whether testing triples hold</example>
		<phraseLemma>we evaluate the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we follow &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the subtask of entity prediction we follow&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the setting in</example>
		<phraseLemma>in np we follow np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For PTransE we ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the best hyperparameters according to the mean rank in validation set</example>
		<phraseLemma>for np we ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; generate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We combine the ranking scores from the textbased model with those from KB representations to rank testing triples and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;generate precisionrecall curves for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both TransE and PTransE</example>
		<phraseLemma>np generate np for np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we set &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For learning of TransE and PTransE we set&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dimensions of entities/relations embeddings k = 1 the learning rate λ = 1 the margin γ = 1 and dissimilarity metric as</example>
		<phraseLemma>for np we set np</phraseLemma>
	</can>
	<can>
		<phrase>We also compare with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also compare with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MIMLRE which is the stateofart method using distant supervision</example>
		<phraseLemma>we also compare with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; there are &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In contrast many testing triples in this task correspond to nonrelation and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;there are usually several relation paths between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two entities in these nonrelation triples</example>
		<phraseLemma>np there be np between np</phraseLemma>
	</can>
	<can>
		<phrase>This paper presents &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This paper presents PTransE a novel representation learning method for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KBs which encodes relation paths to embed both entities and relations in a lowdimensional space</example>
		<phraseLemma>this paper present np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that can be used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To address this problem we present an information extraction method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that can be used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; knowledge base completion</example>
		<phraseLemma>np that can be use for np</phraseLemma>
	</can>
	<can>
		<phrase>We compute &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compute embeddings for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words as is standard in a large body of NLP literature but we also compute embeddings for entities and for types</example>
		<phraseLemma>we compute np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use a variety of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use a variety of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lexical and syntactic features to segment and classify entity mentions</example>
		<phraseLemma>np use a variety of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by using &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Yogatama classify mentions to more ﬁnegrained types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by using different features for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mentions and embedding labels in the same space</example>
		<phraseLemma>np by use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are not covered by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus in contrast to our approach their system is not able to type entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are not covered by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; existing KBs</example>
		<phraseLemma>np that be not cover by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by replacing &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Wang learn embeddings of words and entities in the same space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by replacing Wikipedia anchors with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their corresponding entities</example>
		<phraseLemma>np by replace np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; jointly with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For our context model we learn and use type embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;jointly with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; corpus words to improve generalization a novel contribution of this paper to the best of our knowledge</example>
		<phraseLemma>np jointly with np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; can have &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our problem each example can have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several instances and each instance can have several labels</example>
		<phraseLemma>in np can have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that provides &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Therefore we will only address entity typing in this paper and consider entity linking as an independent module &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that provides contexts of entities for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; FIGMENT</example>
		<phraseLemma>np that provide np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been widely used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The global model scores possible types of entity e based on a distributed vector representation or embedding ~v ∈ Rd of e ~v can be learned from the entityannotated corpus C Embeddings of words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been widely used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different NLP applications</example>
		<phraseLemma>np have be widely use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to learn &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Stochastic gradient descent with AdaGrad and minibatches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parameters</example>
		<phraseLemma>np be use to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the average of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the average of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embeddings of the 1 l words to the left and right of the mention</example>
		<phraseLemma>np be the average of np</phraseLemma>
	</can>
	<can>
		<phrase>If there are &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If there are other entities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the contexts we ﬁrst replace them with their notable type to improve generalization</example>
		<phraseLemma>if there be np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that correspond to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For Sc 1 t we create train dev and test sets of contexts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that correspond to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; train dev and test sets of entities</example>
		<phraseLemma>np that correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we take &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each type t we take&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; contexts from the mentions of those entities whose notable type is t Recall however that each context is labeled with all types of its entity – see Section 1</example>
		<phraseLemma>for np we take np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then if the number of contexts for t is larger than a minimum we sample the contexts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training entities of t</example>
		<phraseLemma>np base on the number of np</phraseLemma>
	</can>
	<can>
		<phrase>To learn &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To learn embeddings for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; units we ﬁrst exclude lines containing test entities and then replace each entity with its notable type</example>
		<phraseLemma>to learn np for np</phraseLemma>
	</can>
	<can>
		<phrase>We set the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We set the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hidden units to</example>
		<phraseLemma>we set the number of np</phraseLemma>
	</can>
	<can>
		<phrase>The table shows that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The table shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CM can leverage larger context sizes well</example>
		<phraseLemma>the table show that np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are only few linguistic contexts in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entities of these types can be exchanged for each other</example>
		<phraseLemma>there be np in which np</phraseLemma>
	</can>
	<can>
		<phrase>For 1 of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For 1 of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the errors on this subset the top predicted type is a subtype of person</example>
		<phraseLemma>for 1 of np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since we require a common semantic framework for KB uniﬁcation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use vector representations based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word senses which are mapped to a very large sense inventory</example>
		<phraseLemma>np we use np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; occurs</phrase>
		<frequency>10</frequency>
		<example>This shared sense inventory then constitutes the common ground &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which disambiguation alignment and ﬁnal uniﬁcation occurs&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in which np occur</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are merged into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A disambiguation stage where all KBi ∈ K are linked to S either by interresource mapping or disambiguation and all Ei &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are merged into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a uniﬁed set of entities E</example>
		<phraseLemma>np be merge into np</phraseLemma>
	</can>
	<can>
		<phrase>As a result of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a result of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this process we obtain a set KS comprising all the KBs in K redeﬁned using the common sense inventory S</example>
		<phraseLemma>as a result of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to construct &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then given an input text the generated semantic signatures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to construct&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a subgraph of the semantic network representing the meaning of the content words in that text</example>
		<phraseLemma>np be use to construct np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; require &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As explained &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Section 1 while each KB in KD can be unambiguously redeﬁned via BabelNet interresource links and added to KS KBs in KU require&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an explicit disambiguation step</example>
		<phraseLemma>np in np require np</phraseLemma>
	</can>
	<can>
		<phrase>We rank &lt;NP&gt; according to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We rank all such triples according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their conﬁdence and select those above a given threshold δdis</example>
		<phraseLemma>we rank np accord to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; compared to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The underlying assumption is that for highconﬁdence subjectobject pairs the embeddings associated with the correct senses s and sg will be closest d &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in VS compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any other candidate pair</example>
		<phraseLemma>np in np compare to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by looking at &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We compute the generality Gen of a given relation r &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by looking at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the spatial dispersion of the sense embeddings associated with its seed subjects and objects</example>
		<phraseLemma>np by look at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; be &lt;NP&gt; associated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Let vD &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;be the set of sense embeddings associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the domain seed arguments of r For both vD and vG we compute the corresponding centroid vectors µ and µ as</example>
		<phraseLemma>np be np associate with np</phraseLemma>
	</can>
	<can>
		<phrase>The result of &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The result of this&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; procedure is a relation speciﬁcity ranking that associates each relation r with its generality Gen</example>
		<phraseLemma>the result of np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by computing &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then for each pair of KBs hKB KB i ∈ K × K we compare all relation pairs hr r i ∈ R × R &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by computing the cosine similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; domain centroids sD and between range centroids s</example>
		<phraseLemma>np by compute np between np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; is above &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If conﬁdence is above&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a given threshold δalign then ri and rj are merged into the same relation synset</example>
		<phraseLemma>if np be above np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used BabelNet 1 as our uniﬁed sense inventory for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the uniﬁcation procedure as well as the underlying inventory for both BABELFY and SENSEMBED</example>
		<phraseLemma>we use np as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We used BabelNet 1 as our uniﬁed sense inventory for the uniﬁcation procedure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as the underlying inventory for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both BABELFY and SENSEMBED</example>
		<phraseLemma>np as well as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; derived from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We used PATTY &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with FREEBASE types and pattern synsets derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Wikipedia and WISENET 1 with Wikipedia relational phrases We selected NELL and REVERB as unlinked resources</example>
		<phraseLemma>np with np derive from np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 also shows that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 also shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a restrictive δspec results in lower coverage values due to the increased number of triples disambiguated without context</example>
		<phraseLemma>table 1 also show that np</phraseLemma>
	</can>
	<can>
		<phrase>Given the results of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 we considered the top 1 k frequent relations for each KB and ran the algorithm over each possible pair of KBs with two different conﬁgurations</example>
		<phraseLemma>give the result of np</phraseLemma>
	</can>
	<can>
		<phrase>We have presented &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We have presented KBUNIFY a novel general approach for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; disambiguating and seamlessly unifying KBs produced by different OIE systems</example>
		<phraseLemma>we have present np for np</phraseLemma>
	</can>
	<can>
		<phrase>To address this problem &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To address this problem&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a multitask recurrent neural network language model for sentencelevel name detection is proposed for use in combination with outofvocabulary word detection</example>
		<phraseLemma>to address this problem np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been applied to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Named entity recognition systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; speech output taking advantage of local contextual cues to names but as illustrated above neighboring words are often affected which obscures lexical cues to name regions</example>
		<phraseLemma>np have be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; taking advantage of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Named entity recognition systems have been applied to speech output &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;taking advantage of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; local contextual cues to names but as illustrated above neighboring words are often affected which obscures lexical cues to name regions</example>
		<phraseLemma>np take advantage of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is sensitive to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While these techniques provide some beneﬁt the use of discrete lexical context cues &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is sensitive to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the limited amount of training data available</example>
		<phraseLemma>np be sensitive to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; provides &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Combining the LM objective &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with name prediction provides&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a regularization effect in training that leads to improved sentencelevel name prediction</example>
		<phraseLemma>np with np provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is introduced in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The overall framework for speech recognition and baseline name error detection system is outlined in Section 1 and the multitask RNN approach for sentencelevel name prediction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is introduced in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np be introduce in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Formally the MT RNN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; t = 1 n for a sentence of length n as</example>
		<phraseLemma>np be deﬁned over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used in this paper</phrase>
		<frequency>10</frequency>
		<example>There are two types of datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used in this paper&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use in this paper</phraseLemma>
	</can>
	<can>
		<phrase>Note that there are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that there are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; very few positive samples of name errors whereas for the sentencelevel name prediction task and the wordlevel OOV prediction task the data skewness is somewhat less severe</example>
		<phraseLemma>note that there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; whereas for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Note that there are very few positive samples of name errors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;whereas for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentencelevel name prediction task and the wordlevel OOV prediction task the data skewness is somewhat less severe</example>
		<phraseLemma>np whereas for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to detect &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The Stanford Name Entity Recognizer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to detect&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; names in each sentence and a sentencelevel name label is assigned if there are any names present</example>
		<phraseLemma>np be use to detect np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; corresponds to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For this task each sample corresponds to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hypothesized sentence generated by the ASR system and a groundtruth label indicating whether there are names in that sentence</example>
		<phraseLemma>for np correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; trained on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The word classes are based on seed words learned from sentencelevel name prediction which are expanded to classes using a nearest neighbor distance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with RNN embeddings trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the BOLT data</example>
		<phraseLemma>np with np train on np</phraseLemma>
	</can>
	<can>
		<phrase>Looking at &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Looking at performance tradeoffs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision and recall we ﬁnd that the use of the RNN systems mainly improves precision though there is also a small gain in recall overall</example>
		<phraseLemma>look at np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; particularly for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The use of the OOV posterior improves precision but limits recall &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;particularly for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the general domain where recall of the OOV posterior alone is only 1 vs 1 for the word context model with the OOV posterior information</example>
		<phraseLemma>np particularly for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained only on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Examples 1 and 1 illustrate the importance of lexical cues to names but wordbased cues are unreliable for the systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained only on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the small amount of domainspeciﬁc data</example>
		<phraseLemma>np train only on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; associated with &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As we can see in Fig 1 a the positive and negative sentence embeddings learned by RNN LM are randomly scattered in the space indicating that embeddings learned via unsupervised training may fail to capture the sentencelevel indicators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;associated with a particular task in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this case presence of a name</example>
		<phraseLemma>np associate with np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we combine the language modeling task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the sentencelevel name prediction task and each training sample has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; labels for both tasks</example>
		<phraseLemma>np with np have np</phraseLemma>
	</can>
	<can>
		<phrase>For the problem of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the problem of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentencelevel name prediction the proposed method of combining the language modeling and sentencelevel name prediction objectives in an MT RNN achieves the best results among studied models for the domain represented by the training data as well as in the opendomain scenario</example>
		<phraseLemma>for the problem of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the problem of sentencelevel name prediction the proposed method of combining the language modeling and sentencelevel name prediction objectives in an MT RNN achieves the best results among studied models for the domain represented by the training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the opendomain scenario</example>
		<phraseLemma>np as well as in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; to identify &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While state of the art distant supervision approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use offtheshelf named entity recognition and classiﬁcation systems to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relation arguments discrepancies in domain or genre between the data used for NERC training and the intended domain for the relation extractor can lead to low performance</example>
		<phraseLemma>np use np to identify np</phraseLemma>
	</can>
	<can>
		<phrase>As an example &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As an example&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the album relation has a Musical Artist and an Album as arguments</example>
		<phraseLemma>as a example np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is applicable to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our method does not rely on handlabeled training data and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is applicable to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any domain which is shown in our evaluation on different relations</example>
		<phraseLemma>np be applicable to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; consisting of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In general relations are of the form R consisting of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a subject a predicate and an object during training we only consider statements which are contained in a knowledge base ie ∈ KB</example>
		<phraseLemma>in np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is a collection of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Relation Extraction The input to the learning task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is a collection of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training examples for a speciﬁc relation</example>
		<phraseLemma>np be a collection of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As shown in Table 1 while supervised identiﬁcation of NE labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves a higher precision for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all NE types the recall is higher for all NE types using POSbased heuristics</example>
		<phraseLemma>np achieve np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are identiﬁed as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>MISC NE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are identiﬁed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PER LOC or ORG</example>
		<phraseLemma>np be identiﬁed as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are preprocessed with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the baselines with offtheshelf NECs sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are preprocessed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two NEC systems Stanford NER and FIGER</example>
		<phraseLemma>np be preprocess with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are added as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the Stanf baseline Stanford NER 1 class labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are added as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; RE features</example>
		<phraseLemma>np be add as np</phraseLemma>
	</can>
	<can>
		<phrase>Some of &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Some of the types are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relation types we evaluate</example>
		<phraseLemma>some of np be np</phraseLemma>
	</can>
	<can>
		<phrase>We also showed that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also showed that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; using Web features increases precision for NEC</example>
		<phraseLemma>we also show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Liu train a supervised ﬁne grained NERC on Wikipedia and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; those types as entity contraints improves precision and recall for a distantly supervised RE on newswire</example>
		<phraseLemma>np show that use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; independently of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On the other extreme one can tackle the task by identifying each element involved in a MOVELINK &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;independently of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the other elements</example>
		<phraseLemma>np independently of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is created from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This contrasts with previous applications of tree kernels where a structured feature &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is created from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a static parse subtree for extracting exactly two arguments involved in a relation</example>
		<phraseLemma>np be create from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are relevant to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Owing to space limitations we will only discuss those aspects &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are relevant to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SpaceEval tasks we focus on</example>
		<phraseLemma>np that be relevant to np</phraseLemma>
	</can>
	<can>
		<phrase>Different types of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Different types of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; spatial elements have different attributes</example>
		<phraseLemma>different type of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To understand why we can do this recall from Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; QSLINKs and OLINKs the trigger has to be a spatial signal element having a semantic type attribute</example>
		<phraseLemma>np that in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are subject to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The role labels assigned to the spatial elements in each triplet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are subject to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the following constraints</example>
		<phraseLemma>np be subject to np</phraseLemma>
	</can>
	<can>
		<phrase>To allow for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To allow for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; implicit participants from each training instance we have created thus far we create three additional training instances where exactly one of the three participants has the value IMPLICIT</example>
		<phraseLemma>to allow for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As in the LINK classiﬁer we enforce global role constraints &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when creating training instances for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MOVELINK classiﬁers</example>
		<phraseLemma>np when np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; assigned to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Speciﬁcally the roles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;assigned to the spatial elements in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each training instance of each MOVELINK classiﬁer are subject to six constraints</example>
		<phraseLemma>np assign to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is assigned to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As another example when more than one role &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is assigned to the same spatial element in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a MOVELINK it favors the role associated with the highest SVM conﬁdence</example>
		<phraseLemma>np be assign to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; Second for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First we identify the smallest subtree that covers all three spatial elements and call its root r &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Second for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each path from each spatial element to r we include in the parse subtree all the nodes that lie on the path and their immediate children</example>
		<phraseLemma>np 1 for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when training &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Recall that a structured feature is used as an additional feature &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when training&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each classiﬁer</example>
		<phraseLemma>np when training np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; containing &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To train a classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on instances containing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original features and SRCTs we employ SVMlightT K which trains an SVM classiﬁer using the features with a linear kernel trains an SVM classiﬁer using only the SRCTs with a convolution kernel and combines these two kernels using a composite kernel</example>
		<phraseLemma>np on np contain np</phraseLemma>
	</can>
	<can>
		<phrase>As we can see &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we can see&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this mistake is propagated to the SRCTs generated in later sieves</example>
		<phraseLemma>as we can see np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we employ &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In each fold experiment we employ&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three folds for training one fold for development and one fold for testing</example>
		<phraseLemma>in np we employ np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; without relying on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Two approaches mined social interaction networks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;without relying on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dialogue unlike the methods of Elson and He</example>
		<phraseLemma>np without rely on np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we follow &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each method we follow&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their procedures for identifying the characters in the social network which produces sets of one or more aliases associated with each identiﬁed character</example>
		<phraseLemma>for np we follow np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was run on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Character detection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was run on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same novels from Elson and we found no statisticallysigniﬁcant difference in the mean number of characters in urban and rural settings even when accounting for text size</example>
		<phraseLemma>np be run on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Similarly Gärtner developed graph kernels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on random walks and Srivastava used them on dependency trees with Vector Tree Kernels adding node similarity based on word embeddings from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SENNA and reporting improvements over SSTK</example>
		<phraseLemma>np base on np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; proposed &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>More recently for short text similarity Song and Roth and Kenter and de Rijke &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;proposed additional semantic metafeatures based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings to enhance classiﬁcation</example>
		<phraseLemma>np propose np base on np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluated &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluated our kernel with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; cooccurrence and syntactic phrases on several standard text categorization tasks</example>
		<phraseLemma>we evaluate np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; of 1</phrase>
		<frequency>10</frequency>
		<example>In all our experiments we used the FANSE parser to generate dependency trees and the pretrained version of word 1 vec 1 a dimensional representation of 1 million English words trained over a Google News dataset of billion words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the Skipgram model and a context size of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use np of 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; especially with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Overall we obtained better results than the ngram baselines DTK and VTK &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;especially with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntactic phrases</example>
		<phraseLemma>np especially with np</phraseLemma>
	</can>
	<can>
		<phrase>There was &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There was little contribution to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accuracy from nonunigram features indicating that large part of the performance improvement is credited to the word embedding resolving the sparsity issue</example>
		<phraseLemma>there be np to np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows the accuracy on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same test set 1 of the dataset when the learning was done on 1 to 1 of the training set 1 of the dataset for the bigram baseline and our bigram PK phrase kernel both with dependency tree representation on PL 1</example>
		<phraseLemma>figure 1 show np on np</phraseLemma>
	</can>
	<can>
		<phrase>We model &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We model the maximum ingredientinstruction word match using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a binary term vector whose vocabulary covers nondirective recipe words</example>
		<phraseLemma>we model np use np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to model &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a binary term vector to model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the nonﬁrst words in the instruction</example>
		<phraseLemma>we use np to model np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; about which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Considering that people often have some prior knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;about which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; potential topics should exist in given data we aim to incorporate such knowledge into the DPMM to improve text clustering</example>
		<phraseLemma>np about which np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our proposed TSDPMM model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document clustering on 1 datasets where each cluster corresponds to a topic</example>
		<phraseLemma>we evaluate np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are available at &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We implement both DPMM and TSDPMM models — their source codes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are available at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; https</example>
		<phraseLemma>np be available at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are denoted as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are denoted as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 N 1 1 N 1 1 N 1 and Reu Reu Reu respectively</example>
		<phraseLemma>np be denote as np</phraseLemma>
	</can>
	<can>
		<phrase>The results show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TSDPMM using prior topics learnt by DPMM outperforms DPMM</example>
		<phraseLemma>the result show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are not suitable for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However these structures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are not suitable for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; modeling sentences</example>
		<phraseLemma>np be not suitable for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are sent to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then the embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are sent to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst layer of GRNN as inputs whose outputs are recursively applied to upper layers until it outputs a single ﬁxedlength vector</example>
		<phraseLemma>np be send to np</phraseLemma>
	</can>
	<can>
		<phrase>The number of &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The number of the internal nodes is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 n in our model</example>
		<phraseLemma>the number of np be np</phraseLemma>
	</can>
	<can>
		<phrase>The analysis of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The analysis of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; temporal aspects is a practical problem as well as the process of largescale heterogeneous data since the WorldWide Web is widely used by various sorts of people</example>
		<phraseLemma>the analysis of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is done by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The earliest discussion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is done by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ML community in a NIPS 1 workshop and more recently transfer learning techniques have been successfully applied in many applications</example>
		<phraseLemma>np be do by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in our method</phrase>
		<frequency>10</frequency>
		<example>There are three novel aspects &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our method&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in we method</phraseLemma>
	</can>
	<can>
		<phrase>Similar to &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Similar to SVM for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; biasedSVM we used the ﬁrst two folds as a training data and classiﬁed test documents directly ie we used closed data</example>
		<phraseLemma>similar to np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Table 1 shows &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each value &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Table 1 shows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; macroaveraged error rate across years</example>
		<phraseLemma>np in table 1 show np</phraseLemma>
	</can>
	<can>
		<phrase>The results obtained by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results obtained by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; biasedSVM show minimum error rate obtained by varying the parameters c” and j”</example>
		<phraseLemma>the result obtain by np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; among &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are no signiﬁcant differences among&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three methods bSVM TrAdaB and TAB when the test and training data are the same time period</example>
		<phraseLemma>there be np among np</phraseLemma>
	</can>
	<can>
		<phrase>This was &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This was the same behaviour as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TrAdaBoost ie TrAdaBoost converges around iterations</example>
		<phraseLemma>this be np as np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we deﬁne &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each term t we deﬁne&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a quantity αt which is the difference between the contribution coming from t depending on whether ut = 1 or ut = 1</example>
		<phraseLemma>for np we deﬁne np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; more effectively</phrase>
		<frequency>10</frequency>
		<example>Thus we here propose a method that extracts strong themes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;more effectively&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np more effectively</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 compares &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 compares PAVEM with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LDA 1 for the MeSH termbased performance</example>
		<phraseLemma>table 1 compare np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; describing &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Based on a random sample of persons pages we noticed that only 1 of them includes a section called Biography or Life typically containing a set of subsections &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;describing the main&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; periods in a persons life from birth to death see for instance https</example>
		<phraseLemma>np describe np in np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we present &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we present&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a simple methodology that given a persons page in Wikipedia recognizes all sections that deal with his/her life even if no Biography section is present</example>
		<phraseLemma>in this work we present np</phraseLemma>
	</can>
	<can>
		<phrase>We derive &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We derive our development training and test data from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Pantheon data set freely available for download</example>
		<phraseLemma>we derive np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are included as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Therefore all subsections of Life or Biography &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are included as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive examples in the training set because we assume that they all have biographical content</example>
		<phraseLemma>np be include as np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; provided by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the implementation provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CRFsuite 1 for both training and classiﬁcation tasks</example>
		<phraseLemma>we use np provide by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provided by &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We use the implementation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provided by CRFsuite 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both training and classiﬁcation tasks</example>
		<phraseLemma>np provide by np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is available as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The ongoing work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is available as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an open source project on GitHub and is released under the GPLv 1 license</example>
		<phraseLemma>np be available as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; achieves &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Experimental results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that our algorithm achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 accuracy about 1 higher than the stateoftheart baseline</example>
		<phraseLemma>np show that np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are smaller than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally according to we generate features related to the raw path and dependence path between two numbers and use the numeric relation between them as a feature to import some basic arithmetic rules such as the positive summands &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are smaller than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their sum</example>
		<phraseLemma>np be smaller than np</phraseLemma>
	</can>
	<can>
		<phrase>We add &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We add at most false derivations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each word problem during the constraint generation step and use 1 fold crossvalidation to avoid overﬁtting</example>
		<phraseLemma>we add np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is much smaller than</phrase>
		<frequency>10</frequency>
		<example>We can observe that the number of possible alignments per word problem of our algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is much smaller than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be much smaller than</phraseLemma>
	</can>
	<can>
		<phrase>We can ﬁnd that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can ﬁnd that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the features are helpful to get the correct solution and none of them dramatically surpasses the others</example>
		<phraseLemma>we can ﬁnd that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is caused by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The ﬁrst problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is caused by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our feature for semantic representation</example>
		<phraseLemma>np be cause by np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we develop &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For phonetic information we develop&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an encoding scheme for Roman Urdu UrduPhone motivated from Soundex</example>
		<phraseLemma>for np we develop np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Firstly UrduPhone generates encoding of length six &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared to length 1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Soundex</example>
		<phraseLemma>np compare to np in np</phraseLemma>
	</can>
	<can>
		<phrase>Instead of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Instead of word IDs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a UrduPhone IDs or string similarity based cluster IDs can be used to reduce sparsity and improve matches among similar words</example>
		<phraseLemma>instead of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To overcome this shortcoming a line of research deploys the order information of the words in the contexts by either deriving the contexts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using dependency relations where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target word participates or directly keeping the order features</example>
		<phraseLemma>np use np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; giving &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Distinguished from English Chinese characters are logograms of which over 1 are phonosemantic compounds &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a semantic component giving&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a broad category of meaning and a phonetic component suggesting the sound</example>
		<phraseLemma>np with np give np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are built for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As the widely used public word similarity datasets like WS 1 RG 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are built for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English embeddings we start from developing appropriate Chinese synonym sets</example>
		<phraseLemma>np be build for np</phraseLemma>
	</can>
	<can>
		<phrase>It is observed that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is observed that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nearly 1 Chinese characters have only one component and 1 Chinese characters have two components</example>
		<phraseLemma>it be observe that np</phraseLemma>
	</can>
	<can>
		<phrase>To cope with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To cope with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the radical variation problem we transform radical variants to their origins such as 亻 to 人 to to and 辶 to</example>
		<phraseLemma>to cope with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is provided in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The complete list of the transformations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is provided in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Appendix for easy reference</example>
		<phraseLemma>np be provide in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; perform worse than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can see that both CBOW and CBOWbi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;perform worse than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corresponding SkipGram and SkipGrambi</example>
		<phraseLemma>np perform worse than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; take &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Recently a series of methods have been developed which train a classiﬁer for each label organize the classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a partially ordered structure and take&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predictions produced by the former classiﬁers as the latter classiﬁers features</example>
		<phraseLemma>np in np take np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>It means that we only model the dependencies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between the ﬁrst label and the 1 label and that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dependencies between the 1 label and the 1 label is missing</example>
		<phraseLemma>np between np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It means &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that we only model the dependencies between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst label and the 1 label and that the dependencies between the 1 label and the 1 label is missing</example>
		<phraseLemma>np that np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is computed with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In BTS parent node &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is computed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its child nodes at the forward pass stage child node receives gradient as the sum of derivatives from all its parents</example>
		<phraseLemma>np be compute with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; in table 1</phrase>
		<frequency>10</frequency>
		<example>We reports the detailed results in terms of different evaluation metrics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on different data sets in table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; share &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In fact BR the original predictionasfeatures methods and our proposed methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;share similar performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Enron data set</example>
		<phraseLemma>np share np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can improve the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The results indicates that our proposed joint algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can improve the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the predictionsasfeatures methods</example>
		<phraseLemma>np can improve the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; sets &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While there is some previous work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the comparison of document sets&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these existing approaches lack the generality to be widely applicable across different use case scenarios with different comparison criteria</example>
		<phraseLemma>np on np set np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was proposed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This result is consistent with history as the National Nanotechnology Initiative &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was proposed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the late 1 s to promote nanotechnology R&amp;D 1 One could also measure such trends using budglocations by incorporating the award amounts into the edge weights of G</example>
		<phraseLemma>np be propose in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; was computed</phrase>
		<frequency>10</frequency>
		<example>These programs were manually judged for relatedness and the Mean Average Precision a standard performance metric for ranking tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in information retrieval was computed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np be compute</phraseLemma>
	</can>
	<can>
		<phrase>This paper proposes &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This paper proposes the novel C 1 EL framework for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; jointly modeling crossdocument coreference resolution and linkage of mention groups to entities in a knowledge base</example>
		<phraseLemma>this paper propose np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to minimize &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The intuition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to minimize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; potential noise from overly speculative mappings to the KB at this initial stage</example>
		<phraseLemma>np be to minimize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are appended to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For mention groups placed in SE the KB features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are appended to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their context summaries while mentions strongly linked to same KB entities are considered to be coreferring and hence grouped together</example>
		<phraseLemma>np be append to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is repeated until &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This process of alternating CCR and NEL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is repeated until&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all mention groups are strongly connected to KB entities or no changes are made anymore</example>
		<phraseLemma>np be repeat until np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are computed based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To infer whether two mention groups represent the same entity the similarity between the context summaries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are computed based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tfidf weighted bagofwords cosine distance and partialmatch scores of multiword keyphrases in bounded text windows</example>
		<phraseLemma>np be compute base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that go beyond &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>CCR aims to process heterogeneous corpora &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that go beyond&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single domain and style such as Web collections</example>
		<phraseLemma>np that go beyond np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is marked as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Mi is concatenated with the best matching entity Mk if the similarity score is above a threshold θ else Mi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is marked as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an outofKB entity and is placed in the N E class</example>
		<phraseLemma>np be mark as np</phraseLemma>
	</can>
	<can>
		<phrase>We analyze &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We analyze the individual gains in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CCR and NEL due to the joint modeling</example>
		<phraseLemma>we analyze np in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we observed &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For both procedures we observed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; statistically signiﬁcant improvements of C 1 EL over BER and NECo with p &amp;lt 1 using the bootstrap resampling ttest</example>
		<phraseLemma>for np we observe np</phraseLemma>
	</can>
	<can>
		<phrase>We observe &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We observe nearly 1 reduction of precision in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; identiﬁcation of outofKB entitymentions compared to C 1 EL</example>
		<phraseLemma>we observe np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since no feature inclusion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; outofKB mentions no effect is observed</example>
		<phraseLemma>np be perform for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to introduce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>One approach to performing inference over such a large space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to introduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; compact representations that are able to encode exponentially many mentions that would enable tractable inference algorithms to be employed</example>
		<phraseLemma>np be to introduce np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is deﬁned as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our task the F measure is deﬁned as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the harmonic mean of the precision and recall scores where precision is the ratio between the number of correctly predicted mentions and the total number of predicted mentions and recall is the ratio between the number of correctly predicted mentions and the total number of gold mentions</example>
		<phraseLemma>in np be deﬁned as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the total number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In our task the F measure is deﬁned as the harmonic mean of the precision and recall scores where precision is the ratio between the number of correctly predicted mentions and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the total number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predicted mentions and recall is the ratio between the number of correctly predicted mentions and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the total number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; gold mentions</example>
		<phraseLemma>np the total number of np</phraseLemma>
	</can>
	<can>
		<phrase>The results are shown in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results are shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the row of CRF ” 1 Another class of models that is often used in information extraction are the semiMarkov conditional random ﬁelds</example>
		<phraseLemma>the result be show in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are reported in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The results of these two models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are reported in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the fourth and ﬁfth row of Table 1 respectively</example>
		<phraseLemma>np be report in np</phraseLemma>
	</can>
	<can>
		<phrase>We are not aware of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We are not aware of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any prior work in the literature that performs joint modeling of mention boundaries types and heads</example>
		<phraseLemma>we be not aware of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; generates &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It does not rely on supervision &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in its extractors and generates&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training data for type selection from WordNet and other resources</example>
		<phraseLemma>np in np generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is comparable to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This extractor &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is comparable to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KB methods discussed above but is unsupervised and takes as candidates the types frequent within the related entities and contexts</example>
		<phraseLemma>np be comparable to np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; occurs in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If a mention of a location occurs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the repository we add all the corresponding types from the repository to the candidate set and move to the type selection phase</example>
		<phraseLemma>if np occur in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which depend on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To exploit it we apply a set of morphological transformations to the verb &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which depend on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the grammatical function of the entity ie subject or object</example>
		<phraseLemma>np which depend on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compatible with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As stated above the set of argument types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compatible with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a verb is limited</example>
		<phraseLemma>np compatible with np</phraseLemma>
	</can>
	<can>
		<phrase>To construct &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To construct a query for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a given entity mention we focus on the part of the sentence directly related to the entity</example>
		<phraseLemma>to construct np for np</phraseLemma>
	</can>
	<can>
		<phrase>The context for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The context for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each individual WordNet type consists of all words appearing in the types gloss and the glosses of its neighbors</example>
		<phraseLemma>the context for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; containing &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally we add all words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in sentences containing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the type in SemCor 1 and Ontonotes 1</example>
		<phraseLemma>np in np contain np</phraseLemma>
	</can>
	<can>
		<phrase>We ran &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ran Pearl in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its hard setting which performed best</example>
		<phraseLemma>we run np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which provides &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We created two datasets New York Times and Twitter and sampled a subset of the CoNLL data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which provides gold annotations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CG types</example>
		<phraseLemma>np which provide np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is consistent with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If NER is highly conﬁdent on its outputs of entity boundaries and types it will encourage entity linking to link an entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NERs outputs and vice versa</example>
		<phraseLemma>np which be consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is proposed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Along with the improvement of Machine Learning techniques statistical approaches have become a major direction for research on NER especially after Conditional Random Field &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is proposed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Lafferty</example>
		<phraseLemma>np be propose by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is described in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Sils work on jointly NER and linking &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the introduction section of this paper</example>
		<phraseLemma>np be describe in np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; with &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use conditional log likelihood with 1 norm as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the objective function in training</example>
		<phraseLemma>we use np with np as np</phraseLemma>
	</can>
	<can>
		<phrase>To the best of our knowledge &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To the best of our knowledge this&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; data set is the biggest data set which has been labeled for both NER and linking tasks</example>
		<phraseLemma>to the best of we knowledge np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are relevant to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Features in linking category &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are relevant to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entity linking</example>
		<phraseLemma>np be relevant to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also known as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This index is used to select entity candidates for any word sequence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also known as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; surface form</example>
		<phraseLemma>np also know as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are compared with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For entity linking systems NereL Kul and Hof &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are compared with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our models</example>
		<phraseLemma>np be compare with np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows the performance of &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the performance of different NER systems on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CoNLL 1 testb data set</example>
		<phraseLemma>table 1 show the performance of np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; to generate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>NereL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses its overgeneration techniques to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mention candidates and outperforms Hoff in both precision and recall</example>
		<phraseLemma>np use np to generate np</phraseLemma>
	</can>
	<can>
		<phrase>To make use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To make use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; arithmetic coding we consider the task of predicting the next target character given the source sentence and target string so far</example>
		<phraseLemma>to make use of np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; we consider &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As candidates we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; target words that can be produced via the current ttable from any source words with probability greater than 1</example>
		<phraseLemma>as np we consider np</phraseLemma>
	</can>
	<can>
		<phrase>To convert &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To convert word predictions into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; character predictions we combine scores for words that share the next character</example>
		<phraseLemma>to convert np into np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; monolingual PPM compresses the Spanish side of our corpus to 1 of the original</example>
		<phraseLemma>figure 1 show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is characterized by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Monolingual PPM compresses to 1 while our HMMbased bilingual compression compresses to 1 1 We can say that a human translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is characterized by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an additional 1 bits per byte on top of the original rather than the 1 bits per byte we Figure</example>
		<phraseLemma>np be characterize by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; yielded &lt;NP&gt; of 1</phrase>
		<frequency>10</frequency>
		<example>Monolinguals guessing on the same data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;yielded an upperbound bpb of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np yield np of 1</phraseLemma>
	</can>
	<can>
		<phrase>To work with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To work with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reallife bilingual corpora the compressor should take care of segment alignment in a way that allows decompression back to the original text</example>
		<phraseLemma>to work with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; back to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To work with reallife bilingual corpora the compressor should take care of segment alignment in a way that allows decompression &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;back to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original text</example>
		<phraseLemma>np back to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learnt in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In their method the paragraph vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learnt in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a similar way of word vector model and there will be N × P parameters if there are N paragraphs and each paragraph is mapped to P dimensions</example>
		<phraseLemma>np be learn in np</phraseLemma>
	</can>
	<can>
		<phrase>To model &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To model the coherence of sentences in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document D which contains N sentences S 1 S 1</example>
		<phraseLemma>to model np in np</phraseLemma>
	</can>
	<can>
		<phrase>From Table 1 we can see that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From Table 1 we can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the maximum entropy model and the recursive neural network model has almost the same performance</example>
		<phraseLemma>from table 1 we can see that np</phraseLemma>
	</can>
	<can>
		<phrase>In order to learn &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation coherence between sentences we apply the HRNNLM to machine translation reranking task</example>
		<phraseLemma>in order to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also train a decoder &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is an inhouse Bracketing Transduction Grammar in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a CKYstyle decoder with a lexical reordering model trained with maximum entropy</example>
		<phraseLemma>np which be np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; capturing &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We hope HRNNLM will enable a contextsensitive reranking process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;capturing the syntactic and logic relationships between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentences in the same document</example>
		<phraseLemma>np capture np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are often used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Language models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are often used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural language processing tasks involving generation of text</example>
		<phraseLemma>np be often use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are computed as follows</phrase>
		<frequency>10</frequency>
		<example>The activations at each layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are computed as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be compute as follow</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; differ only in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Clearly w and v must have the same direction and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;differ only in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; magnitude that is w = α v</example>
		<phraseLemma>np differ only in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provide &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>They reformulate ∞ 1 regularization as a constrained optimization problem and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provide a solution in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; O time</example>
		<phraseLemma>np provide np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with dimensions</phrase>
		<frequency>10</frequency>
		<example>We use a vocabulary size of 1 k and word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with dimensions&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with dimension</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; introduce &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We will review graphical models over strings in section 1 and brieﬂy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;introduce our sample problem in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; section 1</example>
		<phraseLemma>np introduce np in np</phraseLemma>
	</can>
	<can>
		<phrase>To perform &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To perform inference on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a graphical model one ﬁrst converts the model to a factor graph representation</example>
		<phraseLemma>to perform np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; leads to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Combining many cross products as BP iteratively passes messages along a path &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the factor graph leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; blowup that is exponential in the length of the path—which in turn is unbounded if the graph has cycles as ours do</example>
		<phraseLemma>np in np lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; corresponding to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>So when F ∈ F scores a given dtuple of string values it may accept that dtuple along multiple different WFSM paths &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with different scores corresponding to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different alignments of the strings</example>
		<phraseLemma>np with np correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To handle the above challenges without approximation we want to decompose the original problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into subproblems where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each subproblem can be solved efﬁciently</example>
		<phraseLemma>np into np where np</phraseLemma>
	</can>
	<can>
		<phrase>The solution to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The solution to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; achieves the same value in yet may do even better by considering solutions that do not satisfy the constraint</example>
		<phraseLemma>the solution to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; consisting of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given the Lagrange multipliers subproblem k of is simply MAP inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the factor graph consisting of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the variables X k and factors F k as well as an extra unary factor Gk at each Xi ∈ X k i These unary factors penalize strings according to the Lagrange multipliers</example>
		<phraseLemma>np on np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are widely used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Block coordinate updates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are widely used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Lagrangian relaxation and have also been explored speciﬁcally for dual decomposition</example>
		<phraseLemma>np be widely use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is connected to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each underlyingmorpheme variable &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is connected to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a copy of Mφ which gives the prior distribution over its values</example>
		<phraseLemma>np be connect to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we discussed &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In section 1 we discussed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a block coordinate update variation of our DD algorithm</example>
		<phraseLemma>in np we discuss np</phraseLemma>
	</can>
	<can>
		<phrase>The experiments show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The experiments show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our DD algorithm converges and gets better results than both maxproduct and sumproduct BP</example>
		<phraseLemma>the experiment show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; since &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Discourse in multiparty dialogues dramatically differs from monologues &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;since threaded conversations are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; commonplace rendering prediction of the discourse structure compelling</example>
		<phraseLemma>np since np be np</phraseLemma>
	</can>
	<can>
		<phrase>We achieve &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We achieve 1 F 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unlabelled structures and 1 F 1 on fully labeled structures which is better than many state of the art systems for monologues despite the inherent difﬁculties that multiparty chat dialogues have</example>
		<phraseLemma>we achieve np on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we describe &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the following section we describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Settlers of Catan game and our corpus in more detail and discuss some problematic structures for discourse parsing from our corpus</example>
		<phraseLemma>in np we describe np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we present &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In section 1 we present&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our parsing approach which consists of building a local probability distribution model which serves as input to a series of decoder mechanisms</example>
		<phraseLemma>in np we present np</phraseLemma>
	</can>
	<can>
		<phrase>We transform &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We transform our full graphs into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependency struc tures with V 1 ⊂ V the set of EDUs in V by replacing any attachment to a CDU with an attachment to the CDUs head—the textually ﬁrst EDU within the CDU which has no incoming links</example>
		<phraseLemma>we transform np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is encoded as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each pair of EDUs p &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is encoded as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a vector of m indicator features fi</example>
		<phraseLemma>np be encode as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; imposes &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus the nature of dialogue &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;imposes an essential and important constraint on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the attachment process that is not present for monologue or singleauthored text where an EDU may be dependent upon any EDU later in the ordering or not</example>
		<phraseLemma>np impose np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is attached to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also used the baseline L AST where each EDU &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is attached to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the immediately preceding EDU in the linear textual order</example>
		<phraseLemma>np be attach to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As we have said our results better theirs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; attachment and full labeled structures</example>
		<phraseLemma>np both on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Li use both the Eisner algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as the MST algorithm from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; McDonald</example>
		<phraseLemma>np as well as np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; focuses on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Most work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on discourse parsing focuses on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of discourse relation labeling between pairs of discourse units—eg Marcu and Echihabi Sporleder and Lascarides and Lin</example>
		<phraseLemma>np on np focus on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we focus on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our discussion of related work we focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the three subtasks addressed in this paper</example>
		<phraseLemma>in np we focus on np</phraseLemma>
	</can>
	<can>
		<phrase>The relations in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The relations in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tree are neither labeled not directed</example>
		<phraseLemma>the relation in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; proved to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the automation of the structure building however the segmentwise classiﬁcation of attachment with only a small context window around the target segment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;proved to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a very hard task</example>
		<phraseLemma>np prove to be np</phraseLemma>
	</can>
	<can>
		<phrase>This is due to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the longdistance dependencies frequently found in argumentation graphs</example>
		<phraseLemma>this be due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as described in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We train base classiﬁers for the role function and central claim level &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the same learning regime as described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np use np as describe in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we apply &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For signiﬁcance testing we apply&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Wilcoxon signedrank test on the macro averaged scores and assume a signiﬁcance level of α= 1</example>
		<phraseLemma>for np we apply np</phraseLemma>
	</can>
	<can>
		<phrase>We have shown that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We have shown that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; joint prediction of all levels in the evidence graph models helps to improve the classiﬁcation on single levels</example>
		<phraseLemma>we have show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; helps to improve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We have shown that joint prediction of all levels in the evidence graph models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;helps to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁcation on single levels</example>
		<phraseLemma>np help to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to assign &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus without loss of generality we formulate alignment as a binary classiﬁcation task where given all word pairs across two sentences the goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to assign&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each a class label in</example>
		<phraseLemma>np be to assign np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based solely on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To handle such interdependencies we employ a twostage logistic regression model – stage 1 computes an alignment probability for each word pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based solely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its own feature values and stage 1 assigns the eventual alignment labels to all pairs following a comparative assessment of stage 1 probabilities of cooperating and competing pairs</example>
		<phraseLemma>np base solely on np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows a shortened version of sentence pair in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MSR alignment corpus dev set with related units aligned by human annotators</example>
		<phraseLemma>figure 1 show np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be extended to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Named entities are the only phrasal units we consider for alignment in a later section we discuss how our techniques &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be extended to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; general phrasal alignment</example>
		<phraseLemma>np can be extend to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained on &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our feature is the output of a ridge regression model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained on human annotations of word similarity with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two features</example>
		<phraseLemma>np train on np with np</phraseLemma>
	</can>
	<can>
		<phrase>We draw &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We draw several contextual features from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nearby words of T and T ij in the surface forms of T and T</example>
		<phraseLemma>we draw np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is reported on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each pair is aligned by three human annotators Fleiss Kappa agreement of about 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is reported on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both sets</example>
		<phraseLemma>np be report on np</phraseLemma>
	</can>
	<can>
		<phrase>These results show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;These results show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stage 1 is central to the aligners success</example>
		<phraseLemma>these result show that np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows the 1 distribution of word pair types with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; at least aligned instances in at least one test set</example>
		<phraseLemma>figure 1 show np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are embedded in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moreover since predictions and inputs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are embedded in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same vector space and features extracted from inputs and outputs are decoupled our approach is amenable to joint learning of multiple annotation conventions such as PropBank and FrameNet in a single model</example>
		<phraseLemma>np be embed in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; features extracted from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Moreover since predictions and inputs are embedded in the same vector space and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;features extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inputs and outputs are decoupled our approach is amenable to joint learning of multiple annotation conventions such as PropBank and FrameNet in a single model</example>
		<phraseLemma>np feature extract from np</phraseLemma>
	</can>
	<can>
		<phrase>We also evaluate &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also evaluate our system on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dependencybased CoNLL 1 shared task by assuming single word argument spans that represent semantic dependencies and limit our experiments to English</example>
		<phraseLemma>we also evaluate np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; representing &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>There have been several recent studies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;representing input observations and output labels with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distributed representations for example in the WSABIE model for image annotation in models for embedding labels in struc tured graphical models and in techniques to learn joint embeddings of predicate words and their semantic frames in a vector space</example>
		<phraseLemma>np represent np with np</phraseLemma>
	</can>
	<can>
		<phrase>Our model for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SRL performs inference separately for each marked predicate in a sentence</example>
		<phraseLemma>we model for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus our algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs the SRL task in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one step for a single predicate frame</example>
		<phraseLemma>np perform np in np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; similar to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a baseline similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; most prior work one could use a simple linear function of discrete input features denotes a feature function</example>
		<phraseLemma>as np similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; treat &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the dependencybased CoNLL 1 experiments we modify our approach to assume single length spans and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;treat every word of the sentence as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a candidate argument</example>
		<phraseLemma>np treat np as np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows example embeddings from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model trained jointly on FrameNet and PropBank annotations</example>
		<phraseLemma>figure 1 show np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the graph</phrase>
		<frequency>10</frequency>
		<example>Using this PSL model we learn rule weights with a small amount of training data and then perform joint inference over all hypernymy links &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the graph&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the graph</phraseLemma>
	</can>
	<can>
		<phrase>The authors of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The authors of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the HARPY system argue that the sparseness of PATTYs graph comes from the lack of general phrases in the source corpus</example>
		<phraseLemma>the author of np</phraseLemma>
	</can>
	<can>
		<phrase>Our approach is based on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our approach is based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; probabilistic soft logic a popular statistical relational learning system which we brieﬂy describe here</example>
		<phraseLemma>we approach be base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consists of &lt;NP&gt; extracted from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The input to our system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consists of 1 relational phrases and the associated argument types extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Englishlanguage Wikipedia website using the PATTY system</example>
		<phraseLemma>np consist of np extract from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; evaluated &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>During graph inference RELLY &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;evaluated 1 M possible hypernymy links using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 M ground logical rules and constraints</example>
		<phraseLemma>np evaluate np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; had &lt;NP&gt; of 1</phrase>
		<frequency>10</frequency>
		<example>Separately the precision@ dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;had Cohens Kappa of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; and the randomly sampled dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;had Cohens Kappa of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np have np of 1</phraseLemma>
	</can>
	<can>
		<phrase>We compared the performance of &lt;NP&gt; against &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compared the performance of PSL against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Integer Linear Programming formulation by</example>
		<phraseLemma>we compare the performance of np against np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is compared to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We consider a task where an input query document &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a corpus of documents with the aim of ﬁnding the most relevant related documents</example>
		<phraseLemma>np be compare to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are represented by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the bigram model documents are represented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vectors in the bagofbigrams model with bigram frequency weights</example>
		<phraseLemma>in np be represent by np</phraseLemma>
	</can>
	<can>
		<phrase>Were also used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Relations with semantic types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were also used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; typed entailment graphs</example>
		<phraseLemma>np be also use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is encoded in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The common sense knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is encoded in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 model which can for example prefer graphs that model implicit verb arguments when they better match the learned selectional preferences</example>
		<phraseLemma>np be encode in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that models &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally we will show how given a recipe and a set of connections we can construct an action graph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that models&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬂow of ingredients through the recipe</example>
		<phraseLemma>np that model np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; introduces &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>If a span &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;introduces raw ingredient or new location into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the recipe then o = 1 in Fig 1 this occurs for each of the spans that represent raw ingredients as well as oven” and into loaf pan”</example>
		<phraseLemma>np introduce np into np</phraseLemma>
	</can>
	<can>
		<phrase>If for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; example gi has leaf= true then the origin of s must be 1</example>
		<phraseLemma>if for np</phraseLemma>
	</can>
	<can>
		<phrase>We assume that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We assume that a destination subset is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a list of connections</example>
		<phraseLemma>we assume that np be np</phraseLemma>
	</can>
	<can>
		<phrase>The probability of &lt;NP&gt; given &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The probability of a recipe R given&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of connections C can be factored by the chain rule</example>
		<phraseLemma>the probability of np give np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that can be applied to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We score potential local search operators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that can be applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current set of connections C and make a greedy selection that improves P the most until no search operator can improve the probability</example>
		<phraseLemma>np that can be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is initialized by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The verb signature model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is initialized by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁrst identifying f ood arguments using string overlap with the ingredient list</example>
		<phraseLemma>np be initialize by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was assumed to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The ﬁrst verb in each recipe &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was assumed to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the only leaf</example>
		<phraseLemma>np be assume to be np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we introduce &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we introduce a novel framework for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic representation and computational analysis of the structure of comparison in natural language</example>
		<phraseLemma>in this paper we introduce np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; having &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Measurement in natural language is mainly expressed in sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;having comparative morphemes such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more less er as too enough est etc</example>
		<phraseLemma>np have np such as np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For this purpose we train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a multiclass classiﬁer that labels all oneword constituents in the sentence with any of predicate types in Table 1 or None</example>
		<phraseLemma>for np we train np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we collect &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the literal sense we collect&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets that contain the target word and are not labeled with the #sarcastic or #sarcasm hashtags</example>
		<phraseLemma>for np we collect np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared to &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our results support this argument 1 F 1 measure for the best result for S vs L &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared to 1 F 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the best result for S vs Lsent Section 1 1</example>
		<phraseLemma>np compare to np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the majority of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For instance we can align the and where except for the words happy and unhappy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the majority of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the words in the two messages are anchor words and thus happy and unhappy can be extracted as paraphrases via cotraining</example>
		<phraseLemma>np the majority of np</phraseLemma>
	</can>
	<can>
		<phrase>As we will see in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we will see in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the results sections however the size of the training data is not always the key factor in the LSSD task especially for the methods that use word embeddings</example>
		<phraseLemma>as we will see in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are then applied to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Geometric techniques such as cosine similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are then applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these vectors to measure the similarity in meaning of corresponding words</example>
		<phraseLemma>np be then apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We sorted the context words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the PPMI scores and for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each target word t we selected a maximum of 1 context words per sense to approximate the two senses of the target word</example>
		<phraseLemma>np base on np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; us using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We propose a new kernel kernelwe to compute the semantic similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between two tweets ur and us using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the M V M Ewe method introduced for the DSM approach and the three types of word embeddings</example>
		<phraseLemma>np between np we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showing &lt;NP&gt; of 1</phrase>
		<frequency>10</frequency>
		<example>Similarly for S vs L task F 1 scores reported by the kernel using word 1 vec embeddings are in the range of 1 1 compared to 1 given by the SV Mbl &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showing an absolute increase of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np show np of 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are also used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Word embedding based models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are also used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other NLP tasks such as dependency parsing semantic role labeling POS tagging NER questionanswering and our work on LSSD is a novel application of word embeddings</example>
		<phraseLemma>np be also use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed that &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We compared several distributional semantics methods and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed that using word embeddings in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a modiﬁed SVM kernel achieves the best results</example>
		<phraseLemma>np show that np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; useful for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We want to investigate further the nature and size of training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;useful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the LSSD task</example>
		<phraseLemma>np useful for np</phraseLemma>
	</can>
	<can>
		<phrase>For cases where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For cases where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all 1 coders agree none of them were considered sarcastic while when only 1 coders agree 1 tweet out of was considered sarcastic</example>
		<phraseLemma>for case where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; next to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Second we identify synonyms of two terms whose taxonomic relation is being scrutinized by matching such queries as t 1 also known as” against the Web to ﬁnd t 1 s synonyms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;next to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the query matches</example>
		<phraseLemma>np next to np</phraseLemma>
	</can>
	<can>
		<phrase>To estimate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To estimate the optimal combination for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parameters α β γ and δ we apply linear regression algorithm</example>
		<phraseLemma>to estimate np for np</phraseLemma>
	</can>
	<can>
		<phrase>If the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Web search results is too large we use only the ﬁrst 1 results to estimate the average trustiness score</example>
		<phraseLemma>if the number of np</phraseLemma>
	</can>
	<can>
		<phrase>We compare our approach with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our approach with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other three stateoftheart methods in the literature ie and</example>
		<phraseLemma>we compare we approach with np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we use &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For Animal Plant and Vehicle domains we use WordNet as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the gold standards whereas for Virus domain we use MeSH subhierarchy of virus as the reference</example>
		<phraseLemma>for np we use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that hold between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>LOGICIA has knowledge” about a ﬁxed set of predicates which models different relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that hold between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entities in a puzzle world</example>
		<phraseLemma>np that hold between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However the underlying semantics of such relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be deep such as the one in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; clue 1 of Problem</example>
		<phraseLemma>np can be np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between the features</phrase>
		<frequency>10</frequency>
		<example>Maximum entropy classiﬁer has been successfully applied in many natural language processing applications and allows the inclusion of various sources of information without necessarily assuming any independence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between the features&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np between the feature</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; correspond to &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this model the conditional probability distribution is given by where the denominator is the normalization term and the parameter λi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;correspond to the weight for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the feature fi</example>
		<phraseLemma>np correspond to np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be found at</phrase>
		<frequency>10</frequency>
		<example>A detailed description of the model or parameter estimation method used Generalized Iterative Scaling &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be found at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np can be find at</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; that contain &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For puzzles that contain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only one category consisting of numeric elements the translation module goes with the obvious choice</example>
		<phraseLemma>for np that contain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contain a total of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The set of puzzles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contain a total of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; clues with words</example>
		<phraseLemma>np contain a total of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to extract &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Word alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation rules in various way such as the phrase pairs used in a phrasebased SMT system the hierarchical rules used in a HIERO system and the sophisticated translation templates used in treebased SMT systems</example>
		<phraseLemma>np be use to extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is more than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It has been reported that the fast align approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is more than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; times faster than baseline GIZA with comparable results in endtoend French Chinese and ArabictoEnglish translation experiments</example>
		<phraseLemma>np be more than np</phraseLemma>
	</can>
	<can>
		<phrase>We adopt &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We adopt the REV method in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KatzBrown and Collins considering it is the simplest and lightest prereordering approach which may bring a minimal effect on the efﬁciency of fast align</example>
		<phraseLemma>we adopt np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is generated by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>An example seg rev process where the word alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is generated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fast align is illustrated in Fig 1</example>
		<phraseLemma>np be generate by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is illustrated in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>An example seg rev process where the word alignment is generated by fast align &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is illustrated in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fig 1</example>
		<phraseLemma>np be illustrate in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was used to tune &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>MERT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was used to tune&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; development set parameter weights and BLEU was used on test sets to evaluate the translation performance</example>
		<phraseLemma>np be use to tune np</phraseLemma>
	</can>
	<can>
		<phrase>With regard to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;With regard to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; processing time a na¨ıve singlethread implementation of seg rev in C took approximately 1 s / 1 s in the ﬁrst / 1 application on the entire JapaneseEnglish corpus</example>
		<phraseLemma>with regard to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is evaluated in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This proposal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is evaluated in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an N best rescoring step using the framework of ngrambased systems within which they integrate seamlessly</example>
		<phraseLemma>np be evaluate in np</phraseLemma>
	</can>
	<can>
		<phrase>The estimation of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The estimation of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ngram probabilities can be performed via multilayer NN structures as described in for a monolingual language model</example>
		<phraseLemma>the estimation of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; as in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The choice of a ranking loss seems to be the most appropriate &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our setting as in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many recent studies on discriminative training for MT the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance</example>
		<phraseLemma>np in np as in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; due to the fact that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Arguably the difference between the two conditions is much smaller than what was observed with the TED Talks task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;due to the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Patent</example>
		<phraseLemma>np due to the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; along with &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We plan a systematic comparison between these two criteria &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;along with some other discriminative losses in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a future work</example>
		<phraseLemma>np along with np in np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we have proposed &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we have proposed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a new discriminative training procedure for continuousspace translation models which correlates better with translation quality than conventional training methods</example>
		<phraseLemma>in this paper we have propose np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we adopt &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our system we adopt&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TERp one of the stateoftheart alignment tools to serve this purpose</example>
		<phraseLemma>in np we adopt np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; such as and</phrase>
		<frequency>10</frequency>
		<example>That is because of the use of the rules with higher degree of structural consensus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;such as and&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np such as and</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; which is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Arch_LD_PARA represents that For each MT system from the selected top 1 LD and PARA are both carried out &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using hybrid system AE we paraphrase its translations using combination architecture which is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the example the paraphrasing model and lattice decoding sepshown in Figure 1</example>
		<phraseLemma>np use np which be np</phraseLemma>
	</can>
	<can>
		<phrase>We view &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We view MT combination as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a paraphrasing process using a set of hierarchical paraphrases in which more complicated paraphrasing phenomena are able to be modeled such as phrasal and structural consensus</example>
		<phraseLemma>we view np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>We view MT combination as a paraphrasing process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using a set of hierarchical paraphrases in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more complicated paraphrasing phenomena are able to be modeled such as phrasal and structural consensus</example>
		<phraseLemma>np use np in which np</phraseLemma>
	</can>
	<can>
		<phrase>Using &lt;NP&gt; to Augment &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Using TERp to Augment&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the System Combination for SMT</example>
		<phraseLemma>use np to augment np</phraseLemma>
	</can>
	<can>
		<phrase>We assume that &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We assume that each sentence fi has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a known set Di of domains which identify the genre and individual document origin of the sentence</example>
		<phraseLemma>we assume that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; similar to</phrase>
		<frequency>10</frequency>
		<example>Our adaptation schema is an extension of frustratingly easy domain adaptation to multiple domains &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with diﬀerent regularization parameters similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np similar to</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that applies to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The idea is that we simultaneously maintain a generic set of weights &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that applies to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all domains as well as their domainspeciﬁc oﬀsets” describing how a domain diﬀers from the generic case</example>
		<phraseLemma>np that apply to np</phraseLemma>
	</can>
	<can>
		<phrase>We trained &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We trained a standard 1 gram language model with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; modiﬁed KneserNey smoothing using the KenLM toolkit on 1 billion running words</example>
		<phraseLemma>we train np with np</phraseLemma>
	</can>
	<can>
		<phrase>We trained &lt;NP&gt; with &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We trained a standard 1 gram language model with modiﬁed KneserNey smoothing using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the KenLM toolkit on 1 billion running words</example>
		<phraseLemma>we train np with np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used a combination of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Recent bestperforming metrics in the WMT 1 metric shared task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used a combination of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different metrics</example>
		<phraseLemma>np use a combination of np</phraseLemma>
	</can>
	<can>
		<phrase>The selection of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The selection of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WMT 1 corpus over other WMT workshops is motivated by the fact that it is the largest among them</example>
		<phraseLemma>the selection of np</phraseLemma>
	</can>
	<can>
		<phrase>The scores in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The scores in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bold show best scores overall and the scores in bold italic show best scores in our variants</example>
		<phraseLemma>the score in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; longer than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For the sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;longer than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words the system uses scores of LSick and scores of LSick for the rest</example>
		<phraseLemma>np longer than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a machine learning algorithm along with quality labels given by humans to learn models to predict the quality of unseen translations</example>
		<phraseLemma>np be use by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to determine &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A key factor for quality inference of a translated text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to determine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬂuency of such a text and how well it conforms to the linguistic regularities of the target language</example>
		<phraseLemma>np be to determine np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to select &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used a data selection method to select&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most relevant training data with respect to a development set</example>
		<phraseLemma>we use np to select np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by adding &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For these two tasks we are able to achieve stateoftheart performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by adding the two CSLM features to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all available or selected feature sets</example>
		<phraseLemma>np by add np to np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the official results from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the shared task those from training an SVR on these features with and without CSLM features</example>
		<phraseLemma>table 1 show np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally results comparing all official WMT 1 QE feature sets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed significant improvements in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the predictions when CSLM features were added to those submitted by participating teams</example>
		<phraseLemma>np show np in np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we develop &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we develop&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a supervised learning technique that improves noisy phrase translation scores obtained by phrase table triangulation</example>
		<phraseLemma>in this paper we develop np</phraseLemma>
	</can>
	<can>
		<phrase>By representing &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By representing words as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; embeddings our model can generalize the information contained in the sourcetarget data to a much larger vocabulary and can assign lexicalweighting probabilities to most of the phrase pairs in T</example>
		<phraseLemma>by represent np as np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to estimate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used the skipgram model to estimate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Spanish and French word embeddings and set the dimension to d = 1 and context window to w = 1</example>
		<phraseLemma>we use np to estimate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; independently from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we argued that constructing a triangulated phrase table &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;independently from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; even very limited sourcetarget data underutilizes that parallel data</example>
		<phraseLemma>np independently from np</phraseLemma>
	</can>
	<can>
		<phrase>This research was supported in part by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This research was supported in part by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Google Faculty Research Award to Chiang</example>
		<phraseLemma>this research be support in part by np</phraseLemma>
	</can>
	<can>
		<phrase>This work focuses on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This work focuses on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of ﬁnding latent vector representations of the words in a corpus</example>
		<phraseLemma>this work focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in case &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We could not exactly reproduce Guos result with the code we were provided so we report all results from our use of the provided code &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in case&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some parameter settings are different from those used in Guos paper</example>
		<phraseLemma>np in case np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to evaluate &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used wordvectorsorg to evaluate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our learned vectors on a variety of Englishlanguage word similarity tasks</example>
		<phraseLemma>we use np to evaluate np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In these settings the dictionaries used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each mode might be more different in the subjectverbobject example one of the dictionaries would only have verbs while the other two would only have nouns for instance</example>
		<phraseLemma>in np use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; instead of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our work is also similar to prior work on multilingual latent semantic analysis Bader and Chew also include a translation dictionary when decomposing the X matrix though their formulation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses a termdocument matrix instead of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a wordcontext matrix and the way they use the translation dictionary is quite different</example>
		<phraseLemma>np use np instead of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is taken into account</phrase>
		<frequency>10</frequency>
		<example>That is to say the order in which the decoder would prefer to generate the target sequence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is taken into account&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be take into account</phraseLemma>
	</can>
	<can>
		<phrase>It is possible that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is possible that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no such state exists in which case since the stream decoder is required to make an output it must use an alternative strategy</example>
		<phraseLemma>it be possible that np</phraseLemma>
	</can>
	<can>
		<phrase>In the performance of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the phrasebased decoder suffered as expected in comparison to pairs of European languages</example>
		<phraseLemma>in the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; it is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this setup it is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of disambiguating rules with the same source side and aligned target nonterminals</example>
		<phraseLemma>in np it be np</phraseLemma>
	</can>
	<can>
		<phrase>We create &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We create training examples using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the rule extraction procedure in 1 We begin by generating a ruletable using this procedure</example>
		<phraseLemma>we create np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is done using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Rule scoring &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is done using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relative frequencies normalized over the source rhs and aligned nonterminals in the target rhs</example>
		<phraseLemma>np be do use np</phraseLemma>
	</can>
	<can>
		<phrase>We trained &lt;NP&gt; on &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We trained a 1 gram language model on the English side of each training corpus using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SRI Language Modeling Toolkit</example>
		<phraseLemma>we train np on np use np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train the model in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the standard way and generate word alignments using GIZA</example>
		<phraseLemma>we train np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; according to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>After training we reduced the number of translation rules by only keeping the best rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the same source side according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the direct rule translation rule probability</example>
		<phraseLemma>np with np accord to np</phraseLemma>
	</can>
	<can>
		<phrase>We also thank &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also thank Andreas Maletti for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; his shared expertise on tree grammars</example>
		<phraseLemma>we also thank np for np</phraseLemma>
	</can>
	<can>
		<phrase>We tuned &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We tuned the translation models using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MERT using the development set of the above mentioned campaign consisting of sentence pairs for each language pair</example>
		<phraseLemma>we tune np use np</phraseLemma>
	</can>
	<can>
		<phrase>For the majority of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the majority of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the traits the native results outperform both translation settings in some cases by considerable margin</example>
		<phraseLemma>for the majority of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; which &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>First we explored if there were gender signals &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the English corpus which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a classiﬁer could uncover</example>
		<phraseLemma>np in np which np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to compute &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use our new method to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; aligned wordembeddings for twentyone languages using English as a pivot language</example>
		<phraseLemma>we use np to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not exist in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We show that some linguistic features are aligned across languages for which we do not have aligned data even though those properties &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not exist in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pivot language</example>
		<phraseLemma>np do not exist in np</phraseLemma>
	</can>
	<can>
		<phrase>We compare our method with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our method with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; previous approaches on a crosslingual document classiﬁcation task and on a word translation task and obtain state of the art results on these tasks</example>
		<phraseLemma>we compare we method with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was represented using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We trained our classiﬁer on the articles of one language where each document &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was represented using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an IDF weighted sum of the vectors of its words we then tested it on the articles of the other language</example>
		<phraseLemma>np be represent use np</phraseLemma>
	</can>
	<can>
		<phrase>English to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;English to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Spanish and Spanish to English</example>
		<phraseLemma>english to np</phraseLemma>
	</can>
	<can>
		<phrase>Previous work on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Previous work on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; continuity hypothesis suggests that discourse connectives are indicators of the continuity of discourse and help the interlocutors predict the level of continuity of upcoming sentences</example>
		<phraseLemma>previous work on np</phraseLemma>
	</can>
	<can>
		<phrase>We also notice that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also notice that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precedence has a lot more implicit occurrences than succession meaning that inferring a normal temporal relation is much easier than inferring a reversed temporal relation</example>
		<phraseLemma>we also notice that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Figure 1 shows &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The results for CDTB &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Figure 1 shows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same trends as for English</example>
		<phraseLemma>np in figure 1 show np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; containing &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First we collect a new open domain deception datasso &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;containing demographic data such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; gender and age</example>
		<phraseLemma>np contain np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; play an important role in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We further explore the linguistic differences in deceptive content that relate to deceivers gender and age and ﬁnd evidence that both age and gender &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;play an important role in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; peoples word choices when fabricating lies</example>
		<phraseLemma>np play a important role in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; related to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular information related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the demographics of the deceivers could be potentially useful as recent studies have shown that online users lie frequently about their appearance gender age or even education level</example>
		<phraseLemma>in np related to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; obtain &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In order to explore language differences among deceivers and truetellers we use the linguistic ethnography method and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;obtain the most dominant semantic word classes in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the LIWC lexicon associated to truth and lies provided by males and females</example>
		<phraseLemma>np obtain np in np</phraseLemma>
	</can>
	<can>
		<phrase>We generate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We generate a phonological class for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each position in the template using this feature system and a parameter p ∈ which controls the models willingness to consider more or less abstract phonological classes</example>
		<phraseLemma>we generate np for np</phraseLemma>
	</can>
	<can>
		<phrase>We implement &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We implement a topdown parser for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our new CFG of Section 1 following the Earley algorithm</example>
		<phraseLemma>we implement np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are excluded from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In some definitions equations and inequations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are excluded from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; math expressions</example>
		<phraseLemma>np be exclude from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are in fact &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Since SBMT and AMR parsing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are in fact&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distinct tasks as outlined in Figure 1 to adapt the SBMT parsing framework to AMR parsing we develop novel representations and techniques</example>
		<phraseLemma>np be in fact np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Figure 1 a</phrase>
		<frequency>10</frequency>
		<example>Figure 1 progressively shows all the transformations described in this section the example we follow is shown in its original form &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Figure 1 a&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in figure 1 a</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The quant role &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; representation of numerical quantities can be ﬁlled by an instance or a string</example>
		<phraseLemma>np which be use for np</phraseLemma>
	</can>
	<can>
		<phrase>We now turn to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We now turn to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; language models of AMRs which help us prefer reasonable target structures over unreasonable ones</example>
		<phraseLemma>we now turn to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We follow this practice here and additionally detect person names &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at decodetime using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Stanford Named Entity Recognizer</example>
		<phraseLemma>np at np use np</phraseLemma>
	</can>
	<can>
		<phrase>As an alternative to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As an alternative to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the unsupervised approach of Pourdamghani we build alignments by taking a linguisticallyaware supervised heuristic approach to alignment</example>
		<phraseLemma>as a alternative to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to identify &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In that work multiple discriminatively trained models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; individual concept instances and then a minimum spanning tree algorithm connects the concepts</example>
		<phraseLemma>np be use to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in performance</phrase>
		<frequency>10</frequency>
		<example>However they are unable to reach the state of the art &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in performance&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in performance</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is computed from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>According to the principle of compositionality the meaning of a sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is computed from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the meaning of its parts and the way they are syntactically combined</example>
		<phraseLemma>np be compute from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was annotated in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition because the SST &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was annotated in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a constituency manner we also employed the Charniaks constituent parser with Huang s forest pruner</example>
		<phraseLemma>np be annotated in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the difference in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Comparing to CTLSTM although there is no difference in the ﬁnegrained task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the difference in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the binary task is signiﬁcant</example>
		<phraseLemma>np the difference in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; containing &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Interestingly we can see that the circle of incoherent” is larger than the circles of any inner cells suggesting that the network is able to make use of parses &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;containing direct links from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that word to the root</example>
		<phraseLemma>np contain np from np</phraseLemma>
	</can>
	<can>
		<phrase>The idea that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The idea that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a composition function must be able to change its behaviour on the ﬂy according to input vectors is explored by Socher Le and Zuidema among others</example>
		<phraseLemma>the idea that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; providing &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Examples include giving navigational directions to robots and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;providing hints to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; automated gameplaying agents</example>
		<phraseLemma>np provide np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been studied in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Each task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been studied in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; isolation but we are unaware of any published approaches capable of robustly handling all three</example>
		<phraseLemma>np have be study in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; representing &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Almost all existing policylearning approaches make use of an unstructured parameterization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a single feature vector representing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all text and observations</example>
		<phraseLemma>np with np represent np</phraseLemma>
	</can>
	<can>
		<phrase>We assume &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We assume access to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a pretrained parser and in particular that each of the instructions xi is represented by a treestructured dependency parse</example>
		<phraseLemma>we assume np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponding to &lt;NP&gt; corresponding to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The example contains a node &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponding to the primitive action move and several nodes corresponding to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; locations in the environment that are visible after the action is performed</example>
		<phraseLemma>np correspond to np correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the score for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This is just an unnormalized synchronous derivation between x and y—at any aligned pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the score for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the entire derivation is the score produced by combining that word and node times the scores at all the aligned descendants</example>
		<phraseLemma>np the score for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is accompanied by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This map &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is accompanied by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of instructions specifying a path from the starting position to some destination point</example>
		<phraseLemma>np be accompany by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; of features</phrase>
		<frequency>10</frequency>
		<example>The map task was previously studied by Vogel and Jurafsky who implemented SARSA &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a simple set of features&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np of feature</phraseLemma>
	</can>
	<can>
		<phrase>By combining &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By combining these features with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our alignment model and search procedure we achieve stateoftheart results on this task by a substantial margin</example>
		<phraseLemma>by combine np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; like &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Future work might extend this approach to tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;like question answering where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; logicbased approaches have been successful</example>
		<phraseLemma>np like np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learned on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While there are exceptions most stateoftheart modelings view string transduction as a twostage process in which string pairs in the training data are ﬁrst aligned and then a subsequent module &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learned on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the aligned data</example>
		<phraseLemma>np be learn on np</phraseLemma>
	</can>
	<can>
		<phrase>To our knowledge &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To our knowledge&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all three outlined aspects of alignments — bigram models supervised learning and systematically estimating the relationship between alignment quality and overall string transduction performance — are novel in the G 1 P setting and its related ﬁelds as outlined however see also the related work section</example>
		<phraseLemma>to we knowledge np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are of &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both data sets contain quite complex matchups of character subsequences such as 1 as in English soirees/swOArPz or 1 as in weight/wPt but the majority of matchups &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are of type 1 and to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a lesser degree and 1</example>
		<phraseLemma>np be of np to np</phraseLemma>
	</can>
	<can>
		<phrase>The reason is that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The reason is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a unigram alignment model adds logprobabilities of matchedup subsequences which if not appropriately corrected for makes alignments with few matchups a priori more likely than alignments with many matchups when probabilities of individual matchups are uniformly or randomly initialized</example>
		<phraseLemma>the reason be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The table shows that the coefﬁcients are on the order of about 1 to 1 meaning that all else &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being equal increasing alignment quality by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 edit distance to the goldstandard alignment increases overall G 1 P by about 1 to 1</example>
		<phraseLemma>np be np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; yields &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>To see whether this relation gold standard alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in each case yields&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; betship changes when we vary the amount of train ter overall G 1 P transcriptions than training them ing data we run several more experiments</example>
		<phraseLemma>np in np yield np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as long as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Additionally the effect of alignment quality on overall G 1 P system performance may simply vanish as training set sizes become large enough because the translation modules can better accomodate noisy data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as long as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its size is sufﬁciently large</example>
		<phraseLemma>np as long as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This task is called word segmentation and the accuracy of stateoftheart methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on machine learning techniques is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than 1 for Japanese and 1 for Chinese</example>
		<phraseLemma>np base on np be np</phraseLemma>
	</can>
	<can>
		<phrase>As we show in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we show in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this paper logs collected from IMs are a valuable source of word boundary information</example>
		<phraseLemma>as we show in np</phraseLemma>
	</can>
	<can>
		<phrase>Since there are &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since there are some ambiguities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; conversion a conversion engine based on a word ngram model has been proposed</example>
		<phraseLemma>since there be np in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; instead of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the LM unit instead of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words we propose to adopt wordpronunciation pairs u = hy wi</example>
		<phraseLemma>for np instead of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are estimated from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As in existing statistical IM engines parameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are estimated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a corpus whose sentences are segmented into words annotated with their pronunciations as follows</example>
		<phraseLemma>np be estimate from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are segmented into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As in existing statistical IM engines parameters are estimated from a corpus whose sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are segmented into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words annotated with their pronunciations as follows</example>
		<phraseLemma>np be segmented into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are estimated by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These probabilities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are estimated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a model based on logistic regression trained on a manually segmented corpus referring to the same features as those used in</example>
		<phraseLemma>np be estimate by np</phraseLemma>
	</can>
	<can>
		<phrase>As Figure 1 shows &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As Figure 1 shows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the client of our IM running on the users PC is used to input characters and to modify conversion results</example>
		<phraseLemma>as figure 1 show np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; selects &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Sometimes the user changes some word boundaries makes the IM engine enumerate candidate words covering the region and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;selects the intended one from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the list of candidates</example>
		<phraseLemma>np select np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tend to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus the phoneme sequence and the ﬁnal word sequence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tend to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence fragments and as a result they lose context information</example>
		<phraseLemma>np tend to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are considered to be &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Resources The phoneme sequences and edit results in the ﬁnal selection themselves &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are considered to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; partially segmented sentences</example>
		<phraseLemma>np be consider to be np</phraseLemma>
	</can>
	<can>
		<phrase>To make &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To make the training data for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our IM server we ﬁrst chose randomly selected tweets 1 sentences in addition to the unannotated part of the BCCWJ</example>
		<phraseLemma>to make np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is much higher than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The fact that the precision on TWItest &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is much higher than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the recall indicates that the baseline model suffers from oversegmentation</example>
		<phraseLemma>np be much higher than np</phraseLemma>
	</can>
	<can>
		<phrase>To investigate the impact of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To investigate the impact of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the log size we measured WS accuracy on TWItest when varying the log size during training</example>
		<phraseLemma>to investigate the impact of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to assign &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The goal of sequence labeling &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to assign labels to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all elements in a sequence which can be handled with supervised learning algorithms such as Maximum Entropy and Conditional Random Fields</example>
		<phraseLemma>np be to assign np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; due to &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Thus it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;due to the considerable time lag between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the inputs and their corresponding outputs</example>
		<phraseLemma>np due to np between np</phraseLemma>
	</can>
	<can>
		<phrase>For evaluation we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For evaluation we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the standard bakeoff scoring program to calculate precision recall F 1 score and outofvocabulary word recall</example>
		<phraseLemma>for evaluation we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; in Table 1</phrase>
		<frequency>10</frequency>
		<example>We also evaluate the our 1 proposed models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the hyperparameter settings in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also shows that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>Our evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our method signiﬁcantly outperforms the stateoftheart monolingual and bilingual semisupervised approaches</example>
		<phraseLemma>np also show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to combine &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The heuristic strategy of growdiagﬁnaland &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to combine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the bidirectional alignments to extract phrase translations and to reorder tables</example>
		<phraseLemma>np be use to combine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; trained on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We justify the original CWS model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the new CRF model trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the segmentation of unlabeled bilingual data</example>
		<phraseLemma>np use np train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is made up of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As the corpus of Bakeoff &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is made up of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several sets provided by different organizations we only select two sets whose segmenting standards are similar to the training corpus</example>
		<phraseLemma>np be make up of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned as &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The precision p &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned as the percentage of words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the decoder output that are segmented correctly and the recall r is the percentage of goldstandard output words that are correctly segmented by the decoder</example>
		<phraseLemma>np be deﬁned as np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may result in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The larger rule table incurs more run time for decoding and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may result in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lower translation quality</example>
		<phraseLemma>np may result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; cannot handle &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Even though we may learn a statistically sound model on the basis of nonparametric Bayesian methods current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;cannot handle&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rules and phrases of various granularities</example>
		<phraseLemma>np can not handle np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For faster biparsing we run sampling in parallel in the same way as Zhao and Huang in which biparsing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parallel among the bilingual sentences in a minibatch</example>
		<phraseLemma>np be perform in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; following &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In addition to the model parameters hyperparameters are resampled after each training iteration &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;following the discount and strength hyperparameter resampling in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hierarchical PitmanYor process</example>
		<phraseLemma>np follow np in np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; for &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used the ﬁrst 1 K sentence pairs of the WMT 1 NewsCommentary corpus for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; German/Spanish/FrenchtoEnglish pairs and NTCIR 1 corpus for JapaneseEnglish for the translation model</example>
		<phraseLemma>we use np for np for np</phraseLemma>
	</can>
	<can>
		<phrase>Were the use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The major differences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the minimal phrase pairs used in the previous work in which only minimal phrase pairs in the leaves of derivation trees were included in the model</example>
		<phraseLemma>np be the use of np</phraseLemma>
	</can>
	<can>
		<phrase>We propose to use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; coverage which reﬂects how well extracted phrases can recover the training data to enable word alignment to model consistency and correlate better with machine translation</example>
		<phraseLemma>we propose to use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is designed for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although coverage &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is designed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extracting phrases using coverage is still beneﬁcial to hierarchical phrasebased models because hierarchical phrases are derived from phrases consistent with word alignment</example>
		<phraseLemma>np be design for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consistent with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Although coverage is designed for extracting phrases using coverage is still beneﬁcial to hierarchical phrasebased models because hierarchical phrases are derived from phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word alignment</example>
		<phraseLemma>np consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; mainly focus on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Efforts using sourceside associations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;mainly focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the exploitation of either sentencelevel context or the utilization of documentlevel context</example>
		<phraseLemma>np mainly focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; linking &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the edge set E an edge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;linking a source word to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a target translation is referred to as a sourcetarget association edge and an edge connecting two target translations is called as a targettarget relatedness edge</example>
		<phraseLemma>np link np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is referred to as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the edge set E an edge linking a source word to a target translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is referred to as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sourcetarget association edge and an edge connecting two target translations is called as a targettarget relatedness edge</example>
		<phraseLemma>np be refer to as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to build &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These content words and target translations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a translation graph where each node represents a sourceside content word or a candidate target translation</example>
		<phraseLemma>np be use to build np</phraseLemma>
	</can>
	<can>
		<phrase>We calculate &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We calculate the importance score of the source word s using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tfidf as follows</example>
		<phraseLemma>we calculate np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; of 1 on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Especially our graphbased lexical selection model GM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves an average BLEU score of 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two test sets which is higher than that of the baseline by 1 BLEU points</example>
		<phraseLemma>np achieve np of 1 on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is complementary to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>These experiment results suggest that exploring longdistance dependencies among target translations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is complementary to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the previous lexical selection methods which focus on sourceside context information</example>
		<phraseLemma>np be complementary to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are important for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This performance curve on the values of λ suggests that targetside global dependencies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are important for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lexical selection</example>
		<phraseLemma>np be important for np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to capture &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a graph representation to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; local and global context information which to the best of our knowledge is the ﬁrst attempt to explore graphbased representations for lexical selection</example>
		<phraseLemma>we use np to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been applied in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Because of the advantage of global consistency random walk algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been applied in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SMT</example>
		<phraseLemma>np have be apply in np</phraseLemma>
	</can>
	<can>
		<phrase>In spite of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In spite of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their success these approaches center around capturing relations between entire source and target phrases</example>
		<phraseLemma>in spite of np</phraseLemma>
	</can>
	<can>
		<phrase>Results on &lt;NP&gt; show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Results on the NIST 1 and datasets show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our system achieves signiﬁcant improvements over baseline methods</example>
		<phraseLemma>result on np show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as much as possible</phrase>
		<frequency>10</frequency>
		<example>We expect T to satisfy structural alignment consistency &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as much as possible&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as much as possible</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we apply &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this process we apply&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LBFGS to tune parameters based on gradients over the joint error as implemented in</example>
		<phraseLemma>in np we apply np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; containing &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>During training we extract bilingual phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;containing up to 1 words on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source side from the training corpus</example>
		<phraseLemma>np contain np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; according to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Compared with these works our model exploits different levels of correspondence relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inside bilingual phrases instead of only the top level of entire phrases and reconstructs tree structures of subphrases in one language according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; aligned nodes in the other language which to the best of our knowledge has never been investigated before</example>
		<phraseLemma>np in np accord to np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we have presented &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we have presented&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the BCorrRAE to learn phrase embeddings and tree structures of bilingual phrases for SMT</example>
		<phraseLemma>in this paper we have present np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to maximize &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The standard way to train NNLMs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to maximize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the log likelihood of the training data</example>
		<phraseLemma>np be to maximize np</phraseLemma>
	</can>
	<can>
		<phrase>We choose &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We choose ArabicEnglish language pair for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the development experiments and train domainwise models to measure the relatedness of each domain with respect to the indomain</example>
		<phraseLemma>we choose np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showing that there is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The perplexity numbers improved signiﬁcantly in each case &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showing that there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; useful information available in each domain which can be utilized to improve the baseline</example>
		<phraseLemma>np show that there be np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is treated as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In some dialog systems content selection is treated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an optimization problem balancing the placement of fullstops and the insertion or deletion of propositions with the similarity of the resulting output and an existing corpus of acceptable productions</example>
		<phraseLemma>in np be treat as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; addressing &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>With the introduction of the concept of contentheavy sentences we can envision dialog systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;addressing the sentence realization task in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two steps ﬁrst predicting if the semantic content will require multiple sentences then having different rankers for expressing the content in one or multiple sentences</example>
		<phraseLemma>np address np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For about 1 of segments in OpenMT and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MTC at least three of the translators produce a multisentence translation a rate high enough to warrant closer inspection of the problem</example>
		<phraseLemma>np 1 in np</phraseLemma>
	</can>
	<can>
		<phrase>In Table 1 we show &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Table 1 we show&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the BLEU scores and ∆bleu for sentences that are heavy and nonheavy</example>
		<phraseLemma>in table 1 we show np</phraseLemma>
	</can>
	<can>
		<phrase>When there is &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When there is a fullstop comma in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence there is a higher chance that the sentence is contentheavy</example>
		<phraseLemma>when there be np in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the latter we use the universal partofspeech tags for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each word rather than the word itself to avoid too detailed and sparse representations</example>
		<phraseLemma>for np we use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on the left</phrase>
		<frequency>10</frequency>
		<example>First we indicate the presence of noun phrases with heavy modiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the left&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on the left</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; illustrated in Figure 1</phrase>
		<frequency>10</frequency>
		<example>The 1 type of construction is the use of serial verb phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;illustrated in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np illustrate in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>To capture &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To capture the transition of each phrase and clause in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence we construct functional POS trigrams for each sentence by removing all nouns verbs adjectives adverbs numbers and pronouns in the sentence</example>
		<phraseLemma>to capture np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; before and after &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>POS tags immediately &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;before and after&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the comma left and rightsibling node labels of the parent of the comma the punctuation tokens ordered from left to right in the sentence whether the comma has a coordinating IP structure whether the commas parent is a child of the root of the tree whether there is a subordination before the comma whether the difference in number of words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;before and after&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the comma is greater than or equal to seven</example>
		<phraseLemma>np before and after np</phraseLemma>
	</can>
	<can>
		<phrase>The results from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; forward selection analysis reveal that the dependency structure of a sentence captures the most helpful information for heavy sentence identiﬁcation</example>
		<phraseLemma>the result from np</phraseLemma>
	</can>
	<can>
		<phrase>In order to address &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to address&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this problem searchaware tuning aims to promote not only the accurate complete translations in the ﬁnal beam but more importantly those promising partial derivations in nonﬁnal beams</example>
		<phraseLemma>in order to address np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On the test data a simple multiclass classiﬁer then sufﬁces to predict accurate tags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; novel words</example>
		<phraseLemma>np even for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was &lt;NP&gt; proposed by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The idea of projecting annotated resources across languages using parallel data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was ﬁrst proposed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Yarowsky</example>
		<phraseLemma>np be np propose by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been shown to improve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In more of an unsupervised context adding more languages to the mix &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been shown to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; partofspeech performance across all component languages</example>
		<phraseLemma>np have be show to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In our own previous multilingual work we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the absence of parallel text</example>
		<phraseLemma>np even in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; representing &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We then perform a 1 CCA &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between these word representations and vectors representing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the projected tags from all resourcerich languages</example>
		<phraseLemma>np between np represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; yielded &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As the results indicate this ﬁnal method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;yielded the best tagging performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CONLL test data achieving average accuracy of 1</example>
		<phraseLemma>np yield np on np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we show that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a very simple semisupervised variant of Ratnaparkhis method results in a much tighter tag dictionary than either Ratnaparkhis or our previous method with accuracy as high as with our previous tag dictionary but much faster tagging—more than 1 tokens per 1 in Perl</example>
		<phraseLemma>in this paper we show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is selected as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The tag sequence assigned the highest score by the model for a given word sequence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is selected as the tagging for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the word sequence</example>
		<phraseLemma>np be select as np for np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; described in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In our new version of this procedure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use the KNsmoothed tag dictionary described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np we use np describe in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; whether &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>State generally reﬂects the deﬁniteness in nominals and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;whether a nominal is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the head of genitive construction</example>
		<phraseLemma>np whether np be np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we trained &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For syntactic features we trained an Arabic dependency parser using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MaltParser on the Columbia Arabic Treebank version of the PATB</example>
		<phraseLemma>for np we train np use np</phraseLemma>
	</can>
	<can>
		<phrase>We excluded &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We excluded all DevTrain words whose predicted POS switches from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nominal to nonnominal or vice versa but kept them as part of other words context</example>
		<phraseLemma>we exclude np from np</phraseLemma>
	</can>
	<can>
		<phrase>The increase in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The increase in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Diac accuracy on All Words is 1 absolute and on Nominals is 1 absolute</example>
		<phraseLemma>the increase in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; to train &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We perform experiments on the SANCL 1 web data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the Wall Street Journal training corpus to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the models and the WSJ development corpus to tune parameters</example>
		<phraseLemma>np use np to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Following Chen and Manning we use the pretrained word embedding released by Collobert and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set h = 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hidden layer size λ = 1 for regularization and α = 1 for the initial learning rate of Adagrad</example>
		<phraseLemma>np set np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; shared &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Note that the accuracies of our parsers are lower than the best systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the SANCL shared&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; task which use ensemble models</example>
		<phraseLemma>np in np share np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the literature</phrase>
		<frequency>10</frequency>
		<example>Our combined parser gives accuracies competitive to stateoftheart deterministic parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the literature&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the literature</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; giving &lt;NP&gt; of 1</phrase>
		<frequency>10</frequency>
		<example>Honnibal applies dynamic oracle to the deterministic transitionbased parsing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;giving a UAS of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np give np of 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; involving &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this section we describe our new labeled parsing model that can exploit labeled features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;involving two edgelabels in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sibling part</example>
		<phraseLemma>np involve np in np</phraseLemma>
	</can>
	<can>
		<phrase>In our experiments we set &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our experiments we set&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; k = 1 and ﬁx the number of iteration to instead of tuning these parameters on development sets</example>
		<phraseLemma>in we experiment we set np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Empty categories are phonetically null elements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; representing dropped pronouns controlled elements and traces of movement such as WHquestions and relative clauses</example>
		<phraseLemma>np that be use for np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we propose &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we propose a novel method for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings</example>
		<phraseLemma>in this paper we propose np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are marked in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The phrase structure of the sentences is annotated and any grammatical errors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are marked in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the trees</example>
		<phraseLemma>np be mark in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; most of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Most of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the capitalisation errors involve proper nouns and most of the deleted tokens are cases of missing punctuation</example>
		<phraseLemma>np most of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when evaluated on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our derived features improve the accuracy of a ﬁrstorder dependency parser by 1 UAS points &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when evaluated on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the indomain WSJ testset obtaining a ﬁnal accuracy of 1 UAS for a ﬁrstorder parser</example>
		<phraseLemma>np when evaluate on np</phraseLemma>
	</can>
	<can>
		<phrase>This is in contrast to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is in contrast to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; previous works in which improvements over using Brownclusters features were achieved only by adding to the clusterbased features not by replacing them</example>
		<phraseLemma>this be in contrast to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is measured by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Parsing accuracy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is measured by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unlabeled attachment score excluding punctuations</example>
		<phraseLemma>np be measure by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in terms of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We focus on ﬁrstorder parsers as they are the most practical graphbased parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of running time in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; realistic parsing scenarios</example>
		<phraseLemma>np in term of np in np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we extend &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we extend&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this line of work and introduce two new types of features that signiﬁcantly improve parsing performance</example>
		<phraseLemma>in this paper we extend np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produces &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We ﬁnd that for all settings the dense neural network model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produces higher POS tagging and parsing accuracy gains than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its sparse linear counterpart</example>
		<phraseLemma>np produce np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are weighted by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Parses &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are weighted by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their probabilities and combined using an adapted version of Sagae and Lavie</example>
		<phraseLemma>np be weight by np</phraseLemma>
	</can>
	<can>
		<phrase>To study the effects of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To study the effects of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these parameters Figure 1 shows three slices of the tuning surface for BLLIP parser on WSJ section around the optimal settings</example>
		<phraseLemma>to study the effect of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs better than &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Model combination &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs better than fusion&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on BNC and GENIA but surprisingly fusion outperforms model combination on three of the six domains</example>
		<phraseLemma>np perform better than np on np</phraseLemma>
	</can>
	<can>
		<phrase>The difference is that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The difference is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; novel predictions with fusion better incorporate model conﬁdence whereas when stacking a novel prediction is less trusted than those produced by one or both of the base parsers</example>
		<phraseLemma>the difference be that np</phraseLemma>
	</can>
	<can>
		<phrase>This is similar to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accuracy obtained from actual model combination techniques but at a fraction of the computational cost</example>
		<phraseLemma>this be similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that take into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We introduce an extension to the bagofwords model for learning words representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that take into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; account both syntactic and semantic properties within language</example>
		<phraseLemma>np that take into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; within the context</phrase>
		<frequency>10</frequency>
		<example>The main intuition behind our model is that the prediction of a word is only dependent on certain words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;within the context&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np within the context</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; differs from &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The continuous bagofwords model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;differs from other proposed models in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sense that its complexity does not rise substantially as we increase the window b since it only requires two extra additions to compute c which correspond to d operations each</example>
		<phraseLemma>np differ from np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; ai given to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The attention &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;ai given to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word w ∈ V at the relative position i is computed as</example>
		<phraseLemma>np be give to np</phraseLemma>
	</can>
	<can>
		<phrase>To evaluate the quality of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To evaluate the quality of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our vectors in terms of semantics we use the sentiment analysis task which is a binary classiﬁcation task for movie reviews</example>
		<phraseLemma>to evaluate the quality of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are common in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Attention models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are common in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vision related tasks where models learn to pay attention to certain parts of a image in order to make accurate predictions</example>
		<phraseLemma>np be common in np</phraseLemma>
	</can>
	<can>
		<phrase>In the area of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the area of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word representation learning no prior work that uses attention models exists to our knowledge</example>
		<phraseLemma>in the area of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In their preprints &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both Dyer and Weiss report error reductions of around 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; greedy onebest parsing and much more modest improvements for transitionbased parsers with beam search</example>
		<phraseLemma>np both np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>We show that the Nivre and FernandezGonzalez Unshift operation serves as a far superior nonmonotonic Reduce action than the one Honnibal use &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in their system and that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the resulting transition system improves parse accuracy by considerably more than either the Honnibal or Nivre systems do</example>
		<phraseLemma>np in np that np</phraseLemma>
	</can>
	<can>
		<phrase>We follow &lt;NP&gt; in using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We follow Honnibal in using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dynamic oraclebased searchandlearn training strategy introduced by Goldberg and Nivre</example>
		<phraseLemma>we follow np in use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are trained and evaluated on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are trained and evaluated on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same data used by Tetreault in their recent bakeoff of leading dependency parsing models</example>
		<phraseLemma>np be train and evaluate on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; which is &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We show that this combination of innovations results in a parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with 1 directed accuracy which is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an improvement of 1 directed accuracy over an equivalent arcstandard parser</example>
		<phraseLemma>np with np which be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used to improve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this paper we show that a graph propagation approach that uses PPDB paraphrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; overall translation quality</example>
		<phraseLemma>np can be use to improve np</phraseLemma>
	</can>
	<can>
		<phrase>To evaluate &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To evaluate the similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two phrases we use cosine similarity</example>
		<phraseLemma>to evaluate np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is constructed using &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Fig 1 shows a small slice of the actual graph used in one of our experiments This graph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is constructed using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the paraphrase database on the right side of the ﬁgure</example>
		<phraseLemma>np be construct use np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we consider &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our setting we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the translation e to be the label” and so we propagate the labeling distribution p which is taken from the feature function for the SMT loglinear model that is taken from the SMT phrase table and we propagate this distribution to unlabeled nodes in the graph</example>
		<phraseLemma>in np we consider np</phraseLemma>
	</can>
	<can>
		<phrase>We varied the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We varied the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; iterations from 1 to on a heldout dev set and found that 1 iterations was optimal</example>
		<phraseLemma>we vary the number of np</phraseLemma>
	</can>
	<can>
		<phrase>For our experiments we use &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For our experiments we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Hadoop distributed computing framework executed on a cluster with nodes</example>
		<phraseLemma>for we experiment we use np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to construct &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use unigram nodes to construct&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; graphs for both DP and PPDB</example>
		<phraseLemma>we use np to construct np</phraseLemma>
	</can>
	<can>
		<phrase>We would like to explore &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We would like to explore&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; new propagation methods that can directly use conﬁdence estimates and control propagation based on label sparsity</example>
		<phraseLemma>we would like to explore np</phraseLemma>
	</can>
	<can>
		<phrase>Through the use of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Through the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semisupervised graph propagation a large scale multilingual paraphrase database can be used to improve the quality of statistical machine translation</example>
		<phraseLemma>through the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; extracted from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The syntactically augmented translation model proposed by Zollmann and Venugopal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses syntactic categories extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; targetside parse trees to augment nonterminals in hierarchical rules</example>
		<phraseLemma>np use np extract from np</phraseLemma>
	</can>
	<can>
		<phrase>We calculate &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We calculate semantic similarities between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; projected representations of phrases and those of nonterminals</example>
		<phraseLemma>we calculate np between np</phraseLemma>
	</can>
	<can>
		<phrase>The features used in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The features used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline system includes a 1 gram language model trained on the Xinhua section of the English Gigaword corpus a 1 gram language model trained on the target part of the bilingual training data bidirectional translation probabilities bidirectional lexical weights a word count a phrase count and a glue rule count</example>
		<phraseLemma>the feature use in np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we apply &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we apply&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ngram models with modiﬁed KneserNey smoothing during phrasebased decoding and neural JTR models in rescoring</example>
		<phraseLemma>in this work we apply np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was integrated into &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In a slightly enhanced version of OSM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was integrated into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the loglinear framework of the Moses system</example>
		<phraseLemma>np be integrate into np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; were used for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the full data 1 grams were used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the IWSLT and WMT tasks and 1 grams for BOLT</example>
		<phraseLemma>for np be use for np</phraseLemma>
	</can>
	<can>
		<phrase>In comparison &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In comparison&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the FFNN of Eq</example>
		<phraseLemma>in comparison np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; exploring &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However there has been little work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;exploring useful architectures for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; attentionbased NMT</example>
		<phraseLemma>np explore np for np</phraseLemma>
	</can>
	<can>
		<phrase>We demonstrate &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We demonstrate the effectiveness of both approaches on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WMT translation tasks between English and German in both directions</example>
		<phraseLemma>we demonstrate np on np</phraseLemma>
	</can>
	<can>
		<phrase>With &lt;NP&gt; we achieve &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;With local attention we achieve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a signiﬁcant gain of 1 BLEU points over nonattentional systems that already incorporate known techniques such as dropout</example>
		<phraseLemma>with np we achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to consider &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The idea of a global attentional model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the hidden states of the encoder when deriving the context vector ct</example>
		<phraseLemma>np be to consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is computed as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given the alignment vector as weights the context vector ct &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is computed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the weighted average over all the source hidden states</example>
		<phraseLemma>np be compute as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is similar in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>While our global attention approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is similar in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; spirit to the model proposed by Bahdanau there are several key differences which reﬂect how we have both simpliﬁed and generalized from the original model</example>
		<phraseLemma>np be similar in np</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our NMT systems in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the EnglishGerman task with various other systems</example>
		<phraseLemma>we compare np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are more effective than &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As demonstrated in Figure 1 our attentional models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are more effective than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the other nonattentional model in handling long sentences</example>
		<phraseLemma>np be more effective than np</phraseLemma>
	</can>
	<can>
		<phrase>We test &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We test the effectiveness of our models in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WMT translation tasks between English and German in both directions</example>
		<phraseLemma>we test np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are superior to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our analysis shows that attentionbased NMT models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are superior to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nonattentional ones in many cases for example in translating names and handling long sentences</example>
		<phraseLemma>np be superior to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which states that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The method is on the basis of the principle of compositionality &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which states that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the meaning of a longer expression depends on the meanings of its constituents</example>
		<phraseLemma>np which state that np</phraseLemma>
	</can>
	<can>
		<phrase>We report &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report empirical results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 largescale datasets and show that the approach outperforms stateoftheart methods for document level sentiment classiﬁcation</example>
		<phraseLemma>we report np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are fed to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Recurrent Neural Network The obtained sentence vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are fed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a document composition component to calculate the document representation</example>
		<phraseLemma>np be feed to np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to measure &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use MSE to measure&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the divergences between predicted sentiment la bels and ground truth sentiment labels because review labels reﬂect sentiment strengths</example>
		<phraseLemma>we use np to measure np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; while &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This stems from the fact that there are many sentiment shifters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in document level reviews while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Tang learn SSWE by assigning sentiment label of a text to each phrase it contains</example>
		<phraseLemma>np in np while np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; obtain &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We can see that sophisticated baseline methods such as JMARS paragraph vector and convolutional NN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;obtain signiﬁcant performance boosts over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; AverageSG by capturing deeper semantics of texts</example>
		<phraseLemma>np obtain np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been successfully applied to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Conventional RNNs and LSTM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been successfully applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; various sequence prediction tasks such as language modeling speech recognition and spoken language understanding</example>
		<phraseLemma>np have be successfully apply to np</phraseLemma>
	</can>
	<can>
		<phrase>To capture &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To capture longrange dependencies from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the future as well as from the past we propose to use bidirectional RNNs which allow bidirectional links in the network</example>
		<phraseLemma>to capture np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; annotated by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This means that a candidate aspect term is considered to be correct only if it exactly matches with the aspect term &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;annotated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the human</example>
		<phraseLemma>np annotated by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; include &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The features used in the baseline model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;include the current word its POS tag its preﬁxes and sufﬁxes between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one to 1 characters its position its stylistics and its context</example>
		<phraseLemma>np include np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Finally &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to show the importance of ﬁnetuning the word embeddings in RNNs on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our task we present in Table 1 the performance of Elman and Jordan RNNs when the embeddings are used as they are and when they are ﬁnetuned on the task</example>
		<phraseLemma>np in np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; like &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Note that these RNNs only &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use word embeddings while IHS RD and DLIREC use complex features like&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependency relations named entity sentiment orientation of words word cluster and many more in their CRF models most of which are expensive to compute see</example>
		<phraseLemma>np use np like np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; gives &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the opinion target extraction task demonstrate that word embeddings improve the performance of both CRF and RNN models however ﬁnetuning them in RNNs on the task gives&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the best results</example>
		<phraseLemma>np on np give np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is updated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Then after each item is added to the chart the agenda &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is updated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all binary and unary rules that can be applied to the new item</example>
		<phraseLemma>np be update with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The space of possible extended lexical entries for word xi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; i which is expressed with a CFG as shown in Figure 1 a</example>
		<phraseLemma>np be deﬁned by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; achieves &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Previous studies show that semantic parsing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with synchronous contextfree grammars achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; favorable performance over most other alternatives</example>
		<phraseLemma>np with np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>First we enrich nonterminal symbols &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; capture contextual and structured information</example>
		<phraseLemma>np as to np</phraseLemma>
	</can>
	<can>
		<phrase>In particular for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tgt 1 src alignment it obtains improvements of 1 in accuracy and 1 in</example>
		<phraseLemma>in particular for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our approach of tripling the training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves comparable performance to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the one with gold alignment suggesting that instead of developing a brand new algorithm for semantic parsing alignment we can simply make use of GIZA alignment output</example>
		<phraseLemma>np achieve np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; obtains &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>It shows that our approach including enriched SCFG tripling training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with three alignments and unknown word translation obtains&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; consistent improvement over the 1 languages</example>
		<phraseLemma>np with np obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to explore &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Therefore another direction of our future work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to explore&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; speciﬁc problems that will emerge when employing treebased SMT systems to semantic parsing and provide solutions to them</example>
		<phraseLemma>np be to explore np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is related to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Relation extraction is another area of NLP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our task</example>
		<phraseLemma>np that be related to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is not present in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Diagrams often contain critical information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is not present in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the text</example>
		<phraseLemma>np that be not present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are paired with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>The sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are paired with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; short videos that visualize different interpretations of each sentence</example>
		<phraseLemma>np be pair with np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; associated with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the 1 interpretation associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parse 1 the bag is on the chair rather than with Sam</example>
		<phraseLemma>in np associate with np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 presents &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 presents the corpus templates for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each ambiguity class along with the number of sentences generated from each template</example>
		<phraseLemma>table 1 present np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is required for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>A custom corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is required for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task because no existing corpus containing either videos or images systematically covers multimodal ambiguities</example>
		<phraseLemma>np be require for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the fact that &lt;CL&gt;</phrase>
		<frequency>10</frequency>
		<example>The structure of the formula and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple predicates often refer to the same variables is recorded by θ a mapping between predicates and their arguments</example>
		<phraseLemma>np the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; described in this paper</phrase>
		<frequency>10</frequency>
		<example>We then situate our methods in the context of related work and provide additional experimental motivation for the improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;described in this paper&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np describe in this paper</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is done with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In this method some careful bookkeeping &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is done with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dynamic programming such that the probability can be computed correctly when the twosided search meets at an intermediate node</example>
		<phraseLemma>np be do with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to ﬁnd &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In PRA these random walks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a relatively small set of potentially useful path types for which more speciﬁc random walk probabilities are then computed at great expense</example>
		<phraseLemma>np be use to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that combined &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>On the data they were using a graph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that combined a formal knowledge base with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; textual relations extracted from text they found that this technique gave a substantial performance improvement</example>
		<phraseLemma>np that combine np with np</phraseLemma>
	</can>
	<can>
		<phrase>To test whether &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To test whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vector space similarity features give us any beneﬁt over just replacing relations with dummy symbols we add a feature extractor that is identical to the one above assuming an empty vector similarity mapping</example>
		<phraseLemma>to test whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; enables &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Representing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;information about realworld entities and their relations in structured knowledge base form enables&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; numerous applications</example>
		<phraseLemma>np in np enable np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; evaluating &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>From this line of work most relevant to our study is prior work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;evaluating continuous representation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; models on the FB 1 k dataset</example>
		<phraseLemma>np evaluate np on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is mapped to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the hidden layer every window of three elements is mapped to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hidden vector using positionspeciﬁc maps W a bias vector b and a tanh activation function</example>
		<phraseLemma>in np be map to np</phraseLemma>
	</can>
	<can>
		<phrase>Since the number of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such entities is impractically large we sample negative triples from the full set</example>
		<phraseLemma>since the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Having a mention increases the chance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that a random entity pair has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a relation from 1 to 1 — a ﬁftyfold increase</example>
		<phraseLemma>np that np have np</phraseLemma>
	</can>
	<can>
		<phrase>Around 1 of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Around 1 of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the test triples have mentions and contribute toward the measures in the With mentions column and the other 1 of the test triples contribute to the Without mentions column</example>
		<phraseLemma>around 1 of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In our implementation of model F we created entity pair parameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entity pairs that cooccur in the text data also trained pairwise vectors for cooccuring entities only but all of the training and test tuples in their study were cooccurring</example>
		<phraseLemma>np only for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provides us with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Using topic mixtures instead of a bag of words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provides us with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a lowdimensional interpretable representation that is useful for analyzing authors behaviors and preferences</example>
		<phraseLemma>np provide we with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are observed in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>This means that word lookup tables cannot generate representations for previously unseen words such as Frenchiﬁcation even if the components French and iﬁcation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are observed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other contexts</example>
		<phraseLemma>np be observe in np</phraseLemma>
	</can>
	<can>
		<phrase>To address &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To address OOV words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline setup these are replaced by an unknown token and also associated with a set of embeddings</example>
		<phraseLemma>to address np in np</phraseLemma>
	</can>
	<can>
		<phrase>Accuracies on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Accuracies on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the test set is reported on column acc”</example>
		<phraseLemma>accuracy on np</phraseLemma>
	</can>
	<can>
		<phrase>The work in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The work in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deﬁnes a simple compositional model by summing over all characters in a given word while the work in deﬁnes a convolutional network which combines windows of characters and a maxpooling layer to ﬁnd important morphological features</example>
		<phraseLemma>the work in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to produce &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Deep compositional models of meaning acting on distributional representations of words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to produce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vectors of larger text constituents are evolving to a popular area of NLP research</example>
		<phraseLemma>np in order to produce np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In all current deep compositional distributional settings the word embeddings are internal parameters of the model with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no use for any other purpose than the task for which they were speciﬁcally trained</example>
		<phraseLemma>in np be np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is produced by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In general the above models are shallow in the sense that they do not have functional parameters and the output &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is produced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the direct interaction of the inputs yet they have been shown to capture the compositional meaning of sentences to an adequate degree</example>
		<phraseLemma>np be produce by np</phraseLemma>
	</can>
	<can>
		<phrase>We propose &lt;NP&gt; for learning &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose a novel architecture for learning&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings and a compositional model to use them in a single step</example>
		<phraseLemma>we propose np for learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is replaced by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>In the ﬁrst set S 1 the target word within a given context &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is replaced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a random word as in the original C&amp;W paper this set is used to enforce semantic coherence in the word vectors</example>
		<phraseLemma>np be replace by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is illustrated in Figure 1</phrase>
		<frequency>10</frequency>
		<example>The overall architecture of our model as described in this and the previous section &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is illustrated in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be illustrate in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; applies &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>However note that by replacing the plausibility layer with a classiﬁer trained for some task at hand you get a taskspeciﬁc network that transparently trains multisense word embeddings and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;applies dynamic disambiguation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on the ﬂy</example>
		<phraseLemma>np apply np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; create &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>By leveraging the ﬂexibility of two grammar formalisms ContextFree Grammars and Linear ContextFree Rewriting Systems our models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;create desirable structures for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; forum threads</example>
		<phraseLemma>np create np for np</phraseLemma>
	</can>
	<can>
		<phrase>Given &lt;NP&gt; consisting of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given a thread consisting of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sequence of posts in chronological order the task is to produce a constituency tree with yield</example>
		<phraseLemma>give np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisting of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Given a thread &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisting of a sequence of posts in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; chronological order the task is to produce a constituency tree with yield</example>
		<phraseLemma>np consist of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was chosen by &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>For each thread of length l kl = l/h = 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was chosen by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tuning</example>
		<phraseLemma>np be choose by np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; are in &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If these nodes are in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a governing relation then ℓ is the dominating node</example>
		<phraseLemma>if np be in np</phraseLemma>
	</can>
	<can>
		<phrase>To make &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To make this feasible on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large datasets such as JSTOR we employ a parallelized MetropolisHastings and aliastable sampling framework adapted from LightLDA</example>
		<phraseLemma>to make np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is suitable for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>As a result CLDA &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is suitable for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; collections without a large proportion of common topics and can also reduce noise</example>
		<phraseLemma>np be suitable for np</phraseLemma>
	</can>
	<can>
		<phrase>Three kinds of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Three kinds of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; collectionlevel imbalance can confound intercollection topic models</example>
		<phraseLemma>three kind of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is chosen for &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Speciﬁcally a unit ηc &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is chosen for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each collection such that the average equivalent number of assigned words pertopic is equal</example>
		<phraseLemma>np be choose for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>CLDA provides empirical measurable and reproducible evidence of the shared research &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between these disciplines as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; how concepts are articulated</example>
		<phraseLemma>np between np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; 1 on &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Our model achieves 1 accuracy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on ﬁnegrained classiﬁcation and 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; binary classiﬁcation outperforming the best published numbers obtained by a deep recursive model and a convolutional model</example>
		<phraseLemma>np on np 1 on np</phraseLemma>
	</can>
	<can>
		<phrase>The dataset consists of &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The dataset consists of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parsed English sentences annotated at both the root level and the phrase level using 1 class ﬁnegrained labels</example>
		<phraseLemma>the dataset consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are normalized to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>Both word vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are normalized to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unit norm and are ﬁxed in the experiments without ﬁnetuning</example>
		<phraseLemma>np be normalize to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 to &lt;NP&gt;</phrase>
		<frequency>10</frequency>
		<example>We also analyze the effect of modeling nonconsecutive nthanks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some 1 1 1 1 1 no # 1 1 1 1 1 movement 1 # 1 1 1 grams</example>
		<phraseLemma>np 1 to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is guided by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>As a result textual analysis &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is guided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the received control feedback and the learned strategy directly builds on the text interpretation</example>
		<phraseLemma>np be guide by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be formulated as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The game environment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be formulated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sequence of state transitions of a Markov Decision Process</example>
		<phraseLemma>np can be formulate as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; We learn &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Parameter Learning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We learn the parameters θR of the representation generator and θA of the action scorer using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stochastic gradient descent with RMSprop</example>
		<phraseLemma>np we learn np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are performed over &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>In practice online updates to the parameters θ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are performed over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a mini batch of state transitions instead of a single transition</example>
		<phraseLemma>np be perform over np</phraseLemma>
	</can>
	<can>
		<phrase>We then apply &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We then apply positive Pointwise Mutual Information to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the raw counts and reduce the dimensions to through Singular Value Decomposition</example>
		<phraseLemma>we then apply np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; about the world</phrase>
		<frequency>9</frequency>
		<example>Our modeltheoretic space may then be described as an underspeciﬁed settheoretic representation of some shared beliefs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;about the world&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np about the world</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when tested on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>As one might expect categoryspeciﬁc training data yields high performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when tested on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same category</example>
		<phraseLemma>np when test on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; reaching &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Although this expectation seems intuitive it is worth noting that our system produces promisingly high correlations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;reaching humanperformance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a subset of our data</example>
		<phraseLemma>np reach np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs slightly worse than &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Our results show however that the M ikolov model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs slightly worse than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the cooccurrence space disproving the idea that predictive models always outperform countbased models</example>
		<phraseLemma>np perform slightly worse than np</phraseLemma>
	</can>
	<can>
		<phrase>On &lt;NP&gt; we ﬁnd that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On a manual check we ﬁnd that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translations produced by our system are more ﬂuent than those of both Moses HPB and Dep 1 Str</example>
		<phraseLemma>on np we ﬁnd that np</phraseLemma>
	</can>
	<can>
		<phrase>We decompose &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We decompose the permutation πm into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a forest of permutation trees P EF in O following algorithms in with trivial modiﬁcations</example>
		<phraseLemma>we decompose np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; calculate &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To ensure that rewrites leave meaning unchanged we use the SEMAFOR semantic role labeler on the original and modiﬁed sentence for each rolelabeled token &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the reference sentence we examine its corresponding role in the rewritten sentence and calculate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the average accuracy acrosss all sentences</example>
		<phraseLemma>np in np calculate np</phraseLemma>
	</can>
	<can>
		<phrase>For both of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For both of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the models described in we present results for scoring a proposition like OBAMA IS A SOCIALIST based only on the conditional predicate score</example>
		<phraseLemma>for both of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is interested in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Moreover for each of the topics of interest we created a binary variable indicating whether a participant &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is interested in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a speciﬁc topic</example>
		<phraseLemma>np be interested in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; including &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>For each brand we built threeway classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using different classiﬁcation algorithms including&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; AdaBoost Decision Tree Logistic Regression Naive Bayes Random Forest and SVM</example>
		<phraseLemma>np use np include np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we rank &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each user we rank&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the brands in each product category based on his preferences in the survey</example>
		<phraseLemma>for np we rank np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we compute &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each user and each product category we compute the ρ between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the users ground truth rank in the survey and the rank produced by the baseline</example>
		<phraseLemma>for np we compute np between np</phraseLemma>
	</can>
	<can>
		<phrase>As illustrated in Figure 1 &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As illustrated in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the entities closer to the Olympics get more updates in the revisions of their Wikipedia articles with subsequent links pointing to articles of more distant entities</example>
		<phraseLemma>as illustrate in figure 1 np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is particularly useful for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>This task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is particularly useful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; readers to quickly get the main idea of documents written in a source language that they are not familiar with</example>
		<phraseLemma>np be particularly useful for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; highlighting the importance of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Correlations with human assessment reveal an extremely wide range in performance among variants &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;highlighting the importance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an optimal choice of ROUGE variant in system evaluations</example>
		<phraseLemma>np highlight the importance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is limited by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Although the approach to evaluation of metrics provides insight into the accuracy of conclusions drawn from metric/test combinations the evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is limited by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inclusion of only six variants of ROUGE fewer than 1 of possible ROUGE variants</example>
		<phraseLemma>np be limit by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; focused on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>However none of the datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in those studies focused on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets and related articles linked to these tweets</example>
		<phraseLemma>np in np focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was 1 with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The Pearson correlation value &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was 1 with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a pvalue of 1 e 1 indicating that the interaction between formality and overlap was highly signiﬁcant</example>
		<phraseLemma>np be 1 with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; despite the fact that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>The results in Table 1 indicate that the perimage CNNAVGMAX metric outperforms the aggregated visual representationbased metrics of CNNMEAN and CNNMAX &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;despite the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Kiela and Bottou achieved optimal performance using the latter metrics on a wellknown conceptual relatedness dataset</example>
		<phraseLemma>np despite the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; against &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We compare our visual models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;against the current stateoftheart lexicon induction model using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; comparable data</example>
		<phraseLemma>np against np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; than on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>This may explain why we score higher in absolute terms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on that dataset than on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the more abstract one</example>
		<phraseLemma>np on np than on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that deals with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Sentiment Analysis is a NLP task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that deals with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extraction of opinion from a piece of text on a topic</example>
		<phraseLemma>np that deal with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are useful for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>They can be used to efficiently learn feature encodings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are useful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classification</example>
		<phraseLemma>np that be useful for np</phraseLemma>
	</can>
	<can>
		<phrase>In this method &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this method&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a standard recursive autoencoder pretrains the phrase embedding with an unsupervised algorithm by greedily minimizing the reconstruction error while the bilinguallyconstrained model learns to finetune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between nontranslation pairs</example>
		<phraseLemma>in this method np</phraseLemma>
	</can>
	<can>
		<phrase>Each of &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Each of the above functions are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; monotone submodular as the parameters sj and wi are positive</example>
		<phraseLemma>each of np be np</phraseLemma>
	</can>
	<can>
		<phrase>As far as we know &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As far as we know&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the algorithm has not been implemented for such problems because of complexity constraints</example>
		<phraseLemma>as far as we know np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are included in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The score assigned to the ground atom is the proportion of the words in the opinion span &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are included in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the subjectivity lexicon</example>
		<phraseLemma>np that be include in np</phraseLemma>
	</can>
	<can>
		<phrase>In total we have &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In total we have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three methods for eTarget candidate selection and three models for sentiment analysis</example>
		<phraseLemma>in total we have np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; does not have &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In many cases a simile vehicle does not have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive or negative polarity by itself</example>
		<phraseLemma>in np do not have np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; from &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a Java implementation of SVM from LIBLINEAR with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original parameter values used by the NRC Canada system</example>
		<phraseLemma>we use np from np with np</phraseLemma>
	</can>
	<can>
		<phrase>We label &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We label a simile as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive or negative if the sentiment classiﬁer labels it as positive or negative respectively</example>
		<phraseLemma>we label np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; please refer to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>For a complete list of datasets and their descriptions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;please refer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the supplementary website</example>
		<phraseLemma>np please refer to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was manually annotated with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Each video &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was manually annotated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several sentences describing what occurs in the video</example>
		<phraseLemma>np be manually annotated with np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we found &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In all situations we found&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; clear evidence that all three features contribute to the prediction task</example>
		<phraseLemma>in np we find np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have become &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Distributional Semantic Models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have become standard paraphernalia in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the natural language processing toolbox and even though there is a wide variety of models available the basic parameters of DSMs are now well understood</example>
		<phraseLemma>np have become np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compares them to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Table 1 summarizes our top results on the TOEFL BLESS and also the SimLex 1 similarity test and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compares them to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline score from the Skipgram model trained on the same data using a window size of 1 negative samples and dimensional vectors</example>
		<phraseLemma>np compare they to np</phraseLemma>
	</can>
	<can>
		<phrase>The results presented in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results presented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the previous section suggest a type of inversion of this principle in the case of DSMs</example>
		<phraseLemma>the result present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consists of &lt;NP&gt; representing &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The input layer to the network &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consists of a 1 hot vector representing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the previous word in the sequence and the hidden vector from the previous time step</example>
		<phraseLemma>np consist of np represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; belonging to &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The probability distributions over words and classes are calculated by multiplying the hidden vector with the corresponding weight matrix and applying the softmax function where σ is the logistic function and W is the weight matrix between the hidden layer and the output words in class c Finally we multiply the probability of the next word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;belonging to class c with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the output probability of the next word given the class to get the overall probability of the next word given the previous words</example>
		<phraseLemma>np belong to np with np</phraseLemma>
	</can>
	<can>
		<phrase>C is the number of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;C is the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classes V is vocabulary size and E is the expected number of words that need to be processed in the output layer during one step</example>
		<phraseLemma>c be the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are represented by &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The general idea to solve this type of problems is to map the input and class labels to a semantic space of usually lower dimension in which similar classes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are represented by closer points in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the space</example>
		<phraseLemma>np be represent by np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; parse &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Following the success in transfer learning from parsing to understanding tasks we use dependency &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;parse bigrams in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our features as well</example>
		<phraseLemma>np parse np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is trained as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The dialogue act representation is a vector composition of its constituent labels embeddings and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is trained as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hyperplane of a large margin binary classiﬁer for that dialogue act</example>
		<phraseLemma>np be train as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; fails to capture &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>ARMSE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;fails to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this and hence we use PRMSE</example>
		<phraseLemma>np fail to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared to &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>However one serious drawback that has impeded their adoption in production systems is the slow runtime speed of neural network models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared to alternate models such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; maximum entropy classiﬁers</example>
		<phraseLemma>np compare to np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; corresponds to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Distributional representations Lund and Burgess Sahlgren Turney and Pantel Baroni and Lenci represent a word as a highdimensional vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which each dimension corresponds to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a context word</example>
		<phraseLemma>np in which np correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>We summarize &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We summarize related work in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 and ﬁnish with conclusion in Section 1 and discussion of future work in Section 1</example>
		<phraseLemma>we summarize np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can take advantage of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>In such cases distributional initialization makes the learning task easier since in addition to the contexts of the rare word the learner now also has access to the global distribution of the rare word and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can take advantage of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; weight sharing with other words that have similar distributional representations to smooth embeddings systematically</example>
		<phraseLemma>np can take advantage of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; demonstrate that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>Experimental results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a word similarity judgment task demonstrate that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; embeddings of rare words learned with distributional initialization perform better than embeddings learned with traditional onehot initialization</example>
		<phraseLemma>np on np demonstrate that np</phraseLemma>
	</can>
	<can>
		<phrase>In order to account for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to account for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the compositionality of relationships we add two additional regularization terms</example>
		<phraseLemma>in order to account for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; resulting in &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Plank incorporate annotator disagreement in POS tags into the loss function of a POStag machine learner &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;resulting in improved performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; downstream chunking</example>
		<phraseLemma>np result in np on np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁnd &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ﬁnd consistent differences between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions</example>
		<phraseLemma>we ﬁnd np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being set by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Grifﬁths and Steyvers —mirroring the original inspirations for Gibbs sampling —draw an analogy to statistical physics viewing standard LDA as a system that favors conﬁgurations z that compromise between having few topics per document and having few words per topic with the terms of this compromise &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being set by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hyperparameters α and β</example>
		<phraseLemma>np be set by np</phraseLemma>
	</can>
	<can>
		<phrase>We perform &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>For 1 NG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we perform tokenization and stopword removal using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Mallet and remove words that appear fewer than times</example>
		<phraseLemma>np we perform np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; directly as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Since documents in the 1 NG dataset are associated with labels we use the labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;directly as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior knowledge</example>
		<phraseLemma>np directly as np</phraseLemma>
	</can>
	<can>
		<phrase>We assume that we have &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We assume that we have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parallel data in two languages</example>
		<phraseLemma>we assume that we have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; provided by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We use our ﬁnal θ 1 models to parse the treebank &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with automatic tags provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same POS tagger used for tagging the parallel data</example>
		<phraseLemma>np with np provide by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>For this purpose we use a joint model of English and French &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using all the available French treebank as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a bilingual dictionary</example>
		<phraseLemma>np use np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is capable of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>In languages that show little morphology performance remains good showing that the RNN composition strategy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is capable of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; capturing both morphological regularities and arbitrariness in the sense of Saussure</example>
		<phraseLemma>np be capable of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; designed to capture &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>In this section we present the standard word embeddings as in Dyer and the improvements we made generating word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;designed to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; morphology based on orthographic strings</example>
		<phraseLemma>np design to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depicted in Figure 1</phrase>
		<frequency>9</frequency>
		<example>In these experiments we have used the architecture &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depicted in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np depict in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is initialized to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Given an input bag of words ρ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is initialized to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input and σ is initialized as empty</example>
		<phraseLemma>np be initialize to np</phraseLemma>
	</can>
	<can>
		<phrase>In order to measure &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to measure&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Ngram probabilities on our data we train the 1 gram language model WSJ AFP and XIN data and randomly sample 1 gram probabilities from the syntactic model output on the WSJ development data ﬁnding that most of 1 gram probabilities p are larger than 1</example>
		<phraseLemma>in order to measure np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which acts as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The network contains both a neural probabilistic language model and an encoder &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which acts as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a conditional summarization model</example>
		<phraseLemma>np which act as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been proposed in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Document summarization is a relatively well studied area and various types of approaches for document summarization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been proposed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the past twenty years</example>
		<phraseLemma>np have be propose in np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>As for the similarity function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use cosine similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tfidf vectors of the sentences</example>
		<phraseLemma>np we use np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; already in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Where for each sentence S the score is a linear interpolation of similarity of sentence with all other sentences and the similarity of sentence with the sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;already in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the summary and λ is a constant</example>
		<phraseLemma>np already in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by selecting &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>These clusters are then used to generate the ﬁnal summary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by selecting the top central sentences from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each cluster in a roundrobin fashion</example>
		<phraseLemma>np by select np from np</phraseLemma>
	</can>
	<can>
		<phrase>We incorporate &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Based on the different uses of hashtags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we incorporate the type of hashtag into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the DPMM as a hidden variable</example>
		<phraseLemma>np we incorporate np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed that &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>It refers to draws from G Let θ represent a sequence of independent and identically distributed random variables distributed according to G Blackwell and MacQueen &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed that the conditional distributions of θ given θ have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the following form</example>
		<phraseLemma>np show that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; commonly used in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The Support vector machine and logistic regression model are two classiﬁcation models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;commonly used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these methods</example>
		<phraseLemma>np commonly use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is then used in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is then used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the graphbased classiﬁcation framework for readability assessment</example>
		<phraseLemma>np be then use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is employed in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is employed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the graphbased classiﬁcation framework for readability assessment which involves graph building merging and label propagation</example>
		<phraseLemma>np be employ in np</phraseLemma>
	</can>
	<can>
		<phrase>Because &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Because social media data such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets varies considerably across different cities the training of efﬁcient models requires labeling data from each city of interest which is costly and time consuming</example>
		<phraseLemma>because np such as np</phraseLemma>
	</can>
	<can>
		<phrase>This resulted in &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This resulted in ten datasets with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; regional diversity to be used for evaluation</example>
		<phraseLemma>this result in np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are commonly used in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>As shown in the state of the art named entities ie entities that have been assigned a name such as Seattle &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are commonly used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets</example>
		<phraseLemma>np be commonly use in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we demonstrated that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Section 1 we demonstrated that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the proportion of data representing individual classes varies strongly</example>
		<phraseLemma>in np we demonstrate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; takes &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Manually grading all essays &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;takes a lot of time and effort for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the graders</example>
		<phraseLemma>np take np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is much larger than &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>This is done to prevent the system from choosing only ngrams from the source domain as the useful ngrams since the number of source domain essays &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is much larger than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target domain essays</example>
		<phraseLemma>np be much larger than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contributes &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>When we examine the bagofwords features we see that prompt 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contributes only 1 to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the bagofwords features of prompt 1 in the indomain experiment</example>
		<phraseLemma>np contribute np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is based on &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The ﬁrst &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is based on the cosine similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TFIDF vectors representing each text</example>
		<phraseLemma>np be base on np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; according to &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>After preprocessing which includes tokenization stop word removal and stemming we use BM 1 to rank all relevant candidates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;according to their similarity to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the query namely to the input claim</example>
		<phraseLemma>np accord to np to np</phraseLemma>
	</can>
	<can>
		<phrase>Combining &lt;NP&gt; with &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Combining a CDED solution with recent works in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁeld of argumentation mining may give rise to a new generation of methods that will be able to automatically construct relevant arguments on demand for a variety of topics</example>
		<phraseLemma>combine np with np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is left for future work</phrase>
		<frequency>9</frequency>
		<example>A ﬁnal investigation in combination with query expansion that will be evaluated in the context of the live site &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is left for future work&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be leave for future work</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are collapsed into &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Identical corrections &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are collapsed into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one output system names with the same output are recorded internally</example>
		<phraseLemma>np be collapse into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as proposed by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Bojar notice that the similarity of the participants in terms of methods and training data causes some of them to be very similar and group systems into equivalence classes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as proposed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Koehn</example>
		<phraseLemma>np as propose by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; mapping them to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Consider the case of a child learning to discriminate between object categories and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;mapping them to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words given only a small amount of explicitly labeled data and a large portion of unsupervised learning where the child comprehends an adults speech or experiences positive feedback for his or her own utterances regardless of their correctness</example>
		<phraseLemma>np map they to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was to determine if &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The 1 class classiﬁcation problem we deﬁned using this dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was to determine if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a webpage could be identiﬁed as one belonging to a Student Faculty Course or a Project yielding a subset of usable 1 samples</example>
		<phraseLemma>np be to determine if np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisted only of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>In all this previous work text features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisted only of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; surfacelevel text features</example>
		<phraseLemma>np consist only of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both in &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>This click probability serves a crucial role &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both in the user experience and in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the revenue for the search engine</example>
		<phraseLemma>np both in np in np</phraseLemma>
	</can>
	<can>
		<phrase>We split the data into &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We split the data into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 partitions with respect to time</example>
		<phraseLemma>we split the datum into np</phraseLemma>
	</can>
	<can>
		<phrase>The results also show that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results also show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings can generate more extraction patterns</example>
		<phraseLemma>the result also show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are calculated from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Our CRF gazetteer features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are calculated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an input token from the text that we wish to label</example>
		<phraseLemma>np be calculate from np</phraseLemma>
	</can>
	<can>
		<phrase>We propose to learn &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; state changing verbs from Wikipedia edit history</example>
		<phraseLemma>we propose to learn np</phraseLemma>
	</can>
	<can>
		<phrase>When &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When a statechanging event such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a marriage or death happens to an entity the infobox on the entitys Wikipedia page usually gets updated</example>
		<phraseLemma>when np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is an example of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>At approximately the same time the article text might be updated with verbs that express the event eg X is now married to Y Figure 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is an example of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an infobox of an entity changing at the same time as the articles main text to reﬂect a marriage event</example>
		<phraseLemma>np be a example of np</phraseLemma>
	</can>
	<can>
		<phrase>We use 1 of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use 1 of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our labeled documents that have verb edits as features as training data and test on the remaining 1</example>
		<phraseLemma>we use 1 of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by extracting &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>ENTICE aims to improve knowledge density &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by extracting facts from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all 1 extraction classes ie for a given entity it extracts facts involving known relations identiﬁes potentially new relations that might be relevant for this entity establishes such relations between the given entity and other known as well as new entities – all in a single system</example>
		<phraseLemma>np by extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is mapped to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Now the relation phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is mapped to an existing predicate in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the KG based on the extraction patterns in the metadata of the target relation</example>
		<phraseLemma>np be map to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are chosen from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Can didate predicates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are chosen from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the above mapped predicates based on category signature of the two noun phrases</example>
		<phraseLemma>np be choose from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to improve &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>From this we observe that ENTICE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; knowledge density in NELL by a factor of 1 while maintaining 1 accuracy</example>
		<phraseLemma>np be able to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by adding &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>These embeddings will be used as features in the NER system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by adding a feature for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each dimension of the embedding for the current word/character</example>
		<phraseLemma>np by add np for np</phraseLemma>
	</can>
	<can>
		<phrase>In practice we use &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In practice we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a threshold ρ to control whether to use Eq</example>
		<phraseLemma>in practice we use np</phraseLemma>
	</can>
	<can>
		<phrase>To address this problem we propose &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To address this problem we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a latent document type discriminative model by introducing a latent layer to capture the correlations between documents and their underlying types</example>
		<phraseLemma>to address this problem we propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; aim to capture &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Both experiments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;aim to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; attitudes and opinions about risk by analyzing CEO letters and outlook sections of Eurozone banks</example>
		<phraseLemma>np aim to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in a review</phrase>
		<frequency>9</frequency>
		<example>The idea of modeling sentiment ﬂow was introduced by Mao and Lebanon who classify local sentiment based on neighboring local sentiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a review&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in a review</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we test &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Section 1 we test&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; what combinations of transformations lead to an adequate sentiment ﬂow model</example>
		<phraseLemma>in np we test np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are two sub tasks in targeted sentiment analysis namely entity recognition and sentiment classiﬁcation for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each entity mention which apply to both scenarios above</example>
		<phraseLemma>there be np in np for np</phraseLemma>
	</can>
	<can>
		<phrase>Partly due to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Partly due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this reason most previous work has addressed targeted sentiment analysis as a pure classiﬁcation task assuming that target mentions have been given</example>
		<phraseLemma>partly due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; heavily relies on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The neural model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;heavily relies on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁnetuning of embeddings and a likely reason is that manual discrete features offer sufﬁcient parameters for capturing invocabulary patterns</example>
		<phraseLemma>np heavily rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to model &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The evaluation by a target user for past reviews &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the user and predict the helpfulness for unread reviews which results in different predictions depending on the user</example>
		<phraseLemma>np be use to model np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; denote &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Here Zx &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;denotes a normalization factor and fk and gk denote feature functions for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unigram and bigram models respectively</example>
		<phraseLemma>np denote np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by taking advantage of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We further evaluate the LSTM model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by taking advantage of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its ready support for transfer learning and show that it can be adapted to an existing NLI challenge task yielding the best reported performance by a neural network model and approaching the overall state of the art 1 A new corpus for NLI To date the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment challenge tasks</example>
		<phraseLemma>np by take advantage of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the hypothesis</phrase>
		<frequency>9</frequency>
		<example>An indicator for every unigram and bigram &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the hypothesis&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the hypothesis</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in how &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Training on SICK alone yields poor performance and the model trained on SNLI fails when tested on SICK data labeling more neutral examples as contradictions than correctly possibly as a result of subtle differences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in how&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the labeling task was presented</example>
		<phraseLemma>np in how np</phraseLemma>
	</can>
	<can>
		<phrase>Given a collection of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given a collection of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; short documents our goal is to determine which documents contain the target entities</example>
		<phraseLemma>give a collection of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; that included &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For another larger dataset that included&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stock names we found only of them had their reference pages in Wikipedia</example>
		<phraseLemma>for np that include np</phraseLemma>
	</can>
	<can>
		<phrase>We design &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We design a multilayer directed graph for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; modeling short texts that possess different trust levels during propagation which signiﬁcantly improves the performance</example>
		<phraseLemma>we design np for np</phraseLemma>
	</can>
	<can>
		<phrase>We chose &lt;NP&gt; because &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We chose these datasets because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the identiﬁcation of entities in these domains meets the practical requirements of many applications the target entities are ambiguous and there is little information in the existing KBs</example>
		<phraseLemma>we choose np because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; in terms of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Using the topics as references we detect the relevant documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the candidate source in terms of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the topiclevel biographydocument relevance P</example>
		<phraseLemma>np in np in term of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; instead of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To achieve the goal we propose a stochastic ListNet approach which samples a small set of the Topk permutation classes and train the Topk model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on this small set instead of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the full set of permutation classes</example>
		<phraseLemma>np base on np instead of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; due to the lack of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Conventional EL systems are very likely to fail in linking protein mentions in the text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;due to the lack of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; labeled training data</example>
		<phraseLemma>np due to the lack of np</phraseLemma>
	</can>
	<can>
		<phrase>As a result of &lt;NP&gt; we obtain &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a result of this process we obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set KS comprising all the KBs in K redeﬁned using the common sense inventory S</example>
		<phraseLemma>as a result of np we obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; We use &lt;NP&gt; to generate &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We identify a set of highconﬁdence seeds from Ti ie triples hed r egi where subject ed and object eg are highly semantically related and disambiguate them using the senses that maximize their similarity in our vector space VS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the seeds to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a ranking of the relations in Ri according to their degree of speciﬁcity</example>
		<phraseLemma>np we use np to generate np</phraseLemma>
	</can>
	<can>
		<phrase>From &lt;NP&gt; we obtained &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From each pair of KBs hKBi KBji we obtained&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a list of candidate alignments ie pairs of relations hrirji where ri ∈ KBi and rj ∈ KBj</example>
		<phraseLemma>from np we obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; covers &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Since the external training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;covers mostly topics in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the General domain the performance gain in that domain is most signiﬁcant</example>
		<phraseLemma>np cover np in np</phraseLemma>
	</can>
	<can>
		<phrase>As we can see in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we can see in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fig 1 a the positive and negative sentence embeddings learned by RNN LM are randomly scattered in the space indicating that embeddings learned via unsupervised training may fail to capture the sentencelevel indicators associated with a particular task in this case presence of a name</example>
		<phraseLemma>as we can see in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; among &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>For the problem of sentencelevel name prediction the proposed method of combining the language modeling and sentencelevel name prediction objectives in an MT RNN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves the best results among&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; studied models for the domain represented by the training data as well as in the opendomain scenario</example>
		<phraseLemma>np achieve np among np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are retrieved from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Lexicalisations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are retrieved from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the KB where they are represented as the name or alias ie less frequent name of a resource</example>
		<phraseLemma>np be retrieve from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; obtaining &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Following Vlachos and Clark for each training instance we obtain supervision for the NEC stage by taking both options for this stage true or false &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;obtaining the prediction from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the RE stage in the former case and then comparing the outcomes against the label obtained from distant supervision</example>
		<phraseLemma>np obtain np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are appropriate for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We used precision ie how many of the relation candidates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are appropriate for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the relation and recall to compare the relation candidate identiﬁcation strategies described above against the identiﬁcation of candidates by Stanford NER</example>
		<phraseLemma>np be appropriate for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is extracted from &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Text content &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is extracted from HTML pages using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Jsoup API 1 and processed with Stanford CoreNLP 1</example>
		<phraseLemma>np be extract from np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; including &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>First we decompose the MOVELINK octuple &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into seven smaller tuples including&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one pair and six triplets</example>
		<phraseLemma>np into np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; subject to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To resolve these conﬂicts we select for each spatial element the role that was predicted with highest conﬁdence by the SVM classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;subject to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the global constraint</example>
		<phraseLemma>np subject to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; combines &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To train a classiﬁer on instances containing the original features and SRCTs we employ SVMlightT K which trains an SVM classiﬁer using the features with a linear kernel trains an SVM classiﬁer using only the SRCTs with a convolution kernel and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;combines these two kernels using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a composite kernel</example>
		<phraseLemma>np combine np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; which contains &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To extract the spatial elements needed for task 1 a we follow Bastianelli s sequence labeling approach except that we train the sequence labeler using a CRF rather than an HMM 1 Since the annotated test set used in SpaceEvals ofﬁcial evaluation is not available to us at the time of writing we conduct our evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the SpaceEval training corpus which contains&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; spatial relations QSLINKs 1 OLINKs and MOVELINKs and MOVELINK optional roles sources 1 midpoints 1 goals 1 landmarks 1 paths and motion signals</example>
		<phraseLemma>np on np which contain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was reported as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>An error analysis for our approach revealed that while many characters were extracted the coreference resolution did not link a characters different referents together and hence each name &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was reported as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a separate character which caused a drop in performance</example>
		<phraseLemma>np be report as np</phraseLemma>
	</can>
	<can>
		<phrase>To account for &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To account for variability in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; novel length we normalize the novels number of characters by its number of tokens</example>
		<phraseLemma>to account for np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; employed &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To train and test the SVM classiﬁer we used the LibSVM library and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;employed the onevsone strategy for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiclass tasks</example>
		<phraseLemma>np employ np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; according to which &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>The DP process of DPMM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;according to which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic θi for a document xi is drawn can be explained by the popular metaphor of Po´lya urn scheme equivalent to the Chinese Restaurant Process</example>
		<phraseLemma>np accord to which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as demonstrated in Figure 1</phrase>
		<frequency>9</frequency>
		<example>An algebra word problem describes a mathematical problem which can be typically modeled by an equation system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as demonstrated in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as demonstrate in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; introduces &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>As to another line Luong captures morphological composition by using neural networks and Qiu &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;introduces the morphological knowledge as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both additional input representation and auxiliary supervision to the neural network framework</example>
		<phraseLemma>np introduce np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; incorporate &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>For instance both SimRank and ASCOS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;incorporate magnitude in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the similarity computation</example>
		<phraseLemma>np incorporate np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; increases the number of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>On the other hand a high δw value &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;increases the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mentions in the N E class while low values tends to accumulate mentions in the W E class</example>
		<phraseLemma>np increase the number of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; present in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For entities present in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the KB we observe an accuracy improvement of 1 over AIDA</example>
		<phraseLemma>for np present in np</phraseLemma>
	</can>
	<can>
		<phrase>From Table 1 we observe that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From Table 1 we observe that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the algorithms exhibit comparable coreference resolution performance thus validating propagation of global semantics in C 1 EL due to the joint formulation</example>
		<phraseLemma>from table 1 we observe that np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we consider &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a special class of hypergraphs where each hyperedge consists of a designated parent node and an ordered list of child nodes</example>
		<phraseLemma>in this work we consider np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; there exists &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each mention there exists&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a unique path in the mention hypergraph to represent it</example>
		<phraseLemma>for np there exist np</phraseLemma>
	</can>
	<can>
		<phrase>It does not rely on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It does not rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervision in its extractors and generates training data for type selection from WordNet and other resources</example>
		<phraseLemma>it do not rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is associated with &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>A popular approach is to train an extractor on a corpus of sentences in which each named entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is associated with all its types in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a KB</example>
		<phraseLemma>np be associate with np in np</phraseLemma>
	</can>
	<can>
		<phrase>We take &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We take all words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence as entityoblivious context</example>
		<phraseLemma>we take np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performed better than &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Hyena &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performed better than Pearl and in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many cases extracted the largest number of types</example>
		<phraseLemma>np perform better than np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; connects &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Given a mention and context as input entity linking &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;connects the mention to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a referent entity in a knowledge base</example>
		<phraseLemma>np connect np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; yielding &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We also apply our method to rank machine translation output and conduct experiments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a ChineseEnglish document translation task yielding&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a better translation results compared with a stateoftheart baseline system</example>
		<phraseLemma>np on np yield np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; within and across &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Following the thread modeling the word sequence relationship &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;within and across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences we propose a hierarchical recurrent neural network language model consist of a sentencelevel language model and a wordlevel language model</example>
		<phraseLemma>np within and across np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; according to &lt;NP&gt; For &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The training objective &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;according to Mikolov For&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; weight matrix Ws Us Vs and hash feature weight D the parameter are trained similar to the conventional recurrent neural network</example>
		<phraseLemma>np accord to np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in term of accuracy</phrase>
		<frequency>9</frequency>
		<example>Compared with the baseline systems the proposed HRNNLM achieves signiﬁcant improvement with nearly 1 improvement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in term of accuracy&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in term of accuracy</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is built upon &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The IWSLT 1 baseline system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is built upon&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the opensource machine translation toolkit Moses at the default conﬁguration proposed by</example>
		<phraseLemma>np be build upon np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is shown as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Duchi s algorithm modiﬁed for the present problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is shown as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1</example>
		<phraseLemma>np be show as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can deal with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>In NLP such graphical models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can deal with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large incompletely observed lexicons</example>
		<phraseLemma>np can deal with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; converts &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To perform inference on a graphical model one ﬁrst &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;converts the model to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a factor graph representation</example>
		<phraseLemma>np convert np to np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we made &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each dialogue we made&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an artiﬁcial node as the root with special dummy features</example>
		<phraseLemma>for np we make np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; resulting in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Burstein and Marcu trained classiﬁers for identifying thesis and conclusion statements in student essays &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using additional automatic discourse parse features and cue words resulting in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an average Fscore of 1 for thesis and 1 for conclusion segments</example>
		<phraseLemma>np use np result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; followed by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the Englishspeaking school of essay writing and debating there is the tendency to state the central claim of a text or a paragraph in the very ﬁrst sentence followed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supporting arguments</example>
		<phraseLemma>np in np follow by np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we replace &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this setting we replace&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the edge labels predicted by the mstparser with the predictions of the base classiﬁer for argumentative function</example>
		<phraseLemma>in np we replace np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that the EG model with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; equal weighting scores slightly better than the one with optimized weighting for German but not for English</example>
		<phraseLemma>note that np with np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to refer to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the term unit to refer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both words and phrases in a snippet</example>
		<phraseLemma>we use np to refer to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is heavily dependent on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Low recall for the latter follows naturally as function word alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is heavily dependent on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; related content word alignment</example>
		<phraseLemma>np be heavily dependent on np</phraseLemma>
	</can>
	<can>
		<phrase>We show the results of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the intrinsic evaluation in Table 1 with 1 conﬁdence Wilson score interval</example>
		<phraseLemma>we show the result of np</phraseLemma>
	</can>
	<can>
		<phrase>We chose &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We chose plot synopses from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; James Bond movies and movies based on the Marvel Comics characters</example>
		<phraseLemma>we choose np from np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁrst &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ﬁrst select a number of verbs n in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the recipe from a geometric distribution</example>
		<phraseLemma>we ﬁrst np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as mentioned in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>It is important to note that the base positive predicate is actually the implicit pos operator &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as mentioned in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 however for easier representation we specify it by marking its corresponding adjective or adverb</example>
		<phraseLemma>np as mention in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depending on the context</phrase>
		<frequency>9</frequency>
		<example>The words from these pairs that appear in the original sarcastic utterances are then considered as our collection of target words that can have both a sarcastic and a literal sense &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depending on the context&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np depend on the context</phraseLemma>
	</can>
	<can>
		<phrase>Our analysis of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our analysis of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; target words where the sarcastic sense is the opposite of the literal sense is related to the idea of positive sentiment toward a negative situation” proposed by Riloff and recently studied by Joshi</example>
		<phraseLemma>we analysis of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the length of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>For each taxonomic relationt t 1 its evidence score is estimated as τ where τ is the threshold value for Score and Dist &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the length of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the shortest path between and found in WordNet</example>
		<phraseLemma>np be the length of np</phraseLemma>
	</can>
	<can>
		<phrase>We introduce &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We introduce prepostediting possibly the most basic form of interactive translation as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a touchbased interaction with iteratively improved translation hypotheses prior to classical postediting</example>
		<phraseLemma>we introduce np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; set for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The weights for all old or new models in the loglinear combination are found by tuning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a development set for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each PPE iteration</example>
		<phraseLemma>np on np set for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is decomposed into &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>This sentence pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is decomposed into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sequence of L bilingual units called tuples deﬁning a joint segmentation</example>
		<phraseLemma>np be decompose into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is projected into &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>All translation systems are based on the open source implementation of the bilingual ngram approach to MT For the NN structure each vocabularys word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is projected into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dimension space followed by two hidden layers of and units</example>
		<phraseLemma>np be project into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not correspond to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Note that this group &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not correspond to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any number in Table 1 which reports Bleu on the entire test sets</example>
		<phraseLemma>np do not correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represents the number of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>If interpreting from speech the parameter &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represents the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words the system is allowed to fall behind the speaker before being required to provide an output translation</example>
		<phraseLemma>np represent the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; gets &lt;NP&gt; of 1</phrase>
		<frequency>9</frequency>
		<example>The target side γ of the extracted rule is a positive instance and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;gets a loss of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np get np of 1</phraseLemma>
	</can>
	<can>
		<phrase>The data consisted of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The data consisted of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; approximately 1 million parallel sentences for each language pair with million tokens for each language</example>
		<phraseLemma>the datum consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; is presented in</phrase>
		<frequency>9</frequency>
		<example>A study for deception detection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on essays and product reviews is presented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np be present in</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that start with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>English speakers are aware of this constraint and judge forms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that start with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a as impossible English words</example>
		<phraseLemma>np that start with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are transformed to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>In symbolic approaches math problem sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are transformed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; certain structures by pattern matching or verb categorization</example>
		<phraseLemma>np be transform to np</phraseLemma>
	</can>
	<can>
		<phrase>This is &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is possible thanks to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the max pooling operation and that heads are often more informative than nonheads</example>
		<phraseLemma>this be np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; called &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We thus propose a new model working on charts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the CKY style called&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Chart Neural Network</example>
		<phraseLemma>np in np call np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in their experiments</phrase>
		<frequency>9</frequency>
		<example>All of these approaches are special cases of our unigram model — ie they consider particular S and sim Eger Yao and Kondrak and Eger generalize to alignments of multiple strings but likewise only consider unigram alignment models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in their experiments&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in they experiment</phraseLemma>
	</can>
	<can>
		<phrase>All of &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;All of these approaches are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; special cases of our unigram model — ie they consider particular S and sim Eger Yao and Kondrak and Eger generalize to alignments of multiple strings but likewise only consider unigram alignment models in their experiments</example>
		<phraseLemma>all of np be np</phraseLemma>
	</can>
	<can>
		<phrase>To see whether &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To see whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this relation gold standard alignments in each case yields betship changes when we vary the amount of train ter overall G 1 P transcriptions than training them ing data we run several more experiments</example>
		<phraseLemma>to see whether np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows the results of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 models on PKU development set from 1 epoch to th epoch</example>
		<phraseLemma>figure 1 show the result of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are incorporated in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Intuitively the use of the hierarchical backoff increases the Hiero grammar size since the phrases of all the granularities in the derivation trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are incorporated in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the grammar</example>
		<phraseLemma>np be incorporate in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; not only for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;not only for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrasebased models but also for syntaxbased models</example>
		<phraseLemma>np not only for np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to extract &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used Moses to extract loose bilingual phrases from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; wordaligned bilingual corpora from all methods</example>
		<phraseLemma>we use np to extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; but &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The framework is established on a translation graph that captures not only local associations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between sourceside content words and their target translations but&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; also targetside global dependencies in terms of relatedness among target items</example>
		<phraseLemma>np between np but np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which focus on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>These experiment results suggest that exploring longdistance dependencies among target translations is complementary to the previous lexical selection methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sourceside context information</example>
		<phraseLemma>np which focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is composed of &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Then we describe the objective function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is composed of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three types of errors</example>
		<phraseLemma>np which be compose of np</phraseLemma>
	</can>
	<can>
		<phrase>To investigate the impact of &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To investigate the impact of embedding dimensionality on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our BCorrRAE we tried 1 different dimensions from to with an increment of each time</example>
		<phraseLemma>to investigate the impact of np on np</phraseLemma>
	</can>
	<can>
		<phrase>This may be because &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This may be because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a larger dimension brings in much more parameters and therefore makes parameter tuning more difﬁcult</example>
		<phraseLemma>this may be because np</phraseLemma>
	</can>
	<can>
		<phrase>On the other hand using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On the other hand using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the full feature set achieves an accuracy of above 1 a precision close to 1 and a recall about 1</example>
		<phraseLemma>on the other hand use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; including &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The baseline tuning methods are batch tuning methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on kbest translations including&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MERT MIRA and PRO from Moses</example>
		<phraseLemma>np base on np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to provide &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We concatenate this primary feature vector with the embeddings of the previous and subsequent words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; contextsensitive POS predictions</example>
		<phraseLemma>np in order to provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; estimates &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Recall that the KNsmoothed tag dictionary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;estimates a nonzero probability p for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; every possible word/tag pair and that the possible tags for a given word are determinted by setting a threshold T on this probability</example>
		<phraseLemma>np estimate np for np</phraseLemma>
	</can>
	<can>
		<phrase>We computed &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To allow pruning the dictionary as described in Section 1 for each word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we computed a probability distribution p using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unsmoothed relative frequencies</example>
		<phraseLemma>np we compute np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improve over &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Inspired by Habash s simple set of rules for determining case on gold dependency trees we reimplemented these rules to work with our different dependency representation and extended them to include state assignment in a manner similar to Alkuhlani 1 These rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improve over the baseline by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 absolute in Nominal Diac accuracy but produce no gains in All Words setting</example>
		<phraseLemma>np improve over np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We also ﬁnd that the set of dot products between the word embeddings for a verb and those for case particles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used as a substitution for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; case frames</example>
		<phraseLemma>np can be use as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is estimated based on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The quantity S ranges between 1 and 1 and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is estimated based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large quantities of autoparsed data</example>
		<phraseLemma>np be estimate base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are relevant for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>This is done by employing an attention model that ﬁnds within the contextual words the words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are relevant for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each prediction</example>
		<phraseLemma>np that be relevant for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that learns &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The continuous bagofwords is one of the many models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that learns word representations from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; raw textual data</example>
		<phraseLemma>np that learn np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; sets &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>To prevent cycles the Shift action checks and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;sets a bit in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the new boolean vector S</example>
		<phraseLemma>np set np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used by &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Our parsers are trained and evaluated on the same data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used by Tetreault in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their recent bakeoff of leading dependency parsing models</example>
		<phraseLemma>np use by np in np</phraseLemma>
	</can>
	<can>
		<phrase>In this experiment we use &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this experiment we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a setup similar to</example>
		<phraseLemma>in this experiment we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are optimized on &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>All systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are optimized on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dev corpus named dev here</example>
		<phraseLemma>np be optimize on np</phraseLemma>
	</can>
	<can>
		<phrase>Were trained on &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Latest results in WMT 1 – despite the fact that our models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were trained on WMT 1 with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; slightly less data we test them on newstest to demonstrate that they can generalize well to different test sets</example>
		<phraseLemma>np be train on np with np</phraseLemma>
	</can>
	<can>
		<phrase>Our analysis shows that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our analysis shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; attentionbased NMT models are superior to nonattentional ones in many cases for example in translating names and handling long sentences</example>
		<phraseLemma>we analysis show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; so that &lt;CL&gt;</phrase>
		<frequency>9</frequency>
		<example>GEOS uses an equation analyzer and preprocesses question text by replacing =” &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with equals” and replacing mathematical terms with a dummy noun so that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dependency parser does not fail</example>
		<phraseLemma>np with np so that np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; similar to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For diagram parsing similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Seo we assume that GEOS has access to ground truth optical character recognition for labels in the diagrams</example>
		<phraseLemma>for np similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; enables &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Second we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;enables a systematic study of linguistic ambiguities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; visual contexts</example>
		<phraseLemma>np enable np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may correspond to &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>The corpus addresses several classes of semantic quantiﬁcation ambiguities in which a syntactically unambiguous sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may correspond to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different logical forms</example>
		<phraseLemma>np may correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is scored by &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>Each detection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is scored by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its conﬁdence from the object detector f and each object track &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is scored by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a motion coherence metric g which determines if the motion of the track agrees with the underlying optical ﬂow</example>
		<phraseLemma>np be score by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>This extension enables us to represent ambiguities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which a given sentence has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple logical interpretations for the same syntactic parse</example>
		<phraseLemma>np in which np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; then using &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the KB and then using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these representations to infer missing relationships</example>
		<phraseLemma>np in np then use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to predict &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>A separate line of research into KB completion can be broadly construed as performing some kind of inference over graphs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; missing instances in a knowledge base</example>
		<phraseLemma>np in order to predict np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; is &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As morphology is a strong indicator for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntax in many languages a much effort has been spent engineering features</example>
		<phraseLemma>as np be np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use them for &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>While the PTB dataset provides standard train tuning and test splits there are no tuning sets in the datasets in other languages so we withdraw the last sentences from the training dataset and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use them for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tuning</example>
		<phraseLemma>np use they for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which scores &lt;NP&gt;</phrase>
		<frequency>9</frequency>
		<example>We amend the above setting by introducing a novel layer on the top of the compositional one &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which scores&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the linguistic plausibility of the composed sentence or phrase vector with regard to both syntax and semantics</example>
		<phraseLemma>np which score np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; keeps &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>H &amp;gt S Speciﬁcally the underlying state h &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the game engine keeps&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; track of attributes such as the players location her health points time of day etc</example>
		<phraseLemma>np in np keep np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is evident from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The need for a better semantic representation of the text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is evident from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the average performance of this representation in playing MUDgames</example>
		<phraseLemma>np be evident from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; produces &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our setup the LSTM network takes in word embeddings wk from the words in a description s and produces&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; output vectors xk at each step</example>
		<phraseLemma>np in np produce np</phraseLemma>
	</can>
	<can>
		<phrase>We consider &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>If the game does not provide such clues for the current state &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we consider all objects in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the game</example>
		<phraseLemma>np we consider np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; comes from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;information in FreeBase comes from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; various sources including Wikipedia and domainspeciﬁc databases plus user content generation and correction</example>
		<phraseLemma>np in np come from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; would be to use &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>One way of achieving this goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;would be to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ρ on the union of training and test instances but this could lead to misleadingly high correlation coefﬁcients since this method would include the labels of the training instances in the evaluation</example>
		<phraseLemma>np would be to use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; showed that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>A signiﬁcance test &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with bootstrap resampling showed that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all pairwise comparisons are statistically signiﬁcant at p 1</example>
		<phraseLemma>np with np show that np</phraseLemma>
	</can>
	<can>
		<phrase>It is not surprising that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is not surprising that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distributional models cannot make such subtle distinction between presidential and parliamentary systems</example>
		<phraseLemma>it be not surprising that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produced by &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In our work we use the annotation layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produced by Herbelot and Vecchi for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the McRae norms</example>
		<phraseLemma>np produce by np for np</phraseLemma>
	</can>
	<can>
		<phrase>We estimate &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We estimate the coefﬁcients of the function using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; partial least squares regression as implemented in the R pls package</example>
		<phraseLemma>we estimate np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which includes &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The result on the MTanimals test set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which includes animals from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the AD and QMR datasets shows that this category fares indeed very well at ρ = 1</example>
		<phraseLemma>np which include np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; when &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>This has the effect that when the mapped quantiﬁer equals the gold quantiﬁer the system scores 1 when the mapped value deviates from the gold standard but produces a true sentence the system gets a partial score proportional to the distance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between its output and the gold data when&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the mapping results in a false sentence the system is penalised with minus points</example>
		<phraseLemma>np between np when np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as this is &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We evaluate on the test instances of MTAD &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as this is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the only dataset containing a fair number of negatively quantiﬁed conceptpredicate pairs</example>
		<phraseLemma>np as this be np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we need &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In SMT we need&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a synchronous grammar to simultaneously parse an input graph and produce translations</example>
		<phraseLemma>in np we need np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which can be used for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In this paper we deﬁne a synchronous ERG over dependency graphs as a dependency graphtostring grammar &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which can be used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MT N is a ﬁnite set of nonterminal symbols</example>
		<phraseLemma>np which can be use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; covering &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Given a wordaligned dependency graphstring pair P = hG e ∼i let Gj stand for the subgraph i &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;covering words from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; position i to position j Gj is a dependencygraph fragment</example>
		<phraseLemma>np cover np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; induce &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Furthermore we can use the same rule extraction algorithm as that in HPB except that we need to check if a span of a source sentence indicates a dependencygraph fragment in which case we keep the dependency structure and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;induce a nonterminal for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the fragment</example>
		<phraseLemma>np induce np for np</phraseLemma>
	</can>
	<can>
		<phrase>With &lt;NP&gt; of using &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;With a restriction of using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; contiguous edges our translation model built using this grammar can decode an input dependency graph which is directly converted from a dependency tree in cubic time using the CYK algorithm</example>
		<phraseLemma>with np of use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; in terms of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Therefore PETs expose maximal sharing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between different permutations in terms of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both phrases and their reordering</example>
		<phraseLemma>np between np in term of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; results in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Learning the splits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a latent treebank of PETs results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Reordering PCFG which we use to parse input source sentences into splitdecorated trees ie the labels are the splits of prime permutations</example>
		<phraseLemma>np on np result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; whereas &lt;NP&gt; does not</phrase>
		<frequency>8</frequency>
		<example>Hiero conditions its reordering on the lexical target side &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;whereas the Reordering Grammar does not&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np whereas np do not</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; speciﬁcally designed for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Our model is completely monolingual and unlexicalized in contrast with the Latent SCFG used in Our Latent PCFG label splits are deﬁned as reﬁnements of prime permutations ie &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;speciﬁcally designed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; learning reordering whereas aims at learning label splitting that helps predicting NDTs from source sentences Our model exploits all PETs and all derivations both during training and during inferences</example>
		<phraseLemma>np speciﬁcally design for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is separated from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>GMT today &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is separated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the PP as an NP and is mistaken as the object</example>
		<phraseLemma>np be separate from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For RWGD we combine the translation models of RW and GD &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by ﬁllup combination where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all entries in the tables of RW are preserved and entries from the tables of GD are added if new</example>
		<phraseLemma>np by np where np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows the number of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the number of verbs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the translations of the test sentences produced by GD RW RWGD as well as the number in the gold reference translation</example>
		<phraseLemma>table 1 show the number of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are not &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Many tweets that contain a country name &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are not relevant nor references to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the geolocated country itself</example>
		<phraseLemma>np be not np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with respect to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In the experiments reported below we set the prior distributions on η β and ψ to be standard normals and perform maximum a posteriori inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with respect to η β and ψ in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; turn for a total of iterations</example>
		<phraseLemma>np with respect to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; how well &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Moreover the participants also rated on a ﬁvepoint scale &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;how well&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each derived characteristic matched their perceptions of themselves and their ratings suggest that the inferred characteristics largely matched their selfperceptions</example>
		<phraseLemma>np how well np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are closest to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Instead of establishing a static semantic connection between hashtags and entities we are interested in dynamically linking the hashtags to entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are closest to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the underlying topics during burst time periods of the hashtags</example>
		<phraseLemma>np that be closest to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is measured as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The mention similarity fm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is measured as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the aggregation of link priors of the entity e over all mentions in all tweets with the hashtag h where q is the frequency of the mention m over all mentions of e in all tweets of h To compute fc we ﬁrst construct the contexts for hashtags and entities</example>
		<phraseLemma>np be measure as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; get &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>As illustrated in Figure 1 the entities closer to the Olympics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;get more updates in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the revisions of their Wikipedia articles with subsequent links pointing to articles of more distant entities</example>
		<phraseLemma>np get np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; randomly from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We then pick up hashtags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;randomly from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each cluster resulting in hashtags in total</example>
		<phraseLemma>np randomly from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by combining &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For each input set we generate candidate summaries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by combining whole sentences from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the summaries generated by different systems</example>
		<phraseLemma>np by combine np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; to generate &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Zajic tries a pipeline strategy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with heuristics to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple candidate compressions and extract from this compressed sentences</example>
		<phraseLemma>np with np to generate np</phraseLemma>
	</can>
	<can>
		<phrase>Were asked to rate &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In addition to annotations used to compute human coverage scores human assessors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were asked to rate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the linguistic quality of summaries under 1 different criteria providing ratings from A to E with A denoting highest and E least quality rating</example>
		<phraseLemma>np be ask to rate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was taken to be &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The ROUGE 1 ROUGE 1 and ROUGEL measures were used and a humanwritten reference tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was taken to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the gold standard</example>
		<phraseLemma>np be take to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; except on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>After the text of the window was extracted we performed a similar analysis as the last one &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;except on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a smaller set of sentences</example>
		<phraseLemma>np except on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be extracted from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Hence we can say that the more formal the subject or the article the better the tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the article</example>
		<phraseLemma>np can be extract from np</phraseLemma>
	</can>
	<can>
		<phrase>We experiment with using &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We experiment with using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different layers of the CNN and ﬁnd that performance is not affected signiﬁcantly in either case obtaining a slight improvement for the relatedness task but no improvement for genuine similarity</example>
		<phraseLemma>we experiment with use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; so in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>If however we are interested in relatedness related properties may just as well be encoded deeper in the network &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;so in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the layers preceding FC 1 rather than in FC 1 itself</example>
		<phraseLemma>np so in np</phraseLemma>
	</can>
	<can>
		<phrase>We have presented &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We have presented a novel approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bilingual lexicon induction that uses convolutional neural networkderived visual features</example>
		<phraseLemma>we have present np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained on &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>These identifiers were used as features for creating sentiment classifiers based on Binary/Multiclass SVM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained on bag of words representation using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; libSVM library</example>
		<phraseLemma>np train on np use np</phraseLemma>
	</can>
	<can>
		<phrase>We combined &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We combined the text data from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all English Datasets described above to train the word embeddings using Word 1 Vec toolkit and RAE</example>
		<phraseLemma>we combine np from np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we address the problem of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we address the problem of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; generic extractive summarization of reviews a task commonly known as Opinion Summarization</example>
		<phraseLemma>in this paper we address the problem of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is positive toward &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In the ﬁrst clause the writer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is positive toward&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Imam and Prophet as expressed by may God be satisﬁed with him and peace be upon him respectively</example>
		<phraseLemma>np be positive toward np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to express &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Moreover both corpora contain expressive subjective element annotations which pinpoint speciﬁc expressions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to express&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; subjectivity</example>
		<phraseLemma>np use to express np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to provide &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For each opinion y a ground atom ETARGET is created for each eTarget candidate t ET 1 considers all the nouns and verbs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the sentence to provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a full recall of eTargets</example>
		<phraseLemma>np in np to provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that had &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The annotation task was ﬁrst conducted on a small sample of similes to select workers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that had high annotation agreement with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each other and gold standard labels we prepared</example>
		<phraseLemma>np that have np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are reported for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Moreover in order to increase the impact on audience the same stories and events &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are reported for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mutliple times</example>
		<phraseLemma>np be report for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; capture &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Existing caption datasets provide images paired with captions but such brief image descriptions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;capture only a subset of the content in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each image</example>
		<phraseLemma>np capture np in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we investigate &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the latter we investigate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different architectures of increasing complexities</example>
		<phraseLemma>for np we investigate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are passed through &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>These are multiplied by corresponding weight matrices and the resulting vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are passed through&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an activation function to calculate the hidden vector at the current time step</example>
		<phraseLemma>np be pass through np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have proven to be &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have proven to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; effective models of semantic representation of words in various NLP tasks</example>
		<phraseLemma>np have prove to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; becomes &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We run this algorithm sequentially ie the time t¯ returned from Algorithm 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;becomes starting time s in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the next iteration</example>
		<phraseLemma>np become np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; similar to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Our baseline for comparison is the Relational Topic Model which jointly captures topics and binary link &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicators in a style similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervised LDA instead of modeling links alone eg as in the Latent Multigroup Membership Graph model</example>
		<phraseLemma>np in np similar to np</phraseLemma>
	</can>
	<can>
		<phrase>We demonstrate &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We demonstrate improvements in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; link prediction as measured by predictive link rank and provide both qualitative and quantitative perspectives on the improvements achieved by the model</example>
		<phraseLemma>we demonstrate np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are assigned to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Following Polson and Scott by introducing an auxiliary variable λdd 1 we derive the conditional probability of a topic assignment where Nkv denotes the count of word v assigned to topic k Ndk is the number of tokens in document d &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are assigned to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic k Marginal counts are denoted by · dn denotes that the count excludes token n in document d d 1 denotes the indexes of documents which are linked to document d πdn is estimated based on the maximal ld k path assumption</example>
		<phraseLemma>np that be assign to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; there is &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>As ground truth &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;there is a mentioning link from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; A to B Table 1 shows this links PLR in the mentioning models which generally improves with model sophistication</example>
		<phraseLemma>np there be np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; comparing &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Since the model decomposes across mentions we train by treating them as independent predictions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with multiple gold outputs comparing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the inferred link with the gold link that is scored highest under the current model</example>
		<phraseLemma>np with np compare np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses a small number of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>With this task we explore a case in which there is relatively little training data and the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses a small number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dense features</example>
		<phraseLemma>np use a small number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improves the quality of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In summary we have shown that distributional initialization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improves the quality of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings for rare words</example>
		<phraseLemma>np improve the quality of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; representing &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>This model treats each relationship as a translation vector operating &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the embedding representing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the entities</example>
		<phraseLemma>np on np represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are collected for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In a typical annotation task ﬁve or ten labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are collected for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an instance and are aggregated together into an integrated label</example>
		<phraseLemma>np be collect for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; removes &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Filtering training instances by item agreement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;removes low agreement instances from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training set</example>
		<phraseLemma>np remove np from np</phraseLemma>
	</can>
	<can>
		<phrase>We converted &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The POS tagset was the universal tag set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we converted Gimpel s tags to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the universal tagset using Hovy s mapping</example>
		<phraseLemma>np we convert np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at least for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>This result justiﬁes the use of ofﬂine data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at least for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the similarity task</example>
		<phraseLemma>np at least for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to obtain &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Empirically this hybrid method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; high quality correlated words</example>
		<phraseLemma>np be able to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>Second we ﬁnd that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Second we ﬁnd that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; somewhat surprisingly compositional training also improves upon stateoftheart performance for knowledge base completion which is a special case of answering unit length path queries</example>
		<phraseLemma>1 we ﬁnd that np</phraseLemma>
	</can>
	<can>
		<phrase>On &lt;NP&gt; we show that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On both tasks we show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the compositional training strategy proposed in Section 1 leads to substantial performance gains over standard singleedge training</example>
		<phraseLemma>on np we show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on this data</phrase>
		<frequency>8</frequency>
		<example>To give another perspective our accuracy is close to that of the fully supervised approach of which gives 1 accuracy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on this data&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on this datum</phraseLemma>
	</can>
	<can>
		<phrase>We incorporate &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We incorporate a bilingual dictionary as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of soft constraints on the model such that it learns similar representations for each word and its translation</example>
		<phraseLemma>we incorporate np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relies on the fact that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>Delexicalized parsing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relies on the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; identical partofspeech inventories are highly informative of dependency relations and that there exists shared dependency structures across languages</example>
		<phraseLemma>np rely on the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we compare &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we compare&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our joint training approach with baseline methods of supervised learning in the target language and cascaded learning of source and target parsers</example>
		<phraseLemma>in this section we compare np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that we call &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In the simplest implementation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that we call&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LSTM the input layer has dimensions</example>
		<phraseLemma>np that we call np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We compare the output quality of the two models on different scales of training data and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different amounts of training time</example>
		<phraseLemma>np also on np</phraseLemma>
	</can>
	<can>
		<phrase>However the performance of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Ngram model ﬂattens when the training data size reaches beyond 1 million</example>
		<phraseLemma>however the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is adapted from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The language model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is adapted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a standard feedforward neural network language model particularly the class of NNLMs described by Bengio</example>
		<phraseLemma>np be adapt from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; directly for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In addition to Banko there has been some work using statistical machine translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;directly for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; abstractive summary</example>
		<phraseLemma>np directly for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Therefore our goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to leverage citationcontext in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the reference article to correctly reﬂect the reference paper</example>
		<phraseLemma>np be to np in np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁnd &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Here we opt for eigenvectors and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we ﬁnd the most central sentences in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each group by using the power method” which iteratively updates the eigenvector until convergence</example>
		<phraseLemma>np we ﬁnd np in np</phraseLemma>
	</can>
	<can>
		<phrase>We attribute &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We attribute the low performance of MMR to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its sub optimal greedy selection of sentences from relatively long scientiﬁc articles</example>
		<phraseLemma>we attribute np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on average &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For discourse method there are 1 different discourse facets and for community method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on average&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 communities are detected</example>
		<phraseLemma>np on average np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is parameterized by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>A DP denoted by G ∼ DP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is parameterized by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a base measure H and a concentration parameter α</example>
		<phraseLemma>np be parameterize by np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we employ &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we employ&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; smoothing as follows</example>
		<phraseLemma>in this paper we employ np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that contains &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>It constructs a feature space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that contains all the distinct words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a language</example>
		<phraseLemma>np that contain np in np</phraseLemma>
	</can>
	<can>
		<phrase>From Table 1 &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the readability formulae perform poorly on either the precision or recall measure and their measure values are generally the poorest</example>
		<phraseLemma>from table 1 np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we provide &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Section 1 we provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a description of our datasets followed by a comprehensive evaluation in Section 1</example>
		<phraseLemma>in np we provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shared between &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We found that after preprocessing between 1 and 1 tokens &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shared between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the datasets</example>
		<phraseLemma>np be share between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are connected with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Feature groups &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are connected with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bars if they are not signiﬁcantly different given α = 1</example>
		<phraseLemma>np be connect with np</phraseLemma>
	</can>
	<can>
		<phrase>The score assigned by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The score assigned by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the AES system will be compared against the human assigned score to measure their agreement</example>
		<phraseLemma>the score assign by np</phraseLemma>
	</can>
	<can>
		<phrase>Since &lt;NP&gt; may have &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since the source and target domain may have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different score ranges we scale the scores linearly to range from to 1</example>
		<phraseLemma>since np may have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was sufﬁcient for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>By the above argument alone one might have thought that an overwhelming large number of source domain essays &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was sufﬁcient for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target domain</example>
		<phraseLemma>np be sufﬁcient for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are described in detail in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>All the inhouse lexicons &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are described in detail in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the supplementary material</example>
		<phraseLemma>np be describe in detail in np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We deﬁne the MRR of a claim with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no CDE to be 1</example>
		<phraseLemma>we deﬁne np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in the case of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In an online learning problem samples are presented to the learning architecture at a given rate and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in the case of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a web crawling agent most of these are unlabeled</example>
		<phraseLemma>np as in the case of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is trained using &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is trained using a stochastic gradient descent algorithm on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a per impression basis with regularization to avoid overﬁtting</example>
		<phraseLemma>np be train use np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is part of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>If an extracted instance contains an entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is part of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a seed and if the associated entity in the instance is the same as in in the seed then the extraction is considered positive</example>
		<phraseLemma>np which be part of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shows the results for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Table 1 a &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows the results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the BREDS system while Table 1 b &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows the results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Snowball a modiﬁed Snowball in which a relational pattern based on ReVerb is used to select the words for the BET context</example>
		<phraseLemma>np show the result for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; due to the use of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>NER problems and incorrect relational patterns extraction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;due to the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a shallow heuristic that only captures local relationships</example>
		<phraseLemma>np due to the use of np</phraseLemma>
	</can>
	<can>
		<phrase>This is equivalent to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is equivalent to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CRFNAME but includes known types</example>
		<phraseLemma>this be equivalent to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>We describe a procedure for reducing this noise by using label propagation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a graph in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the nodes are entity mentions and mentions are coupled when they occur in coordinate list structures</example>
		<phraseLemma>np on np in which np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our models by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; macroaveraged using the ofﬁcial evaluation script</example>
		<phraseLemma>we evaluate np by np</phraseLemma>
	</can>
	<can>
		<phrase>We would like to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We would like to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Sheng Zhang Weiwei Sun and Liwei Chen for their helpful discussions</example>
		<phraseLemma>we would like to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are addressed in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Thus rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are addressed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a languageindependent fashion and the simpliﬁed English resources are written as normalization ﬁles from which pattern and normalization ﬁles for all languages are derived</example>
		<phraseLemma>np be address in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are translated to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>As shown in Figure 1 the patterns of the simpliﬁed English resources &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are translated to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all languages in the world using Wiktionary as translation resource</example>
		<phraseLemma>np be translate to np</phraseLemma>
	</can>
	<can>
		<phrase>In addition if &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; patterns in the new language start with a lower case character we allow upper case and lower case for the respective character as shown in Figure 1</example>
		<phraseLemma>in addition if np</phraseLemma>
	</can>
	<can>
		<phrase>They use &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They use a CRF tagger with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a BIOSE encoding that tags individual characters not words since word segmentation errors are especially problematic for NER</example>
		<phraseLemma>they use np with np</phraseLemma>
	</can>
	<can>
		<phrase>We also report results for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also report results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the joint method trained with the characterposition model which performed the best on dev data for joint training</example>
		<phraseLemma>we also report result for np</phraseLemma>
	</can>
	<can>
		<phrase>We remove &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>First since adjectives adverbs and modal verbs can hardly change the type distribution of arguments in a relation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we remove these words from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a pattern</example>
		<phraseLemma>np we remove np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is used in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For comparison we use Pointwise Mutual Information as our baseline model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other selectional preference tasks</example>
		<phraseLemma>np which be use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to generate &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>By maximizing the support of both arguments simultaneously our system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; humanreadable type pairs for a binary relation from Open IE systems</example>
		<phraseLemma>np be able to generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; evaluate &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Formally given a set of KB entities CCR is to ﬁlter relevant documents from a stream corpus and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;evaluate their citationworthiness to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target entities</example>
		<phraseLemma>np evaluate np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are implemented by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Two variants of LDTM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are implemented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; modeling the latent types with document sourcebased and topicbased features respectively</example>
		<phraseLemma>np be implement by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; we obtain &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Interestingly our experiment results indicate that our lightweight features are actually very powerful when used in conjunction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the proposed feature transformation we obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a significant performance improvement over the best challenge system</example>
		<phraseLemma>np with np we obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are also used as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Similarly noun phrases in a verbose query &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are also used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; candidates for key concepts</example>
		<phraseLemma>np be also use as np</phraseLemma>
	</can>
	<can>
		<phrase>In order to minimize &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to minimize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; noise and to enhance the sentiment analysis validity it is crucial to work with the documents well adapted to the task of risk sentiment analysis</example>
		<phraseLemma>in order to minimize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which do not appear in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Hence all words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which do not appear in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst experiments dictionaries are removed</example>
		<phraseLemma>np which do not appear in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; derived from &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Many of these approaches model reviews primarily with contentbased features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;derived from the words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the reviews</example>
		<phraseLemma>np derive from np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that represent &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For each variant we determine all sentiment ﬂows &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that represent at least 1 of all reviews in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a given training set</example>
		<phraseLemma>np that represent np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; each of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We learn a mapping from the edit distance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between a reviews sentiment ﬂow and each of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these ﬂows to global sentiment</example>
		<phraseLemma>np between np each of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we learn &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Having classiﬁed local sentiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in all reviews we learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a random forest classiﬁer on each feature type and different feature sets for all combinations of training and test domain</example>
		<phraseLemma>np in np we learn np</phraseLemma>
	</can>
	<can>
		<phrase>Our work is related to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our work is related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the line of work on using neural networks for sentiment analysis</example>
		<phraseLemma>we work be related to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>As can be seen from the table the neural models give higher Fscores than the discrete CRF models on the English dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while comparable overall Fscores on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Spanish dataset</example>
		<phraseLemma>np while np on np</phraseLemma>
	</can>
	<can>
		<phrase>We use a combination of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a combination of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unigram and bigram models and calculate the conditional probability p for linearchain CRF by Equation</example>
		<phraseLemma>we use a combination of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; denote &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In the upper part of Figure 1 a rectangle and an arrow &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;denote a bunsetstu phrase and a syntactic dependency between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two phrases respectively and in each phrase we show Japanese words based on the Hepburn system and their English translations in parentheses</example>
		<phraseLemma>np denote np between np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; belongs to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If a clue expression belongs to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than one semantic category as in ni” of Table 1 the feature value is a set of these categories</example>
		<phraseLemma>if np belong to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are central to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The semantic concepts of entailment and contradiction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are central to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all aspects of natural language meaning from the lexicon to the content of entire texts</example>
		<phraseLemma>np be central to np</phraseLemma>
	</can>
	<can>
		<phrase>In order to ensure that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to ensure that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our labeling scheme assigns a single correct label to every pair we must select one of these approaches across the board but both choices present problems</example>
		<phraseLemma>in order to ensure that np</phraseLemma>
	</can>
	<can>
		<phrase>This is different from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is different from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the maybe correct category because its impossible for the dogs to be both running and sitting</example>
		<phraseLemma>this be different from np</phraseLemma>
	</can>
	<can>
		<phrase>Nearly all of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Nearly all of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the examples received a majority label indicating broad consensus about the nature of the data and categories</example>
		<phraseLemma>nearly all of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when trained on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In addition though the LSTM and the lexicalized model show similar performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current full corpus the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets</example>
		<phraseLemma>np when train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Nonetheless as we will show it still represents the argument and modiﬁer attachment decisions that have motivated previous SRL deﬁnitions and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; crucial importance for semantic understanding in a range of NLP tasks such as machine translation and coreference resolution</example>
		<phraseLemma>np which be of np</phraseLemma>
	</can>
	<can>
		<phrase>To investigate &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To investigate the similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the annotation schemes we measured the overlap between the newswire domain sentences of our QASRL dataset and the PropBank dataset</example>
		<phraseLemma>to investigate np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>From such a large number of texts entity disambiguation is a critical step &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when extracting text information from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these messages and for various applications such as natural language processing and knowledge acquisition</example>
		<phraseLemma>np when np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; generated &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In our experiments a dataset that included 1 target entities and 1 microblogs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;generated a directed graph with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; billions of edges which required more computer memory than was available even when using a sparse matrix</example>
		<phraseLemma>np generate np with np</phraseLemma>
	</can>
	<can>
		<phrase>The relevance between &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The relevance between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; R and D is normally determined by the agreement of RM and DM</example>
		<phraseLemma>the relevance between np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; we employ &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a modification we employ&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Topic Model for biography modeling</example>
		<phraseLemma>as np we employ np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is approximated by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The topictopic relevance P &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is approximated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Hellinger distance estimation between the topic models of tR and tC</example>
		<phraseLemma>np be approximate by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; mainly due to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>A particular problem of ListNet however is the high computation complexity in model training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;mainly due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the large number of object permutations involved in computing the gradients</example>
		<phraseLemma>np mainly due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; varies from 1 to</phrase>
		<frequency>8</frequency>
		<example>In each ﬁgure the number of document lists &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;varies from 1 to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np vary from 1 to</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; apply &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For the future work we plan to study Topk ListNet models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with other databases and apply&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stochastic learning approach to other listwise learning to rank methods</example>
		<phraseLemma>np with np apply np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; were of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>On average the ground networks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in these runs were of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the order of 1 × 1 ground clauses</example>
		<phraseLemma>np in np be of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; resulted in &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Praline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;resulted in a x speedup over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ERMLN explained in part by much smaller ground networks with only clauses on average</example>
		<phraseLemma>np result in np over np</phraseLemma>
	</can>
	<can>
		<phrase>In order to take &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to take&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relation paths into consideration relation paths should also be represented in a lowdimensional space</example>
		<phraseLemma>in order to take np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depends on &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>It is straightforward that the semantic meaning of a relation path &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depends on all relations in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this path</example>
		<phraseLemma>np depend on np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been successfully used in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Resource allocation over networks was originally proposed for personalized recommendation and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been successfully used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; information retrieval for measuring relatedness between two objects</example>
		<phraseLemma>np have be successfully use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is of &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The order of words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is of critical importance for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accurate assessment of a context and CM takes it into account</example>
		<phraseLemma>np be of np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; respectively</phrase>
		<frequency>8</frequency>
		<example>Two test sets are used to evaluate sentencelevel name prediction and wordlevel name error detection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the BOLT phase 1 and 1 evaluations respectively&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np base on np respectively</phraseLemma>
	</can>
	<can>
		<phrase>For instance while &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst two shared tasks have focused on extracting spatial relations between stationary objects SpaceEval examines for the ﬁrst time spatial relations on objects in motion</example>
		<phraseLemma>for instance while np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which achieved &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>To ensure that we have a stateoftheart baseline system for spatial relation extraction we employ our SpaceEval participating system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which achieved the best results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; task 1 a in the ofﬁcial SpaceEval evaluation</example>
		<phraseLemma>np which achieve np on np</phraseLemma>
	</can>
	<can>
		<phrase>Recall from &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Recall from the introduction that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these sieves are ordered as a pipeline</example>
		<phraseLemma>recall from np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to create &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Note that roles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to create&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; structured features</example>
		<phraseLemma>np be use to create np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is propagated to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>As we can see this mistake &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is propagated to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SRCTs generated in later sieves</example>
		<phraseLemma>np be propagate to np</phraseLemma>
	</can>
	<can>
		<phrase>A major source of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;A major source of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recall/precision error stems from the systems inability to exploit contextual cues that are reliable indicators of a particular role</example>
		<phraseLemma>a major source of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we provide &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each ingredientinstruction edge we provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 features to classify whether the edge exists or not</example>
		<phraseLemma>for np we provide np</phraseLemma>
	</can>
	<can>
		<phrase>This paper addresses &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This paper addresses&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text categorization problem that training data may derive from a different time period from the test data</example>
		<phraseLemma>this paper address np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; mapping &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>This involves segmentation of token sequences to obtain mention boundaries and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;mapping relevant token spans to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predeﬁned entity categories</example>
		<phraseLemma>np mapping np to np</phraseLemma>
	</can>
	<can>
		<phrase>As pointed out by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As pointed out by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Finkel and Manning named entities are often nested</example>
		<phraseLemma>as point out by np</phraseLemma>
	</can>
	<can>
		<phrase>In order to solve &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to solve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the optimization problem described above one needs to compute the values of the gradient scores in Equation 1</example>
		<phraseLemma>in order to solve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the subject of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Moreover if the named entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the subject of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the clause and if the clause contains a direct object we form a new lexical type by adding the direct object as a noun modiﬁer of the deverbal noun</example>
		<phraseLemma>np be the subject of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that cooccur in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Our ﬁnal extractor leverages a large unlabeled corpus to ﬁnd entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that cooccur in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similar contexts</example>
		<phraseLemma>np that cooccur in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; to construct &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Our corpusbased extractor &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses the input sentence to construct&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of relevant queries and the word 1 vec results and a KB</example>
		<phraseLemma>np use np to construct np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we make use of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In FINET we make use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; patterns in most of our extractors and of the distributional hypothesis in our corpusbased extractor</example>
		<phraseLemma>in np we make use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; seems to have &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For λ ≤ 1 the regularizer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;seems to have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; little impact</example>
		<phraseLemma>np seem to have np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; they have &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the string case they have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inﬁnitely many rows and columns indexed by possible strings</example>
		<phraseLemma>in np they have np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we will present &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In section 1 we will present&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dual decomposition method that decomposes the original complex problem into many small subproblems that are free of cycles and high degree nodes</example>
		<phraseLemma>in np we will present np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; varied from &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Similarly on the CELEX data the runtime on Model 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;varied from about 1 hour to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nearly 1 days</example>
		<phraseLemma>np vary from np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to provide &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Our papers main contribution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to provide a discourse parsing model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiparty chat dialogue trained on a large corpus we have developed annotated with full discourse structures</example>
		<phraseLemma>np be to provide np for np</phraseLemma>
	</can>
	<can>
		<phrase>In order to calculate &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to calculate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the recursive head we identify all the DUs with no incoming links if they are CDUs we recursively apply the algorithm until we get an EDU</example>
		<phraseLemma>in order to calculate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; later in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Thus the nature of dialogue imposes an essential and important constraint on the attachment process that is not present for monologue or singleauthored text where an EDU may be dependent upon any EDU &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;later in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ordering or not</example>
		<phraseLemma>np later in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is trained only on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The outer 1 fold CV is for evaluation only ie to ensure that the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is trained only on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training data and tested only on test data</example>
		<phraseLemma>np be train only on np</phraseLemma>
	</can>
	<can>
		<phrase>Studies have shown that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Studies have shown that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such tasks can beneﬁt from an explicit alignment component</example>
		<phraseLemma>study have show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; to perform &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Sultan treat alignment as a bipartite matching problem and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use a greedy algorithm to perform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; onetoone word alignment</example>
		<phraseLemma>np use np to perform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to score &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The similarity of these two representations as measured by their dot product &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to score&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; possible roles for candidate arguments within a graphical model</example>
		<phraseLemma>np be use to score np</phraseLemma>
	</can>
	<can>
		<phrase>In addition there are &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition there are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; noncore roles that encapsulate further arguments of a frame such as temporal and locative adjuncts</example>
		<phraseLemma>in addition there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; use &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Both the tagger and the parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as the SRL models use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word cluster features</example>
		<phraseLemma>np as well as np use np</phraseLemma>
	</can>
	<can>
		<phrase>In addition to providing &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition to providing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a natural way of incorporating uncertainty and similarity into models continuousvalued variables allow the inference objective to be formulated as convex optimization making MAP inference extremely efﬁcient with empirical performance that scales linearly with the number of ground rules</example>
		<phraseLemma>in addition to provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is important to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Despite the fact that such language &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is important to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our everyday lives there has been relatively little effort to design algorithms that can automatically convert it into an actionable form</example>
		<phraseLemma>np be important to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to recover &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>A unique challenge in procedural text understanding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to recover&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; how different arguments ﬂow through a chain of actions the results of intermediate actions</example>
		<phraseLemma>np be to recover np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we ﬁrst describe &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we ﬁrst describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our structured representation of recipe text then we deﬁne how components of the recipe connect</example>
		<phraseLemma>in this section we ﬁrst describe np</phraseLemma>
	</can>
	<can>
		<phrase>We compare our model to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our model to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sequential baselines using both the output of our segmentation system and oracle segmentations</example>
		<phraseLemma>we compare we model to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>These adjectives &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; comparative morphemes indicating less or more of a particular quality on a scale</example>
		<phraseLemma>np can be use with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; as well as on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We report simulated experiments that yield very large improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on classical evaluation metrics as well as on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens conﬁrming the promising prospects of the novel translation scenarios offered by our approach</example>
		<phraseLemma>np on np as well as on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is obtained as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>During training the segmentation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is obtained as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a byproduct of source reordering for details</example>
		<phraseLemma>np be obtain as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to estimate &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The standard feedforward structure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to estimate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the translation models sketched in the previous section</example>
		<phraseLemma>np be use to estimate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outputs &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We therefore assume that the NN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outputs a positive score b for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each word w given its context c this score is simply computed as b = exp where a is the activation at the output layer θ denotes all the network free parameters</example>
		<phraseLemma>np output np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is based on &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The construction of the lattice &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is based on the extraction of phrase pairs from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word alignments between a selected best MT system hypothesis and the other translation hypotheses</example>
		<phraseLemma>np be base on np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are in &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>GS represents the correct ranking and scores Scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are in a scale 1 with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a higher score indicating a better translation</example>
		<phraseLemma>np be in np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; learning &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>They often outperform conventional techniques &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in difﬁcult machine learning&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tasks</example>
		<phraseLemma>np in np learning np</phraseLemma>
	</can>
	<can>
		<phrase>The results reported in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results reported in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 are directly comparable with the ofﬁcial systems submitted for each of the respective tasks</example>
		<phraseLemma>the result report in np</phraseLemma>
	</can>
	<can>
		<phrase>This is &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is a superset of the WMT 1 dataset with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; additional sentences for test and a different quality label</example>
		<phraseLemma>this be np with np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; for &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As training data for science we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the scientiﬁc abstracts data provided by Carpuat</example>
		<phraseLemma>as np for np we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; belonging to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Each feature represents the number of words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a sentence belonging to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a speciﬁc semantic class</example>
		<phraseLemma>np in np belong to np</phraseLemma>
	</can>
	<can>
		<phrase>In order to explore &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to explore&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; language differences among deceivers and truetellers we use the linguistic ethnography method and obtain the most dominant semantic word classes in the LIWC lexicon associated to truth and lies provided by males and females</example>
		<phraseLemma>in order to explore np</phraseLemma>
	</can>
	<can>
		<phrase>Were similar to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Endorsement rates were higher than 1 across the board likely because even words with NONCONFUNATT consonant patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were similar to the exposure words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all other respects eg length syllable structure number of vowels</example>
		<phraseLemma>np be similar to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which amounts to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>If K is the number of templates that have been posited so far and nK indicate the number of sounds that have been drawn from each template then the probability distribution over the template zn that the sound sn will be drawn from is given by Since the probability that an existing template generated sn is proportional to the number of segments currently assigned to that template this prior encourages partitions in which a few templates explain most of the sounds &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which amounts to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a parsimony bias</example>
		<phraseLemma>np which amount to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are randomly sampled from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>For algebracom problems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are randomly sampled from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the number word problems category for answersyahoocom we 1 randomly sample an initial set of problems from the math category and then ask human annotators to manually choose number word problems from them</example>
		<phraseLemma>np be randomly sample from np</phraseLemma>
	</can>
	<can>
		<phrase>This leads to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This leads to a 1 overall reduction in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; crossings and is exempliﬁed in Figure 1 g and Figure 1 h</example>
		<phraseLemma>this lead to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; not seen in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>All remaining errors that the bigram alignment models commits are for the best considered parametrization and training set size typically due to matchup types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;not seen in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data and thus mostly concern foreign names or writings</example>
		<phraseLemma>np not see in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also lead to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>While in the table better alignments do not necessarily imply better overall G 1 P performance the two best alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two best overall G 1 P performances conversely the worst alignment quality is coupled with the worst overall G 1 P performance</example>
		<phraseLemma>np also lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; increases &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The table shows that the coefﬁcients are on the order of about 1 to 1 meaning that all else being equal increasing alignment quality by 1 edit distance to the goldstandard alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;increases overall G 1 P by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; about 1 to 1</example>
		<phraseLemma>np increase np by np</phraseLemma>
	</can>
	<can>
		<phrase>When using &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When using partially annotated sentences for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WS training data word boundary information exists only between some character pairs and is absent for others</example>
		<phraseLemma>when use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; collected &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We launched our IM as a browser addon for Twitter and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;collected 1 IM logs from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 users between April and December 1</example>
		<phraseLemma>np collect np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>As a result we found that it is a good tradeoff between speed and model performance to drop the input layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dropout rate pinput = 1</example>
		<phraseLemma>np only with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is separated by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>First every Chinese character in the bitexts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is separated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; white spaces so that individual characters are recognized as unique /words or alignment targets</example>
		<phraseLemma>np be separate by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is calculated on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The coverage of an alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is calculated on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬂy during search using a local phrase extraction algorithm</example>
		<phraseLemma>np be calculate on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; that contain &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Yu report that only 1 sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the ChineseEnglish training data that contain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 words are fully reachable due to noisy alignments and distortion limit</example>
		<phraseLemma>np in np that contain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; is that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>One problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the above equation is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some nodes in the translation graph do not have evidence outgoing edges such as translation nodes containing only function words or the null node</example>
		<phraseLemma>np with np be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; build &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In order to capture sourceside context for lexical selection some researchers propose triggerbased lexicon models to capture longdistance dependencies and many more researchers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;build classiﬁers with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rich context information to select desirable translations during decoding</example>
		<phraseLemma>np build np with np</phraseLemma>
	</can>
	<can>
		<phrase>We conduct experiments with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We conduct experiments with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a stateoftheart SMT system on largescale data to evaluate the effectiveness of BCorrRAE model</example>
		<phraseLemma>we conduct experiment with np</phraseLemma>
	</can>
	<can>
		<phrase>We compute &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compute a ratio of aligned nodes over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all nodes to estimate how well tree structures of bilingual phrases generated by BRAE and BCorrRAE are consistent with word alignments</example>
		<phraseLemma>we compute np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; would result in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Prior work in machine translation has discussed the existence of sentences in Chinese which &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;would result in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a poor translation if translated in one sentence in English</example>
		<phraseLemma>np would result in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; would be &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For potential BLEU the outputs of partial derivations would be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corresponding complete translations generated by Equation</example>
		<phraseLemma>for np would be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; adopt &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We use the NIST MT 1 evaluation data sentences as the tuning set and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;adopt NIST MT 1 MT 1 MT 1 and MT 1 data as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the test set</example>
		<phraseLemma>np adopt np as np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we tested &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For our ﬁnal experiments we tested&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank parsed Brown corpus subset as an outofdomain evaluation</example>
		<phraseLemma>for np we test np</phraseLemma>
	</can>
	<can>
		<phrase>We present an approach to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present an approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Arabic automatic diacritization that integrates syntactic analysis with morphological tagging through improving the prediction of case and state features</example>
		<phraseLemma>we present a approach to np</phraseLemma>
	</can>
	<can>
		<phrase>They use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They use independent taggers for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all features and language models for lemmas and diacritized surface forms</example>
		<phraseLemma>they use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the performance on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>While the baseline on Test is slightly higher than DevTest &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all metrics are comparable</example>
		<phraseLemma>np the performance on np</phraseLemma>
	</can>
	<can>
		<phrase>The procedure of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The procedure of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our algorithm to derivate incomplete spans can be regarded as two steps</example>
		<phraseLemma>the procedure of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; such as English</phrase>
		<frequency>8</frequency>
		<example>They are important for prodrop languages such as Japanese in particular for the machine translation from prodrop languages to nonprodrop languages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;such as English&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np such as english</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; be &lt;NP&gt; produced by &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Let T = t &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;be the sequence of nodes produced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the postorder traversal from root node and ei be the empty category tag associated with ti</example>
		<phraseLemma>np be np produce by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which they occur</phrase>
		<frequency>8</frequency>
		<example>Although this annotation scheme contains fewer error types than the taxonomies used for learner cor pora its granularity increases when the error sufﬁxes are interpreted in the syntactic context &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which they occur&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in which they occur</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may have &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Referring to the distribution of error sufﬁxes in Table 1 this suggests that the inserted and deleted tokens &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may have a larger effect on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parser error than the substituted tokens as their number is higher for English</example>
		<phraseLemma>np may have np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; smaller than &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Considering that Foreebank is orders of magnitude &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;smaller than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WSJ/FTB these gains are encouraging</example>
		<phraseLemma>np smaller than np</phraseLemma>
	</can>
	<can>
		<phrase>We have introduced &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We have introduced a treebank of technical forum sentences for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English and French based on an annotation strategy adapted to suit usergenerated text in a realistic NLP setting</example>
		<phraseLemma>we have introduce np for np</phraseLemma>
	</can>
	<can>
		<phrase>We are concerned with &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We are concerned with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semisupervised dependency parsing namely how to leverage large amounts of unannotated data in addition to annotated Treebank data to improve dependency parsing accuracy</example>
		<phraseLemma>we be concern with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; proposed in this work</phrase>
		<frequency>8</frequency>
		<example>Finally we conduct a multilingual evaluation that demonstrates the robustness of the overall structured neural approach as well as the beneﬁts of the extensions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;proposed in this work&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np propose in this work</phraseLemma>
	</can>
	<can>
		<phrase>Inspired by the work of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Inspired by the work of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Bohnet and Nivre we embed the set of top tags according to a ﬁrststage tagger</example>
		<phraseLemma>inspire by the work of np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to tune &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use grid search to tune&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these parameters for two separate scenarios</example>
		<phraseLemma>we use np to tune np</phraseLemma>
	</can>
	<can>
		<phrase>We tune &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>When parsing WSJ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we tune parameters on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WSJ section</example>
		<phraseLemma>np we tune np on np</phraseLemma>
	</can>
	<can>
		<phrase>We would also like to thank &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We would also like to thank&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Mohit Bansal Yoav Goldberg Siddharth Patwardhan and Kapil Thadani for helpful discussions and our anonymous reviewers for their insightful comments and suggestions</example>
		<phraseLemma>we would also like to thank np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provide &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>8</frequency>
		<example>On the other hand content words such as antartica rainfall continental and desert are attributed higher weights as these words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provide hints that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the predicted word is likely to be related to these words</example>
		<phraseLemma>np provide np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; propose &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We take inspiration from Frazier and Rayner and other psycholinguists and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;propose repair actions as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a lightweight alternative to beamsearch</example>
		<phraseLemma>np propose np as np</phraseLemma>
	</can>
	<can>
		<phrase>We compute &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compute source and targetside similarities based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; representations of nonterminals and phrasal substitutions for each applied rule and sum up these similarities to calculate the total score of a derivation on the two features</example>
		<phraseLemma>we compute np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; generated by &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In the twopass decoding we collect target phrase candidates from best translations for each source sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;generated by the baseline in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst pass and learn vector representations for these target phrase candidates</example>
		<phraseLemma>np generate by np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are estimated on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The JTR models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are estimated on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word alignments which we obtain using GIZA in this paper</example>
		<phraseLemma>np be estimate on np</phraseLemma>
	</can>
	<can>
		<phrase>To investigate the effect of &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To investigate the effect of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; including jump information in the JTR sequence we trained a BRNN using jump classes and another excluding them</example>
		<phraseLemma>to investigate the effect of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to perform &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>The JTR models are not dependent on the phrasebased framework and one of the longterm goals &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to perform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standalone decoding with the JTR models independently of phrasebased systems</example>
		<phraseLemma>np be to perform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as below</phrase>
		<frequency>8</frequency>
		<example>We consider two variants of the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as below&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as below</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which encodes &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Compared with CNN ConvGRNN shows its superior power in document composition component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which encodes semantics of sentences and their relations in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document representation with gated recurrent neural network</example>
		<phraseLemma>np which encode np in np</phraseLemma>
	</can>
	<can>
		<phrase>We can see that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can see that standard recurrent neural network is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the worst method even worse than the simple vector average</example>
		<phraseLemma>we can see that np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; to compute &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Le and Zuidema extended recursive neural networks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with LSTM to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children</example>
		<phraseLemma>np with np to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been evaluated on &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>A number of other models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been evaluated on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CoNLL 1 data</example>
		<phraseLemma>np have be evaluate on np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to label &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use symbol C to label&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word sequences which are complete</example>
		<phraseLemma>we use np to label np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we report &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we report&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our performance scores and analysis numbers averaged on our SMT models</example>
		<phraseLemma>in this section we report np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; per &lt;NP&gt; on average</phrase>
		<frequency>8</frequency>
		<example>As a result the average number of phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;per sentence increases from to while the decoding time of the SMT decoder increases from 1 seconds to 1 seconds per sentence on average&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np per np on average</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; for example &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Figure 1 problem for example&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the unstated fact that lines BD and AC intersect at E is necessary to solve the problem</example>
		<phraseLemma>in np for example np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; can be &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance radius can be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a type identiﬁer in the length of radius AO is 1 ” or a predicate in AO is the radius of circle O”</example>
		<phraseLemma>for np can be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are due to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Roughly 1 of the errors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are due to failures in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text parsing and about 1 of errors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are due to failures in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; diagram parsing</example>
		<phraseLemma>np be due to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains &lt;NP&gt; per &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Taking these variations into account the resulting video corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains 1 videos per&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence and 1 videos per sentence interpretation corresponding to a total of videos</example>
		<phraseLemma>np contain np per np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which represents &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>To perform the disambiguation task we extend the sentence recognition model of Siddharth &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which represents sentences as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; compositions of words</example>
		<phraseLemma>np which represent np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in several ways</phrase>
		<frequency>8</frequency>
		<example>Our model extends the approach of Siddharth &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in several ways&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in several way</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which assigns &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In order to do this we create a new HMM for this predicate &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which assigns low probability to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tracks that heavily overlap forcing the model to ﬁt two different actors in the previous example</example>
		<phraseLemma>np which assign np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; replace &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We obtain vector representations of relations through factorization of the knowledge base tensor as did Gardner and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;replace each edge type in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a PRAstyle path with edges that are similar to it in the vector space</example>
		<phraseLemma>np replace np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both at &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>We thus need a new way of ﬁnding negative examples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training time and at test time</example>
		<phraseLemma>np both at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are added to &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>After pruning there are 1 million unique textual relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are added to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the knowledge graph</example>
		<phraseLemma>np that be add to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not occur in &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>Without textual information model F is performing essentially randomly because entity pairs in the test sets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not occur in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training set relations</example>
		<phraseLemma>np do not occur in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; taking into consideration &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>In the RecNN case the compositional process continues recursively by following a parse tree until a vector for the whole sentence or phrase is produced on the other hand an RNN assumes that a sentence is generated in a lefttoright fashion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;taking into consideration&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no dependencies other than word adjacency</example>
		<phraseLemma>np take into consideration np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we will use &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Section 1 we will use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pretrained vectors and compositional weights for deriving sentence representations that will be subsequently fed to the siamese network</example>
		<phraseLemma>in np we will use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; prior for &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>To achieve this CLDA assumes document d in collection c has a multinomial documenttopic distribution with an asymmetric Dirichlet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;prior for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; K topics where the ﬁrst K are common across collections</example>
		<phraseLemma>np prior for np</phraseLemma>
	</can>
	<can>
		<phrase>We implement &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>8</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We implement the standard SVM method and the neural bagofwords model NBoW as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; baseline methods in both tasks</example>
		<phraseLemma>we implement np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; trained with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For instance Mnih learn control strategies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using convolutional neural networks trained with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a variant of Qlearning</example>
		<phraseLemma>np use np train with np</phraseLemma>
	</can>
	<can>
		<phrase>In order to handle &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to handle&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sequential nature of text we use Long ShortTerm Memory networks to automatically learn useful representations for arbitrary text descriptions</example>
		<phraseLemma>in order to handle np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used to learn &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>This indicates that these simulated worlds &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; good representations for language that transfer across worlds</example>
		<phraseLemma>np can be use to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which consists of &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We apply the same process to the Cities dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which consists of cities from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the intersection of the distributional and FreeBase city lists</example>
		<phraseLemma>np which consist of np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We write p&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the predicted value of attribute &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; instance i and g&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the gold standard value</example>
		<phraseLemma>np a for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are statistically signiﬁcant at &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>A signiﬁcance test with bootstrap resampling showed that all pairwise comparisons &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are statistically signiﬁcant at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; p 1</example>
		<phraseLemma>np be statistically signiﬁcant at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; related to &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>As mentioned above we also excel at many attributes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;related to a countrys economic and social development such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; GNI GDP CO 1 emissions internet usage or fertility rate</example>
		<phraseLemma>np related to np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not have &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Fortunately almost all contextually unsupported groups are small with only one or two attributes and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not have a large impact on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the overall performance</example>
		<phraseLemma>np do not have np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; of this paper</phrase>
		<frequency>7</frequency>
		<example>Special thanks to Christian Scheible for help with the Machine Learning part to the anonymous reviewers for insightful and constructive feedback and to the FLOSS reading group for helping us shape our ideas &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the topic of this paper&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np of this paper</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was introduced for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>An additional label KIND &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was introduced for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; usages of the concept as a kind where quantiﬁcation does not apply</example>
		<phraseLemma>np be introduce for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; is represented by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>First we build a cooccurrence based space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which a word is represented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; cooccurrence counts with content words</example>
		<phraseLemma>np in which np be represent by np</phraseLemma>
	</can>
	<can>
		<phrase>We give &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Using the distance matrix &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we give a score to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each instance in our test data as follows</example>
		<phraseLemma>np we give np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to investigate &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>A natural next step &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to investigate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the quantiﬁcation of statements involving contextualised subsets</example>
		<phraseLemma>np be to investigate np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; our model &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In both HPB and our model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the length range of a reordering performed on an input sentence is related to the use of glue grammars which bring two beneﬁts during decoding</example>
		<phraseLemma>in np we model np</phraseLemma>
	</can>
	<can>
		<phrase>We present &lt;NP&gt; for &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present a novel approach for unsupervised induction of a Reordering Grammar using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a modiﬁed form of permutation trees which we apply to preordering in phrasebased machine translation</example>
		<phraseLemma>we present np for np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which we use as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Every permutation can be factorized into a forest of PETs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which we use as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a latent treebank for training a Probabilistic ContextFree Grammar tailor made for preordering as we explain next</example>
		<phraseLemma>np which we use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; evaluated &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We use a 1 gram language model trained with KenLM 1 tune 1 times with kbmira to account for tuner instability and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;evaluated using Multeval 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; statistical signiﬁcance on 1 metrics</example>
		<phraseLemma>np evaluate np for np</phraseLemma>
	</can>
	<can>
		<phrase>To be consistent with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To be consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Japanese word order we move the modiﬁed clause to the start of the sentence</example>
		<phraseLemma>to be consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; include &lt;NP&gt; so on</phrase>
		<frequency>7</frequency>
		<example>In standard Japanese such conjunctions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;include no de kara de mo and so on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np include np so on</phraseLemma>
	</can>
	<can>
		<phrase>In addition as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a data preprocessing step our approach is orthogonal to the others with which it can be easily combined</example>
		<phraseLemma>in addition as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the role of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Some studied online discussion about Palestine and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the role of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Twitter in the Arab Spring</example>
		<phraseLemma>np the role of np</phraseLemma>
	</can>
	<can>
		<phrase>We trained &lt;NP&gt; on &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We trained a logistic regression classiﬁer on the relevant tweets in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Section 1 dataset and mapped all other labels to irrelevant</example>
		<phraseLemma>we train np on np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does well on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Our trained classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does well on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task achieving 1 accuracy a 1 absolute increase over baseline</example>
		<phraseLemma>np do well on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; extracted from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>From all propositions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a verbatim predicate extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the entire dataset we rank the most frequent subjects and manually ﬁlter out noncontent terms to yield a set of target topics the most frequent of which are obama democrats bush hillary and america</example>
		<phraseLemma>np with np extract from np</phraseLemma>
	</can>
	<can>
		<phrase>We run &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Since both models are ﬁt using approximate inference with a nonconvex objective function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we run ﬁve models with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different random initializations and present the average across all ﬁve</example>
		<phraseLemma>np we run np with np</phraseLemma>
	</can>
	<can>
		<phrase>Since &lt;NP&gt; relies on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since our automated trait inference system relies on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; linguistic cues derived from a persons Twitter posts to ensure we can have a stable and reliable reading of ones personal traits from his tweets only active Twitter users with over tweets can participate this survey</example>
		<phraseLemma>since np rely on np</phraseLemma>
	</can>
	<can>
		<phrase>We ran &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Since our current goal is not to build the best brand preference prediction system but to show the feasibility of building brand preference prediction systems that are scalable to millions of users &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we ran all our classiﬁers using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the default parameter settings from Weka Eg for Random Forest we used trees</example>
		<phraseLemma>np we run np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is linked with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Then for each candidate entity e we include all entities whose Wikipedia article &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is linked with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the article of e by an outgoing or incoming link</example>
		<phraseLemma>np be link with np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we employ &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we employ&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a simple yet effective metric that is agnostic to the scaling and time lag of time series</example>
		<phraseLemma>in this work we employ np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is much better than &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We show that the oracle among these candidates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is much better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the summaries that we have combined</example>
		<phraseLemma>np be much better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are often used as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The other systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are often used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; competitive baselines we implement these ourselves</example>
		<phraseLemma>np be often use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is relatively small</phrase>
		<frequency>7</frequency>
		<example>Though two summaries with the same set of sentences can have different ROUGE scores due to the truncation of the last sentence because the majority of content covered is still the same the difference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in ROUGE score is relatively small&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np be relatively small</phraseLemma>
	</can>
	<can>
		<phrase>The summary with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The summary with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the smallest JS divergence between the summary and the input or the hypersummaries is selected</example>
		<phraseLemma>the summary with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on R 1</phrase>
		<frequency>7</frequency>
		<example>As shown in Figure 1 and Table 1 our model performs consistently better than all basic systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on R 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on r 1</phraseLemma>
	</can>
	<can>
		<phrase>However to the best of our knowledge &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However to the best of our knowledge&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no previous work of this task tries to focus on summarization beyond pure sentence extraction</example>
		<phraseLemma>however to the best of we knowledge np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the score of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Here in the ﬁrst term g &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the score of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrase p which can be simply set to document frequency</example>
		<phraseLemma>np be the score of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; provided by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To further improve grammaticality of generated summaries we may try to sacriﬁce the time efﬁciency for a little bit and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use syntactic information provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntactic parsers</example>
		<phraseLemma>np use np provide by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shown to be &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Current recommended best variants of ROUGE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shown to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signiﬁcantly outperformed by several other ROUGE variants</example>
		<phraseLemma>np be show to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; alone</phrase>
		<frequency>7</frequency>
		<example>BLEU MT metric achieves highest correlation across all human evaluation combinations and highest again when evaluated against human coverage scores alone and BLEUs brevity penalty that like recall penalizes a system for too short output is a probable cause of the metric overcom ing the recallbased bias of an evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on coverage scores alone&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np base on np alone</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by treating &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Wang generate a coherent event summary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by treating summarization as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an optimization problem for topic cohesion</example>
		<phraseLemma>np by treat np as np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 gives an example of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 gives an example of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the formality of the article which is a low 1 formality words per words where the tweet is not extracted from the article but rephrased from the article instead</example>
		<phraseLemma>table 1 give a example of np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we apply &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we apply&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such CNNderived visual features to the task of bilingual lexicon induction</example>
		<phraseLemma>in this paper we apply np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned as &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The image dispersion d of a concept word w &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned as the average pairwise cosine distance between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the image representations in the set of images for a given word</example>
		<phraseLemma>np be deﬁned as np between np</phraseLemma>
	</can>
	<can>
		<phrase>Given &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given the subtle and different ways in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiments can be expressed and the cultural diversity amongst different languages an MT system has to be of a superior quality to perform well</example>
		<phraseLemma>give np in which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; trained on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>These identifiers were used as features for creating sentiment classifiers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on Binary/Multiclass SVM trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bag of words representation using libSVM library</example>
		<phraseLemma>np base on np train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be modeled as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The extractive summarization task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be modeled as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; optimization problem ie ﬁnding a set S ⊆ V which maximizes a submodular function f subject to budget constraints</example>
		<phraseLemma>np can be model as np</phraseLemma>
	</can>
	<can>
		<phrase>In the experiment &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the experiment&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the partial enumeration based greedy algorithm is used for summary generation of test documents within budget of words</example>
		<phraseLemma>in the experiment np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; assumes that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In terms of sources previous work in sentiment analysis trained on review data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;assumes that the source is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the writer of the review</example>
		<phraseLemma>np assume that np be np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we ﬁrst introduce &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we ﬁrst introduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PSL in Section 1</example>
		<phraseLemma>in this section we ﬁrst introduce np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we kept &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For a tenor phrase we kept&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only the head noun which is usually sufﬁcient to understand the affective polarity target</example>
		<phraseLemma>for np we keep np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 presents examples of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 presents examples of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive and negative similes in the annotated data set</example>
		<phraseLemma>table 1 present example of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; assigning &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For each simile we sum the sentiment scores for all lexicon words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the simile components assigning&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive/negative polarity depending on whether the sum is positive/negative</example>
		<phraseLemma>np in np assign np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we identify &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each simile we identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all tweets that contain the simile and collect all of the words surrounding the simile in these tweets as a collective context” for the tweet</example>
		<phraseLemma>for np we identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; create &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For each simile vehicle we therefore extract all explicit properties mentioned with that vehicle &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our corpus and create&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a binary feature for each</example>
		<phraseLemma>np in np create np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may appear in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>However mentions for the same event &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may appear in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; forms of diverse words and phrases and they do not always cover all arguments or attributes</example>
		<phraseLemma>np may appear in np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate several recent datasets based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all proposed metrics in Section 1 with results reported in Tables 1 1 and Figure 1</example>
		<phraseLemma>we evaluate np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are labeled using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Objects in the scene &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are labeled using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; perinstance segmentations</example>
		<phraseLemma>np be label use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; not as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>On the other hand work that does consider the image content for generating prepositions map geometric relations to a limited set of prepositions using manually deﬁned rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;not as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; humans would naturally use them with a richer vocabulary</example>
		<phraseLemma>np not as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; to improve &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Other related work include training classiﬁers for prepositions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with spatial relation features to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; image segmentation and detection this work is however limited to 1 prepositions</example>
		<phraseLemma>np with np to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between two &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For this paper we constrain trajector and landmark to be entities that are visually identiﬁable in an image since we are interested in discovering the role of visual features and geometric conﬁgurations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between two&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entities in the preposition prediction task</example>
		<phraseLemma>np between two np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; covering &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We deﬁned an dimensional vector of bounding box features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;covering geometric relations such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distance orientation relative bounding box sizes and overlaps between bounding boxes</example>
		<phraseLemma>np cover np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are close in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>This encodes each term as a vector such that semantically related terms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are close in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vector space</example>
		<phraseLemma>np be close in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; is for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the PC removal the optimal number of removed PCs is for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TOEFL 1 for BLESS and for SimLex 1 while the optimal number for the Caron ptransform is 1 for TOEFL 1 for BLESS and 1 for SimLex 1</example>
		<phraseLemma>for np be for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; then &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We consider the two tagging tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with ﬁrst a coarse tagset tags and then&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a morphosyntactical rich tagset items observed on the the training set</example>
		<phraseLemma>np with np then np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are implemented with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>All the models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are implemented with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Theano library</example>
		<phraseLemma>np be implement with np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we investigate &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we investigate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a possible extension of RNNLM by allowing it to continue learning and adapting during testing</example>
		<phraseLemma>in this work we investigate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is extended with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The number of possible dialogue acts grows rapidly as the domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is extended with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; new attribute types and values making this task one of multilabel classiﬁcation with a very large number of labels</example>
		<phraseLemma>np be extend with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as where &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We deﬁne the hyperplane of the label a with its normal vector W &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; φ is the same mapping to d dimensional word vectors that is used above in the utterance representation Wih is a 1 d × h matrix and Who is a h × h matrix</example>
		<phraseLemma>np as where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>On a large scale language modeling task this architecture &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves a 1 x speedup at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; runtime and a signiﬁcant reduction in perplexity when compared to a standard multilayer network</example>
		<phraseLemma>np achieve np at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is in &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Our models primary contribution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is in its novel combination of a straightforward joint modeling approach maxmargin&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; learning and exploitation of lexical information in both topic seeding and regression yielding a simple but effective model for topicinformed discriminative link prediction</example>
		<phraseLemma>np be in np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are linked to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Following Polson and Scott by introducing an auxiliary variable λdd 1 we derive the conditional probability of a topic assignment where Nkv denotes the count of word v assigned to topic k Ndk is the number of tokens in document d that are assigned to topic k Marginal counts are denoted by · dn denotes that the count excludes token n in document d d 1 denotes the indexes of documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are linked to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document d πdn is estimated based on the maximal ld k path assumption</example>
		<phraseLemma>np which be link to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In Table 1 the Average Regression Values &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show this comparison as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a ratio</example>
		<phraseLemma>np show np as np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to minimize &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use stochastic gradient descent to minimize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the overall loss of Eq</example>
		<phraseLemma>we use np to minimize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to remove &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Our ﬁndings suggest that the best crowdsource label training strategy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to remove&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; low item agreement instances from the training set</example>
		<phraseLemma>np be to remove np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; similar to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We can view the relatedness task as the task of evaluating a set of rankings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;similar to ranking evaluation in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Information Retrieval</example>
		<phraseLemma>np similar to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; along with &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For each query word and k we presented the six words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;along with the query word to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the users</example>
		<phraseLemma>np along with np to np</phraseLemma>
	</can>
	<can>
		<phrase>We report &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report the mean accuracy and standard deviation using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁvefold crossvalidation at each threshold frequency in Figure 1</example>
		<phraseLemma>we report np use np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; on &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train a topic model on the NIPS dataset with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different methods and compare the average topic coherence score and the average of the top twenty topic coherence scores</example>
		<phraseLemma>we train np on np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; addressed the problem of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Neelakantan &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;addressed the problem of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; path sparsity by embedding paths using a recurrent neural network</example>
		<phraseLemma>np address the problem of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to induce &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Our goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to induce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dependency parser in a target language of interest without any direct supervision</example>
		<phraseLemma>np be to induce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; describing &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>This section describes our approach giving deﬁnitions of parallel data and of dense projected structures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;describing preliminary exploratory experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; transfer from German to English describing the iterative training algorithm used in our work and ﬁnally describing a generalization of the method to transfer from multiple languages</example>
		<phraseLemma>np describe np on np</phraseLemma>
	</can>
	<can>
		<phrase>We refer &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We refer interested readers to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt the previous hidden state ht and the memory cell ct</example>
		<phraseLemma>we refer np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; rather than &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Swedish English and French use sufﬁxes for the verb tenses and number while Hebrew &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses prepositional particles rather than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; grammatical case</example>
		<phraseLemma>np use np rather than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was implemented based on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was implemented based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the description by McDonald with two changes which were necessary due to the large size of the training data set used for model ﬁtting</example>
		<phraseLemma>np be implement base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is constructed with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>If the current input is the root node then a special parent embedding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is constructed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all nodes set to zero except for one node</example>
		<phraseLemma>np be construct with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was rated by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Every sentencecompression pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was rated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three raters who were asked to select a rating on a ﬁvepoint Likert scale ranging from one to ﬁve</example>
		<phraseLemma>np be rate by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by running &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The two important components of the system are word embeddings which can be obtained by anyone either pretrained or &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by running word 1 vec on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a large corpus and an LSTM model which draws on the very recent advances in research on RNNs</example>
		<phraseLemma>np by run np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using them as features in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We develop a combined model by discretizing probability from Ngram model and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using them as features in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the syntactic model</example>
		<phraseLemma>np use they as feature in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is initialized as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Given an input bag of words ρ is initialized to the input and σ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is initialized as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; empty</example>
		<phraseLemma>np be initialize as np</phraseLemma>
	</can>
	<can>
		<phrase>Compared with &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Compared with the syntactic model the Ngram model has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; less information for disambiguation but also has less structural ambiguities and therefore a smaller search space</example>
		<phraseLemma>compare with np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; increases the performance of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>On the other hand as the scale of automaticallyparsed AFP data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;increases the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the syntactic model rapidly increases surpassing the syntactic model trained on the highquality WSJ data</example>
		<phraseLemma>np increase the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>This can be explained by the fact that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This can be explained by the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; longer sentences have richer underlying syntactic structures which can better captured by the syntactic model</example>
		<phraseLemma>this can be explain by the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is given for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To make recallonly evaluation unbiased to length output of all systems is cutoff after characters and no bonus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is given for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; shorter summaries</example>
		<phraseLemma>np be give for np</phraseLemma>
	</can>
	<can>
		<phrase>We apply &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We apply a minimal preprocessing step using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PTB tokenization lowercasing replacing all digit characters with # and replacing of word types seen less than 1 times with UNK</example>
		<phraseLemma>we apply np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We run experiments both using the DUC 1 evaluation data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set sentences 1 references 1 bytes with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all systems and a randomly heldout Gigaword test set sentences 1 reference</example>
		<phraseLemma>np set np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; set &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We run experiments both using the DUC 1 evaluation data set sentences 1 references 1 bytes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with all systems and a randomly heldout Gigaword test set&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences 1 reference</example>
		<phraseLemma>np with np set np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is to ﬁnd &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In these approaches the goal is to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most central sentences in the document by constructing a graph in which nodes are sentences and edges are similarity between these sentences</example>
		<phraseLemma>in np be to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has a number of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Our proposed method generates a summary of an article with the premise that the article &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has a number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; citations to it</example>
		<phraseLemma>np have a number of np</phraseLemma>
	</can>
	<can>
		<phrase>We observe &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We observe relative consistency between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different rouge scores for each summarization approach</example>
		<phraseLemma>we observe np between np</phraseLemma>
	</can>
	<can>
		<phrase>This is expected since &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is expected since&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LexRank tries to ﬁnd the most central sentences</example>
		<phraseLemma>this be expect since np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we obtained &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>That is &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in all cases we improved over the baselines in some cases we obtained&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; greater than 1 improvement for mean ROUGE scores over the best performing baseline</example>
		<phraseLemma>np in np we obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to discover &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Given an unlabeled data set the task of hashtag recommendation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to discover&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a list of hashtags for each microblog</example>
		<phraseLemma>np be to discover np</phraseLemma>
	</can>
	<can>
		<phrase>Comparing the results of &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Comparing the results of the method CNHR with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the methods NHR 1 and NHR 1 which do not take the types of hashtags into consideration we can see that the proposed method beneﬁts a lot from incorporating the types of hashtags</example>
		<phraseLemma>compare the result of np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which treated &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To handle the vocabulary problem in keyphrase w extraction task Liu proposed a topical ord trigger model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which treated the keyphrase xtraction problem as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a translation process with tent topics</example>
		<phraseLemma>np which treat np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shared among &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To improve the TFIDF matrix M described in Section 1 we multiply it by the word coupling matrix C so that the term frequencies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shared among&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the highly correlated words</example>
		<phraseLemma>np be share among np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we implement &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For GRAW we implement&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; label propagation on both the merged graph Gc and the ﬁnal graph Gcf denoted as GRAWc and GRAWcf respectively</example>
		<phraseLemma>for np we implement np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as examples</phrase>
		<frequency>7</frequency>
		<example>Consider the following two tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as examples&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as example</phraseLemma>
	</can>
	<can>
		<phrase>In order to keep &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to keep&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pvalue inﬂation low we only compared the ten best performing models for each algorithm with respect to the Measure</example>
		<phraseLemma>in order to keep np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are trained using &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Most of the current automated essay scoring systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are trained using manually graded essays from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a speciﬁc prompt</example>
		<phraseLemma>np be train use np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be traced back to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Work related to essay scoring &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be traced back to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; when Ellis Page created a computer grading software called Project Essay Grade</example>
		<phraseLemma>np can be trace back to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponds to the number of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Matrix Oij &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponds to the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; essays that receive a score i by the ﬁrst rater and a score j by the 1 rater</example>
		<phraseLemma>np correspond to the number of np</phraseLemma>
	</can>
	<can>
		<phrase>Furthermore for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Furthermore for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different use cases different evidence types could be more suitable</example>
		<phraseLemma>furthermore for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are similar to those of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For types Expert and Anecdotal the performance of the contextdependent component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are similar to those of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the W 1 V baseline</example>
		<phraseLemma>np be similar to those of np</phraseLemma>
	</can>
	<can>
		<phrase>We also propose &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also propose novel evaluation steps based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; retrieved search results of the corrected queries in terms of quantity and relevance</example>
		<phraseLemma>we also propose np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are organized as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The logs on HDFS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are organized as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sessions where each session contains a stream of user events up to a speciﬁc time of inactivity</example>
		<phraseLemma>np be organize as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is extracted from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In the following we will use the notation for a search query pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the logs explained by our method in the previous section</example>
		<phraseLemma>np that be extract from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; reduces &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We observe that the ﬁltering scheme &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;reduces overall training size by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; almost 1 and increases accuracy on the development set by 1 absolute</example>
		<phraseLemma>np reduce np by np</phraseLemma>
	</can>
	<can>
		<phrase>For training &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For training the autocorrection system we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; basic methods and opensource tools for statistical machine translation</example>
		<phraseLemma>for training np we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; generate &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In addition to the parallel data pull from the query logs as described in Section 1 we also take the top queries from those logs where we are sure they are most likely correct and can be used as gold reference and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;generate artiﬁcially noisiﬁed variants based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an English keyboard layout and statistics derived from our development set</example>
		<phraseLemma>np generate np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We did not observe signiﬁcant differences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when tuning on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLEU versus WER</example>
		<phraseLemma>np when np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when compared against &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We autocorrect the set through our best system which results in changed queries in 1 of the cases after manual ﬁltering of nonerrors and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when compared against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original input set</example>
		<phraseLemma>np when compare against np</phraseLemma>
	</can>
	<can>
		<phrase>Due to &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Due to its adoption as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the main evaluation metric of the CoNLL shared tasks and the QALB shared tasks the metric can be seen as a de facto standard</example>
		<phraseLemma>due to np as np</phraseLemma>
	</can>
	<can>
		<phrase>When used in &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When used in tandem with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior human knowledge to handcraft good features this simple architecture has proven effective in solving practical textbased tasks such as academic document classiﬁcation</example>
		<phraseLemma>when use in np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Sections &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We describe each of the two phases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Sections&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 and 1</example>
		<phraseLemma>np in section np</phraseLemma>
	</can>
	<can>
		<phrase>We see that for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We see that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; small proportions both variants of the SBEN outperform the SVM and furthermore the SBEN trained via full BUTD can reach lower error especially for the most extreme scenario where only 1 labeled examples per class are available</example>
		<phraseLemma>we see that for np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we conducted &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the 1 NewsGroup dataset we conducted&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a simple experiment to uncover some of the knowledge acquired by our model with respect to the target categorization task</example>
		<phraseLemma>for np we conduct np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the ratio of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The price the advertiser pays known as costperclick is the bid price for the 1 ranked advertiser times &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the ratio of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the expected CTR between the 1 and ﬁrst ranked advertisers</example>
		<phraseLemma>np the ratio of np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; derived from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use userindependent features derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; COEC statistics for speciﬁc queryad pairs</example>
		<phraseLemma>we use np derive from np</phraseLemma>
	</can>
	<can>
		<phrase>We also assume that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also assume that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each tag matches at least one mention in the document but do not specify where in the document the mention is</example>
		<phraseLemma>we also assume that np</phraseLemma>
	</can>
	<can>
		<phrase>Adding &lt;NP&gt; results in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Adding type features results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; better performance than either CRF or CRFWIDE at 1 Fscore</example>
		<phraseLemma>add np result in np</phraseLemma>
	</can>
	<can>
		<phrase>We restrict &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To better model busy workers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we restrict the gazetteers to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only KB tags from mentions in the ﬁrst n sentences</example>
		<phraseLemma>np we restrict np to np</phraseLemma>
	</can>
	<can>
		<phrase>We show that &lt;NP&gt; leads to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show that this labeling approach leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; good performance even when offtheshelf classiﬁers are used on the distantlylabeled data</example>
		<phraseLemma>we show that np lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is different from that of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Note that the dependency path of the wrong assignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is different from that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the correct assignment which essentially offers the information for the model to learn to distinguish the subject and the object</example>
		<phraseLemma>np be different from that of np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we exploit &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we exploit&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a convolution neural network model to learn more robust and effective relation representations from shortest dependency paths for relation extraction</example>
		<phraseLemma>in this paper we exploit np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is interpreted as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Regular expressions can be used in the pattern ﬁles and each pattern ﬁle &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is interpreted as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a regular expression pattern which can be called by HeidelTimes rules</example>
		<phraseLemma>np be interpret as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which we use for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Thus we write a simple and generic languageindependent sentence splitter and tokenizer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which we use for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all languages</example>
		<phraseLemma>np which we use for np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to do &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the offtheshelf tool word 1 vec to do&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; skipgram training for language model and implement our own CRF model to modify the embeddings</example>
		<phraseLemma>we use np to do np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To resolve this problem we use a relaxed set inclusion where the set cover &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be a subset of another set cover to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a certain degree</example>
		<phraseLemma>np can be np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when dealing with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The model can better adjust to different types of documents and yield ﬂexible performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when dealing with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a broad range of document types</example>
		<phraseLemma>np when deal with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to improve &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To the best of our knowledge this is the ﬁrst research work that leverages prior knowledge embedded &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in documents to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CCR performance</example>
		<phraseLemma>np in np to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are computed in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>All the metrics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are computed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the test pool of all entitydocument pairs</example>
		<phraseLemma>np be compute in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to capture &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We study CCR as a classiﬁcation problem and propose a latent document type model through &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;introducing a latent layer in a discriminative model to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the correlations between documents and their intrinsic types</example>
		<phraseLemma>np in np to capture np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we examine &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we examine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a slightly different problem we perform novelty detection at the sentence level to highlight sentences that contain novel information</example>
		<phraseLemma>in this paper we examine np</phraseLemma>
	</can>
	<can>
		<phrase>This result indicates that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This result indicates that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the DNNbased transformation allows the system to capture the nontrivial interactions between previous sentences and the current one</example>
		<phraseLemma>this result indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; treated &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Finally the ﬁfthbest team ICRCHIT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;treated the answer selection task as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments</example>
		<phraseLemma>np treated np as np</phraseLemma>
	</can>
	<can>
		<phrase>There are a total of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are a total of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; approximately articles in this collection which we designate as MedlinePlus</example>
		<phraseLemma>there be a total of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are randomly selected from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Twenty progress notes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are randomly selected from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a corpus of deidentiﬁed EHR notes as the EHR document collection</example>
		<phraseLemma>np be randomly select from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are tagged</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the ﬁrst step of the analysis sentiment words in the textual data are tagged&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np be tag</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to outperform &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>None of the classiﬁers based on the feature selection method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to outperform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline</example>
		<phraseLemma>np be able to outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used to extract &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For scenario a named entity recognition system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used to extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; targets before the same targeted sentiment classiﬁcation algorithms are applied</example>
		<phraseLemma>np can be use to extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by integrating &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We make a novel combination of the discrete models and the neural models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by integrating both types of inputs into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a same CRF framework</example>
		<phraseLemma>np by integrate np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are shown in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The integrated model takes both continuous word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; grey nodes and discrete manual features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; black or white nodes as the input</example>
		<phraseLemma>np which be show in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which consists of &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We manually analyzed the ﬁrst 1 1 sentences in the Rakuten Travel data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which consists of 1 Japanese reviews for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hotels in Japan and found that 1 1 sentences are opinions of which opinions are conditional and thus the result for an existing method includes up to 1 errors</example>
		<phraseLemma>np which consist of np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in example</phrase>
		<frequency>7</frequency>
		<example>It can easily be predicted that a typical grammatical unit for CFOs is a conditional clause &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in example&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as in example</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are classiﬁed into &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>However words and phrases in an opinion unit &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are classiﬁed into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its corresponding element</example>
		<phraseLemma>np be classiﬁed into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was shown in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>A similar result &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was shown in Wang and Manning for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bigram features in sentiment analysis</example>
		<phraseLemma>np be show in np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that can be found</phrase>
		<frequency>7</frequency>
		<example>These methods can typically identify entities with a high accuracy but their low recall limits the portion of true mentions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that can be found&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np that can be find</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows some of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows some of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the statistics of the datasets created</example>
		<phraseLemma>table 1 show some of np</phraseLemma>
	</can>
	<can>
		<phrase>Due to the difference in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Due to the difference in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; goals entity search techniques cannot be used directly to solve entity archiving problems</example>
		<phraseLemma>due to the difference in np</phraseLemma>
	</can>
	<can>
		<phrase>Based on &lt;NP&gt; we propose &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Based on these two conjectures we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a stochastic ListNet method which samples a subset of the permutation classes in model training and based on this subset to train the ListNet model</example>
		<phraseLemma>base on np we propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are normalized by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>These scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are normalized by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; softmax and are used as the probability distribution when sampling objects</example>
		<phraseLemma>np be normalize by np</phraseLemma>
	</can>
	<can>
		<phrase>Comparing the results with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Comparing the results with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different k it can be seen that a larger k leads to a better perfor mance with stochastic ListNet</example>
		<phraseLemma>compare the result with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used to obtain &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>If more computation is affordable stochastic Top ListNet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; better performance</example>
		<phraseLemma>np can be use to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>In particular we focus on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular we focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a subset that test students understanding of various kinds of general rules and principles and their ability to apply these rules to reason about speciﬁc situations or scenarios</example>
		<phraseLemma>in particular we focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; Both of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>However this turns out to be too brittle in handling lexical mismatches especially in the presence of differences in parse structures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Both of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the above MLNs rely on exactly matching the relations in the KB and question representation making them too sensitive to syntactic differences</example>
		<phraseLemma>np both of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are tied to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>These constants &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are tied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their respective string representations with the understanding that two entities behave similarly if they have lexically similar strings</example>
		<phraseLemma>np be tie to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represents &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Upon completion every Gic &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represents a collective linking solution to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the given mention set</example>
		<phraseLemma>np represent np to np</phraseLemma>
	</can>
	<can>
		<phrase>One can see that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;One can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst two graphs are very likely to be good solutions since they inherit many of the relation edges from GK while the 1 one is probably a poor collection as the candidates barely connect to one another</example>
		<phraseLemma>one can see that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; indicating &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>It is known that there are also substantial multiplestep relation paths &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between entities indicating&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their semantic relationships</example>
		<phraseLemma>np between np indicate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is compatible with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The reason is that the addition operation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is compatible with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the learning objectives of both TransE and PTransE</example>
		<phraseLemma>np be compatible with np</phraseLemma>
	</can>
	<can>
		<phrase>We aim to learn &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We aim to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a scoring function Sc 1 t that can distinguish these cases</example>
		<phraseLemma>we aim to learn np</phraseLemma>
	</can>
	<can>
		<phrase>In particular if &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the KB contains only a small number of entities of a particular type but the corpus contains a large number of contexts of these entities then CM is more likely to generalize well</example>
		<phraseLemma>in particular if np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in or</phrase>
		<frequency>7</frequency>
		<example>While great effort has been put into aligning knowledge at the concept level most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level nor do they exploit effectively the huge amount of information harvested with OIE systems even when this information is unambiguously linked to a structured resource &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in or&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as in or</phraseLemma>
	</can>
	<can>
		<phrase>We represent &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We represent each r ∈ R in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our vector space V and assign higher speciﬁcity to relations whose arguments are closer in V We ﬁnally disambiguate the remaining nonseed triples in T starting from the most speciﬁc relations and jointly using all participating argument pairs as context</example>
		<phraseLemma>we represent np in np</phraseLemma>
	</can>
	<can>
		<phrase>In order to consider &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this observation in our disambiguation pipeline we ﬁrst need to estimate the degree of speciﬁcity for each relation in the relation set Ri of the target KB to be disambiguated</example>
		<phraseLemma>in order to consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We analyzed results for both the whole set of triples in PATTY and the subset of ambiguous triples ie those triples whose subjects and objects have at least two candidate senses &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the BabelNet inventory</example>
		<phraseLemma>np each in np</phraseLemma>
	</can>
	<can>
		<phrase>We ran &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ran the algorithm over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both linked and unlinked KBs of our experimental setup and computed the coverage for each KB as the overall ratio of disambiguated triples</example>
		<phraseLemma>we run np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showing that &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>KBUNIFY achieves the best result &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showing that a baseline based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stateoftheart disambiguation is negatively affected by the lack of context for each individual triple</example>
		<phraseLemma>np show that np base on np</phraseLemma>
	</can>
	<can>
		<phrase>We showed &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We showed each alignment hri rji to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two human judges and asked whether ri and rj represented the same relation</example>
		<phraseLemma>we show np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are combined in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>These features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are combined in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a maximum entropy classiﬁer trained to predict name errors directly</example>
		<phraseLemma>np be combine in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; typically used in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The sentencelevel output yt differs from the worddependent label predictor &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;typically used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; named entity detection in that it is providing a sequentially updated prediction of a sentencelevel variable rather than a wordlevel indicator that speciﬁes the location of a named entity in the sentence</example>
		<phraseLemma>np typically use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; rather than at &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>At each time step the hidden vector is used to predict both the next word and a sentencelevel indicator of the presence of a name providing a probability distribution for both Another way to train the model is to predict the sentencelevel name label only at the end of the sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;rather than at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; every time step</example>
		<phraseLemma>np rather than at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained on &lt;NP&gt; trained on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Four models are compared including three RNNs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained on the BOLT data and the MT RNN model trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BOLT Reddit data</example>
		<phraseLemma>np train on np train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the probability that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>In this paper we train the neural network model with a multitask objective reﬂecting both the probability of the sequence and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the probability that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sequence contains names</example>
		<phraseLemma>np the probability that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is higher for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>As shown in Table 1 while supervised identiﬁcation of NE labels achieves a higher precision for all NE types the recall &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is higher for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all NE types using POSbased heuristics</example>
		<phraseLemma>np be higher for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only if &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In other words the classiﬁer will output a 1 if and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only if the elements in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set form a relation and their roles in the relation are correct</example>
		<phraseLemma>np only if np in np</phraseLemma>
	</can>
	<can>
		<phrase>We set &lt;NP&gt; to be &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We set sieve 1 to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the LINK classiﬁer and sieve 1 to be the classiﬁer and then order the remaining sieves by precision</example>
		<phraseLemma>we set np to be np</phraseLemma>
	</can>
	<can>
		<phrase>We employ &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Motivated by previous work on tree kernels for relation extraction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we employ parse trees as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a structured feature to encode the syntactic relationships among the roles extracted so far for a given MOVELINK</example>
		<phraseLemma>np we employ np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; centers on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Since this subtree &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;centers on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a spatial relation we call it a spatial relation centered tree</example>
		<phraseLemma>np center on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in over &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Second we propose a new technique for character detection based on inducing character prototypes and in comparisons with three stateoftheart methods demonstrate superior performance achieving signiﬁcant improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the nextbest method</example>
		<phraseLemma>np in over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is measured in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For precision matching &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is measured in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the purity of an extracted set of names Ei with respect to the goldstandard names Gj</example>
		<phraseLemma>np be measure in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is found to be &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Moreover it possesses an additional additive compositionality property obtained from the Skipgram training setting e g the closest word to Germany capital in the vector space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is found to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Berlin</example>
		<phraseLemma>np be find to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponds to &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The 1 rd row &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponds to DTK and the 1 th one to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; VTK – the difference with our model on the 1 th row lies in the function φ that enumerates all random walks in the dependency tree representation following Gärtner whereas we only consider the downward paths</example>
		<phraseLemma>np correspond to np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; focus on &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>A smaller number of efforts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;focus on semantic representations of cooking recipes with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an eye toward more complex and deep understanding</example>
		<phraseLemma>np focus on np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is then &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The subtree headed by inst indicates that ingredients # 1 and are inputs to instruction whose output &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is then an input to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; instruction together with ingredients and</example>
		<phraseLemma>np be then np to np</phraseLemma>
	</can>
	<can>
		<phrase>By applying &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By applying our TSDPMM in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a lifelong continuous learning framework namely TSDPMML can further improve text clustering due to the better prior topic knowledge obtained in the evolving environment</example>
		<phraseLemma>by apply np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; namely &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>By applying our TSDPMM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a lifelong continuous learning framework namely&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TSDPMML can further improve text clustering due to the better prior topic knowledge obtained in the evolving environment</example>
		<phraseLemma>np in np namely np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we propose &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we propose a gated recursive neural network to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; model sentences which employs a full binary tree structure to control the combinations in recursive structure</example>
		<phraseLemma>in this paper we propose np to np</phraseLemma>
	</can>
	<can>
		<phrase>The difference between &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The difference between TrAdaBoost and TABoost is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a weighting manner ie TABoost is a continuous timeline model and it weights these instances by applying Gaussian function in order to weaken their impacts</example>
		<phraseLemma>the difference between np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; sets based on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In particular we extended our training data by creating new training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;sets based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 1 1 and 1 Wikipedia pages</example>
		<phraseLemma>np set base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; show that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>We model this task as a sequence classification problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using CRF and show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the section title alone conveys enough information to achieve a good classification result both in a supervised setting and with a rulebased baseline</example>
		<phraseLemma>np use np show that np</phraseLemma>
	</can>
	<can>
		<phrase>However the number of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the false alignments is still 1 K</example>
		<phraseLemma>however the number of np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 gives the results of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 gives the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our algorithm with different feature ablations</example>
		<phraseLemma>table 1 give the result of np</phraseLemma>
	</can>
	<can>
		<phrase>We map &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In UrduPhone &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we map all such cases to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single form</example>
		<phraseLemma>np we map np to np</phraseLemma>
	</can>
	<can>
		<phrase>We conduct experiments using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We conduct experiments using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; realworld textual data sets and these experiments illustrate the predictionsasfeatures models trained by our algorithm outperform the original models</example>
		<phraseLemma>we conduct experiment use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as it requires &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The multilabel 1 / 1 loss is the exact match measure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as it requires&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any predicted set of labels h to match the true set of labels S exactly</example>
		<phraseLemma>np as it require np</phraseLemma>
	</can>
	<can>
		<phrase>The reason may be that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The reason may be that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the label dependencies in the Enron dataset is weak</example>
		<phraseLemma>the reason may be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are compared using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Document groups &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are compared using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; comparison criteria D a family of subsets of D Intuitively each subset of D represents a set of documents sharing some attribute</example>
		<phraseLemma>np be compare use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by our model</phrase>
		<frequency>7</frequency>
		<example>By contrast the Australia program was designed to support USAustralia cooperative research across many ﬁelds as correctly inferred &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by our model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np by we model</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which operate on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Our extractor makes use of both syntactic patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which operate on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dependency parse and regular expression patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which operate on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the text</example>
		<phraseLemma>np which operate on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was higher than that of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The average depth of correct FG and SFG types in FINET &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was higher than that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Pearl and Hyena</example>
		<phraseLemma>np be higher than that of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared to 1 for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>On SpanishEnglish Europarl data they reach an 1 compression rate on wordinterleaved text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared to 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; concatenated texts a 1 improvement</example>
		<phraseLemma>np compare to 1 for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; In &lt;NP&gt; we propose &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Network &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the previous section we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a RNNSLM which models the coherence between sentences but ignores the word sequence within a sentence</example>
		<phraseLemma>np in np we propose np</phraseLemma>
	</can>
	<can>
		<phrase>Compared with &lt;NP&gt; achieves &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Compared with the baseline systems the proposed HRNNLM achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signiﬁcant improvement with nearly 1 improvement in term of accuracy</example>
		<phraseLemma>compare with np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; trained with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We also train a decoder which is an inhouse Bracketing Transduction Grammar in a CKYstyle decoder &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a lexical reordering model trained with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; maximum entropy</example>
		<phraseLemma>np with np train with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as the result</phrase>
		<frequency>7</frequency>
		<example>With the translation of ﬁrst sentence we score all the translation candidates of the 1 sentence and select the best one &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as the result&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as the result</phraseLemma>
	</can>
	<can>
		<phrase>We apply &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For signiﬁcance testing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we apply the Wilcoxon&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signedrank test on the macro averaged scores and assume a signiﬁcance level of α= 1</example>
		<phraseLemma>np we apply np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the fact that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>Evident from these alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; aligned units are typically semantically similar or related</example>
		<phraseLemma>np be the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depends only on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Noticeable in the designs of the supervised aligners is a lack of attention to the scenarios competing units can pose – alignment of a unit &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depends only on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its own feature values</example>
		<phraseLemma>np depend only on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; take &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Following prior work we only consider the sure alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;take the majority opinion&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on each word pair and leave out threeway disagreements</example>
		<phraseLemma>np take np on np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we ﬁxed &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For PropBank we ﬁxed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the role embedding dimension to which is the number of semantic roles in PropBank datasets</example>
		<phraseLemma>for np we ﬁxed np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was ﬁxed to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In the Adagrad algorithm the minibatch size &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was ﬁxed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; for local estimation</example>
		<phraseLemma>np be ﬁxed to np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; outperforms &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that the linear baseline of Täckström outperforms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; most prior work including ones that employs rerankers except on the Brown test set</example>
		<phraseLemma>note that np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is represented by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In other cases the conﬁdence is set to a similarity score of a feature &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is represented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a predicate</example>
		<phraseLemma>np which be represent by np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we extract &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the relational phrases model we extract relational phrases from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a text and we map them to their synsets from PATTY</example>
		<phraseLemma>in np we extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicating that there is &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>A connection is a sixtuple &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicating that there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a connection from the output of v to the argument span s with syntactic type t T and semantic type t T</example>
		<phraseLemma>np indicate that there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that connects &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We initialize the set of connections using a sequential algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that connects the output of each event to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an argument of the following event which is a strong baseline as shown in Sec 1</example>
		<phraseLemma>np that connect np to np</phraseLemma>
	</can>
	<can>
		<phrase>Given the number of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; verbs we select a set of verbs V using a multinomial distribution</example>
		<phraseLemma>give the number of np</phraseLemma>
	</can>
	<can>
		<phrase>Comparison on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Comparison on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the scale does not always involve two individuals</example>
		<phraseLemma>comparison on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be associated with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Most of these types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; operators from any of our parts of speech</example>
		<phraseLemma>np can be associate with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as listed in Table 1</phrase>
		<frequency>7</frequency>
		<example>Last but not least each role points to an argument which can have various types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as listed in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as list in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; be assigned with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>As a result each predicate has a set of candidate arguments which should be labeled with their argument types and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;be assigned with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a semantic role edge</example>
		<phraseLemma>np be assign with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; implemented in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We also considered a statistical machine translation alignment method IBM Model 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with HMM alignment implemented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Giza</example>
		<phraseLemma>np with np implement in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is similar to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The similarity measure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the kernel is similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the algorithm M V M Ewe described in Algorithm 1 but instead of measuring the similarity between the sense vectors of t and the vector representation of t in test message now we measure the similarity between two tweets ur and us</example>
		<phraseLemma>np in np be similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may be &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>A word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may be an entity in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one puzzle but in a different problem it might not be an entity or might belong to a different category altogether</example>
		<phraseLemma>np may be np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; once in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>As source tokens can appear more than &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;once in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a source text they are located</example>
		<phraseLemma>np once in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; compared to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Furthermore results shown in Table 1 point out the complementarity between negative models and positive models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a drop of almost BLEU points compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corresponding conﬁguration using all models when removing one type of models on both translation directions</example>
		<phraseLemma>np with np compare to np</phraseLemma>
	</can>
	<can>
		<phrase>The work of &lt;NP&gt; is supported by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The work of the ﬁrst author is supported by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a CIFRE grant from French ANRT</example>
		<phraseLemma>the work of np be support by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the size of &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In such architecture &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the size of output vocabulary is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a bottleneck when normalized distributions are expected</example>
		<phraseLemma>np the size of np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used to generate &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In the architecture different fusing techniques &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fused translations for the further sentencelevel selection enabling us to exploit more sophisticated information of the whole sentence</example>
		<phraseLemma>np can be use to generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>Our primary technical contribution is a hierarchical adaptation technique for a postediting scenario &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with incremental adaptation in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system</example>
		<phraseLemma>np with np in which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is capable of capturing &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>This architecture &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is capable of capturing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; important pieces of information seen in a bigger context</example>
		<phraseLemma>np be capable of capture np</phraseLemma>
	</can>
	<can>
		<phrase>Results show &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Results show signiﬁcant improvements in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prediction over the baseline as well as over systems trained on state of the art feature sets for all datasets</example>
		<phraseLemma>result show np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learn &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In particular we represent source and target words using word embeddings and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learn a transformation between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two embedding spaces in order to approximate wsup thus downweighting incorrect translation candidates proposed by triangulation</example>
		<phraseLemma>np learn np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that appeared &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We took words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that appeared more than times in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parallel corpus for the training set and between 1 times for the held out dev set</example>
		<phraseLemma>np that appear np in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; used for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For MalagasyFrench the wsup distributions used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervision were taken to be uniform distributions over the dictionary translations</example>
		<phraseLemma>for np use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is integrated in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Our rule selection model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is integrated in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Moses stringtotree system as an additional feature of the loglinear model</example>
		<phraseLemma>np be integrate in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not improve over &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The results in Table 1 show that even on concatenated data our rule selection model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not improve over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline</example>
		<phraseLemma>np do not improve over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; consisting of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We tuned the translation models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using MERT using the development set of the above mentioned campaign consisting of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence pairs for each language pair</example>
		<phraseLemma>np use np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in previous studies</phrase>
		<frequency>7</frequency>
		<example>Continuity hypothesis has been linked with cognitive difﬁculties in discourse processing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in previous studies&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in previous study</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that both &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We further explore the linguistic differences in deceptive content that relate to deceivers gender and age and ﬁnd evidence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; age and gender play an important role in peoples word choices when fabricating lies</example>
		<phraseLemma>np that both np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not beneﬁt from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Although no English words start with either or English speakers judge srip to be a better potential word of English than mbip this is likely because shares properties with stridentliquid clusters that do exist in English such as as in slip and as in shrewd whereas &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not beneﬁt from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any sonorantstop onset sequences —none exist in English</example>
		<phraseLemma>np do not beneﬁt from np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; each of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the VOICING experiment each of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sets contained ﬁve words one starting with each of the ﬁve CONFATT onsets in the IDENTITY experiment each of the sets contained eight words one with each of the CONFATT and NONCONFATT consonant pairs</example>
		<phraseLemma>in np each of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; given &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We perform inference to ﬁnd the posterior &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over template sets given&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the exposure datasets used in the human experiments described above</example>
		<phraseLemma>np over np give np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are generated from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In that model classes are generated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a probabilistic context free grammar highly speciﬁed rules are therefore implicitly less probable as in our model</example>
		<phraseLemma>in np be generate from np</phraseLemma>
	</can>
	<can>
		<phrase>On &lt;NP&gt; is &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On the other hand variableid assignment is a big issue in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; direct logical form construction for many math problems</example>
		<phraseLemma>on np be np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we create &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>For the other type names &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the top we create&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classes and rules automatically in the form of classTN &amp;gt TN” where TN is a type name</example>
		<phraseLemma>np in np we create np</phraseLemma>
	</can>
	<can>
		<phrase>This is likely because &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is likely because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the questions are short and the parse trees quite shallow such that the two problems that the LSTM was invented for do not play much of a role</example>
		<phraseLemma>this be likely because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; strongly depend on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>These approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;strongly depend on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input parse trees and are very sensitive to parsing errors</example>
		<phraseLemma>np strongly depend on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is distinguished by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Unlike the other tasks we consider Crossblock &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is distinguished by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a challenging associated search problem</example>
		<phraseLemma>np be distinguish by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>So conditional random ﬁelds have been applied to this task and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed better performance than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; POSbased Markov models</example>
		<phraseLemma>np show np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; were regarded as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In that work tags in hypertexts were regarded as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; annotations and used to improve WS performance</example>
		<phraseLemma>np in np be regard as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to build &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Words Essentially the IM engine which we have explained above does not have the ability to enumerate words which are unknown to the word segmenter and the pronunciation estimator &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data</example>
		<phraseLemma>np use to build np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; in performance</phrase>
		<frequency>7</frequency>
		<example>Zheng applied the architecture of Collobert to Chinese word segmentation and POS tagging also he proposed a perceptron style algorithm to speed up the training process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with negligible loss in performance&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np in performance</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that our model achieves &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We evaluate our model on three popular benchmark datasets and the experimental results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that our model achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart performance with the smaller context window size 1</example>
		<phraseLemma>np show that we model achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; ie &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In contrast with these methods we propose to leverage bilingual unlabeled data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;ie a ChineseEnglish corpus with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence alignment</example>
		<phraseLemma>np ie np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into our model</phrase>
		<frequency>7</frequency>
		<example>Inspired by the averaging over the models from different iterations we combine them as a part of a sampling process we treat the derivation trees acquired from different iterations as additional training data and increment the corresponding customers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into our model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np into we model</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we used &lt;NP&gt; to train &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For generative models we used GIZA to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; IBM Model 1 in two directions</example>
		<phraseLemma>for np we use np to train np</phraseLemma>
	</can>
	<can>
		<phrase>In order to model &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to model&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst intuition BCorrRAE punishes bilingual structures that violate word alignment constraints and rewards those in consistent with word alignments</example>
		<phraseLemma>in order to model np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; those in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In order to model the ﬁrst intuition BCorrRAE punishes bilingual structures that violate word alignment constraints and rewards &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;those in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; consistent with word alignments</example>
		<phraseLemma>np those in np</phraseLemma>
	</can>
	<can>
		<phrase>With &lt;NP&gt; we adopt &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;With the collected reordering examples we adopt&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the maximal entropy toolkit developed by Zhang to train the reordering model with the following parameters</example>
		<phraseLemma>with np we adopt np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by marginalizing over &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Kocˇisky´ propose a probability model to capture more semantic information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by marginalizing over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word alignments</example>
		<phraseLemma>np by marginalize over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; probably because &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Training an adapted NDAMv 1 model over selected data gave improvements over MML in two test sets but could not restore the baseline performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;probably because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the useful data has already been ﬁltered by the selection process</example>
		<phraseLemma>np probably because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; used in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Then we study the connection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between heavy sentences and the factors used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior work to split a Chinese sentence into multiple sentences showing that they do not fully determine the empirically deﬁned contentheavy status</example>
		<phraseLemma>np between np use in np</phraseLemma>
	</can>
	<can>
		<phrase>Some of &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Some of the recent systems using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MT techniques separately model the need for sentence splitting</example>
		<phraseLemma>some of np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; in accuracy</phrase>
		<frequency>7</frequency>
		<example>The next selected class is typed dependencies over universal POS tags that have an edge across commas in the sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with an 1 increase in accuracy&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np in accuracy</phraseLemma>
	</can>
	<can>
		<phrase>We show that for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such sentences a multisentence translation is preferred by readers in terms of ﬂow and understandability</example>
		<phraseLemma>we show that for np</phraseLemma>
	</can>
	<can>
		<phrase>As we discussed in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we discussed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 current tuning methods for HIERO system are mostly searchagnostic</example>
		<phraseLemma>as we discuss in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; rather than on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;rather than on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a tokenbytoken basis</example>
		<phraseLemma>np rather than on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; representing &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We then perform a 1 CCA between these word representations and vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;representing the projected tags from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all resourcerich languages</example>
		<phraseLemma>np represent np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to choose &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The majority method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to choose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most common tag from the projected tags of the current token</example>
		<phraseLemma>np be to choose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relied only on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Merialdo &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relied only on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a tag dictionary extracted from annotated data but he used the annotated tags from his test data as well as his training data to construct his tag dictionary so his evaluation was not really fair</example>
		<phraseLemma>np rely only on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are treated as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The tagging model we use has the property that all digits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are treated as indistinguishable for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all features</example>
		<phraseLemma>np be treat as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on the development set for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>At the end of each training pass i while evaluating the current model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the development set for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; early stopping using threshold Ti we also ﬁnd the highest probability threshold Ti such that choosing a lower threshold would not enable any additional correct taggings on the development set using the current model</example>
		<phraseLemma>np on the development set for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is shown in Figure 1 and</phrase>
		<frequency>7</frequency>
		<example>A contrast between a neural network model and a linear model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is shown in Figure 1 and&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be show in figure 1 and</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that arises from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>A natural question &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that arises from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the contrast is whether traditional discrete features and continuous neural features can be integrated for better accuracies</example>
		<phraseLemma>np that arise from np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁnd that using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ﬁnd that using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hidden layer speciﬁcally for embedding features gives better results compared with using no hidden layers</example>
		<phraseLemma>we ﬁnd that use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shared &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Table 1 illustrates the parsing results our parser with nonprojective parsing algorithm together with three baseline systems—the twostage system and the two intermediate models Model 1 and Model 1 —and the best systems reported in CoNLL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shared tasks for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each language</example>
		<phraseLemma>np share np for np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the accuracies of Japanese empty category detection using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original and our modiﬁcation of the with ablation test</example>
		<phraseLemma>table 1 show np use np</phraseLemma>
	</can>
	<can>
		<phrase>They treat &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They treat the developing language of learners as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an interlanguage as suggested by D´ıazNegrillo and annotate it as is</example>
		<phraseLemma>they treat np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs on par with &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The countingbased PMI method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs on par with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Embedding based approximation of it</example>
		<phraseLemma>np perform on par with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; requires &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In contrast integrating such features into a model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with discrete features requires&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nontrivial manual tweaking</example>
		<phraseLemma>np with np require np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; most notably &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Additionally we also experiment with diﬀerent transition systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;most notably&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the integrated parsing and partofspeech tagging system of Bohnet and Nivre and also the swap system of Nivre</example>
		<phraseLemma>np most notably np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which in &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>These features are embedded and then concatenated to form the embedding layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which in turn is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input to the ﬁrst hidden layer</example>
		<phraseLemma>np which in np be np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 gives &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 gives the accuracy of fusion and baselines for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLLIP on the development corpora</example>
		<phraseLemma>table 1 give np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provide &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>While additional parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provide greater diversity nbest lists from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; common parsers are varied enough to provide improvements for parse hybridization</example>
		<phraseLemma>np provide np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; illustrates &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>On the other hand the skipngram model process words at only 1 k words per 1 as it must predict every word in the window b Figure 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;illustrates the attention model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the prediction of the word south in the sentence antartica has little rainfall with the south pole making it a continental desert</example>
		<phraseLemma>np illustrate np for np</phraseLemma>
	</can>
	<can>
		<phrase>We can see that in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can see that in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task our models do not perform as well as the CBOW and Skipngram model which hints that our model is learning embeddings that learn more towards syntax</example>
		<phraseLemma>we can see that in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; so that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>The arc is labelled by noting the bestscoring RightArc label &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on each Shift action so that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the label can be assigned during nonmonotonic Reduce</example>
		<phraseLemma>np on np so that np</phraseLemma>
	</can>
	<can>
		<phrase>Combining &lt;NP&gt; leads to &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Combining the KN FFNN and BRNN JTR models leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an overall gain of 1 BLEU on both dev and test</example>
		<phraseLemma>combine np lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; exploring the use of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>To the best of our knowledge there has not been any other work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;exploring the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; attentionbased architectures for NMT</example>
		<phraseLemma>np explore the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is also based on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The stateoftheart model for opinion target extraction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is also based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a CRF</example>
		<phraseLemma>np be also base on np</phraseLemma>
	</can>
	<can>
		<phrase>In the following we describe &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the following we describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an elegant RNN architecture to address this problem</example>
		<phraseLemma>in the follow we describe np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shows that our approach achieves &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Evaluation on the standard GeoQuery benchmark dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows that our approach achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart across various languages including English German and Greek</example>
		<phraseLemma>np show that we approach achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; increase &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Speciﬁcally since the translation rules play a critical role in SMT we explore to improve translation rule quality and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;increase its coverage in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three ways</example>
		<phraseLemma>np increase np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; inspired by &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We instead adopt a twostage learning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inspired by recent work in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic parsing</example>
		<phraseLemma>np inspire by np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is identical to that of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We train a loglinear classiﬁer for the CC relations where the setup of the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is identical to that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the binary relation model in Section 1</example>
		<phraseLemma>np be identical to that of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only relying on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>GEOS without diagram parsing solves geometry questions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only relying on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the literals interpreted from the text</example>
		<phraseLemma>np only rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>By combining the new ﬁrst order logic &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based semantic representation in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lieu of a syntactic representation with a more expressive model we can encode the sentence interpretations required to perform the disambiguation task</example>
		<phraseLemma>np base np in np</phraseLemma>
	</can>
	<can>
		<phrase>We tested the performance of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We tested the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model described in the previous section on the LAVA dataset presented in section 1</example>
		<phraseLemma>we test the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>We group &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We group such related work into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three groups based on whether KB text or both sources of information are used</example>
		<phraseLemma>we group np into np</phraseLemma>
	</can>
	<can>
		<phrase>However even &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However even&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such a simple approach has been shown to be very competitive</example>
		<phraseLemma>however even np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; out of 1</phrase>
		<frequency>7</frequency>
		<example>Almost all entities occur in textual relations 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;out of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np out of 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also from &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We initialized the KBtext models from the KBonly models and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; random initial values and stopped optimization when the overall MRR on the validation set decreased</example>
		<phraseLemma>np also from np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; for &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used K = 1 for all KBtext models as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; higher dimension was also not helpful for them</example>
		<phraseLemma>we use np for np as np</phraseLemma>
	</can>
	<can>
		<phrase>It is interesting that &lt;CL&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is interesting that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the text and the compositional representations helped most for this combined model</example>
		<phraseLemma>it be interesting that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; rather than as &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>Similarly for preferences η a large value in this vector might reﬂect as skill or the preferences of as sponsors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;rather than as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; personal interest the topic</example>
		<phraseLemma>np rather than as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is updated by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>The selected cluster centroid &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is updated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the addition of the context vector and the associated sense vector is passed as input to the compositional layer</example>
		<phraseLemma>np be update by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is applied for &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>While siamese networks have been also used in the past for NLP purposes to the best of our knowledge this is the ﬁrst time that such a setting &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is applied for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; paraphrase detection</example>
		<phraseLemma>np be apply for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are pretrained on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>In all experiments the word representations and compositional models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are pretrained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the British National Corpus a generalpurpose text corpus that contains 1 million sentences of written and spoken English</example>
		<phraseLemma>np be pretrain on np</phraseLemma>
	</can>
	<can>
		<phrase>This is achieved by &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is achieved by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁrst selecting for each word the sense vector that is the closest to the average of all other word vectors in the same sentence and then composing the selected sense vectors without further considerations regarding ambiguity</example>
		<phraseLemma>this be achieve by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outperform a number of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We show that our models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outperform a number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tree generation baselines</example>
		<phraseLemma>np outperform a number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; where n is the length of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>This algorithm has complexity O &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;where n is the length of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the yield</example>
		<phraseLemma>np where n be the length of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to compute &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>We use cosine similarity between vector representations of two posts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similarity</example>
		<phraseLemma>np in order to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; particularly on &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>On SHORT threads the grammar models outperform LSEG and CLUS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;particularly on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the projective threads</example>
		<phraseLemma>np particularly on np</phraseLemma>
	</can>
	<can>
		<phrase>To compare &lt;NP&gt; against &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To compare our models against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; slower models we sampled 1 documents from JSTOR withholding 1 as testing set</example>
		<phraseLemma>to compare np against np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In other words our ﬁlters are linear mappings over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the higher dimensional interaction terms rather than the original word coordinates</example>
		<phraseLemma>in np be np over np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 reports the results of &lt;NP&gt;</phrase>
		<frequency>7</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 reports the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SVM NBoW and our model on the news categorization task</example>
		<phraseLemma>table 1 report the result of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; aims to maximize &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>An agent playing the game &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;aims to maximize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rewards that it obtains from the game engine upon the occurrence of certain events</example>
		<phraseLemma>np aim to maximize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in terms of number of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We demonstrate that our model LSTMDQN signiﬁcantly outperforms the baselines &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; completed quests and accumulated rewards</example>
		<phraseLemma>np in term of number of np</phraseLemma>
	</can>
	<can>
		<phrase>For instance on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a fantasy MUD game our model learns to complete 1 of the quests while the bagofwords model and a random baseline solve only 1 and 1 of the quests respectively</example>
		<phraseLemma>for instance on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; which is &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This vector is then input &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into the 1 module which is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an action scorer</example>
		<phraseLemma>np into np which be np</phraseLemma>
	</can>
	<can>
		<phrase>In our experiments we consider &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our experiments we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such positivereward transitions to have higher priority and keep track of them in D</example>
		<phraseLemma>in we experiment we consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not provide &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>If the game &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not provide such clues for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current state we consider all objects in the game</example>
		<phraseLemma>np do not provide np for np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; on &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate all the models on the Fantasy world in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same manner as before and report reward quest completion rates and Qvalues</example>
		<phraseLemma>we evaluate np on np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compute &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>However since distributional models represent words as aggregated distributions of their contexts and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compute semantic similarity from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these context distributions the contexts that they use need to be generic enough to yield meaningful overlap between concepts</example>
		<phraseLemma>np compute np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is shared with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The broader goal of getting at referential information with distributional semantics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is shared with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Herbelot</example>
		<phraseLemma>np be share with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; they rely on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Indeed while systems have successfully been developed to model entailment between quantiﬁers ranging from natural logic approaches to distributional semantics solutions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;they rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an explicit representation of quantiﬁcation</example>
		<phraseLemma>np they rely on np</phraseLemma>
	</can>
	<can>
		<phrase>Using &lt;NP&gt; annotated with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Using a publicly available dataset of feature norms annotated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; quantiﬁers 1 we show that humanlike intuitions about the quantiﬁcation of simple subject/predicate pairs can be induced from standard distributional data</example>
		<phraseLemma>use np annotated with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a combination of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Proposals for a FDS ie &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a combination of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both formalisms roughly fall into two groups</example>
		<phraseLemma>np a combination of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which accounts for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This information state is described in terms of probabilistic logic &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which accounts for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an agents uncertainty about what the world is like</example>
		<phraseLemma>np which account for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; set to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In the following we use a derived gold standard including all 1 quantiﬁed classes in QMR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the annotation set to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; majority opinion instances</example>
		<phraseLemma>np with np set to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the form of &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In order to remedy data sparsity we consider the use of additional data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the form of the animal dataset from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Herbelot</example>
		<phraseLemma>np in the form of np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be represented in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Ontologies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be represented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; various ways but in this paper we assume they are formalised in terms of sets of entities</example>
		<phraseLemma>np can be represent in np</phraseLemma>
	</can>
	<can>
		<phrase>We convert &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>As both datasets are annotated with natural language quantiﬁers rather than cardinality ratios &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we convert the annotation into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a numerical format where ALL &amp;gt 1 MOST &amp;gt 1 SOME &amp;gt 1 FEW &amp;gt 1 and NO &amp;gt 1</example>
		<phraseLemma>np we convert np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is in &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>These ﬁgures provide an upper bound performance for the system ie we will consider having reached human performance if the correlation between system and gold standard &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is in the same range as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the agreement between humans</example>
		<phraseLemma>np be in np as np</phraseLemma>
	</can>
	<can>
		<phrase>This is in line with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is in line with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the interannotator agreement 1</example>
		<phraseLemma>this be in line with np</phraseLemma>
	</can>
	<can>
		<phrase>As expected &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As expected the animal representations have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; higher quality than the other two</example>
		<phraseLemma>as expect np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; similarly for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Deriving natural language quantiﬁers from these values involves setting 1 thresholds tall tmost tsome and tfew so that for instance if the value of v~k along dm is more than tall it is the case that all instances of w~k have property pm and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;similarly for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the other quantiﬁers</example>
		<phraseLemma>np similarly for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; proportional to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This has the effect that when the mapped quantiﬁer equals the gold quantiﬁer the system scores 1 when the mapped value deviates from the gold standard but produces a true sentence the system gets a partial score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;proportional to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the distance between its output and the gold data when the mapping results in a false sentence the system is penalised with minus points</example>
		<phraseLemma>np proportional to np</phraseLemma>
	</can>
	<can>
		<phrase>Compared to &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Compared to tree grammars graph grammars have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stronger generative capacity over structures</example>
		<phraseLemma>compare to np have np</phraseLemma>
	</can>
	<can>
		<phrase>We build &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We build our translation model in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the loglinear framework with standard features</example>
		<phraseLemma>we build np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; thus results in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Furthermore compared to Dep 1 Str our system produces a better translation for the ”X X” expression which is not explicitly represented in the dependency structure and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;thus results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a wrong translation in Dep 1 Str</example>
		<phraseLemma>np thus result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by using &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Shen propose a stringtodependency model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by using dependency fragments of neighbouring words on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target side which makes the model easier to include a dependencybased language model</example>
		<phraseLemma>np by use np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; focus on ﬁnding &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>A common approach is to use monolingual syntactic trees and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;focus on ﬁnding&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a transduction function of the sibling subtrees under the nodes</example>
		<phraseLemma>np focus on ﬁnding np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; trained with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a 1 gram language model trained with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KenLM 1 tune 1 times with kbmira to account for tuner instability and evaluated using Multeval 1 for statistical signiﬁcance on 1 metrics</example>
		<phraseLemma>we use np train with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; gives &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Figure 1 shows that Reordering Grammar &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;gives substantial performance improvements at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all distortion limits</example>
		<phraseLemma>np give np at np</phraseLemma>
	</can>
	<can>
		<phrase>We aim to address &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We aim to address&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the data scarcity problem and combine translators lexical precision and interpreters syntactic ﬂexibility</example>
		<phraseLemma>we aim to address np</phraseLemma>
	</can>
	<can>
		<phrase>To show the effect of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To show the effect of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rewritten references we compare the following MT systems</example>
		<phraseLemma>to show the effect of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are optimized for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This result is less clear as MT systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are optimized for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BLEU and RIBES penalizes word reordering making it difﬁcult to compare systems that intentionally change word order</example>
		<phraseLemma>np be optimize for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is much lower than &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The majority class baseline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is much lower than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dining at 1</example>
		<phraseLemma>np be much lower than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; itself or &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We do note that the water is slightly muddied because our algorithm may not distinguish well between sentiment toward the war Israel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;itself or&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; even sympathy toward casualties</example>
		<phraseLemma>np itself or np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the other</phrase>
		<frequency>6</frequency>
		<example>The table shows the average of all R edges for each nation within an alliance to a nation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the other&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the other</phraseLemma>
	</can>
	<can>
		<phrase>According to &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;According to our ratios NATO countries have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stark differences between how they view themselves versus how they view African Union/Arab League nations</example>
		<phraseLemma>accord to np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; solely based on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We added these during training and developement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;solely based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our interpretation and analysis of the data</example>
		<phraseLemma>np solely base on np</phraseLemma>
	</can>
	<can>
		<phrase>To compare against &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To compare against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; another purely unsupervised model we evaluate against principal component analysis a latent linear model that minimizes the average reconstruction error between an original data matrix X ∈ Rn×p and a lowdimensional approximation ZW&amp;gt where Z ∈ Rn×K can be thought of as a Kdimensional latent representation of the input and W ∈ Rp×K contains the eigenvectors of the K largest eigenvalues of the covariance matrix XX&amp;gt providing a Kdimensional representation for each feature</example>
		<phraseLemma>to compare against np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to be &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To evaluate this tradeoff we compare against a supervised model trained using naturally occurring data – users who selfdeclare themselves &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in their proﬁles to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; liberal conservative democrat or republican</example>
		<phraseLemma>np in np to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; relative to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>While we might have considered variation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the predicate to be sufﬁcient in distinguishing between political parties we see that this is simply not the case variation in the subject may help anchor propositions in the spectrum relative to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each other</example>
		<phraseLemma>np in np relative to np</phraseLemma>
	</can>
	<can>
		<phrase>Among &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Among consumer related factors demographics such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; age gender and income have been studied extensively in marketing research</example>
		<phraseLemma>among np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that includes &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>It is the ﬁrst study &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that includes a comprehensive set of personal traits in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; brand preference analysis</example>
		<phraseLemma>np that include np in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we compute &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the 1 step we compute different similarities between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each candidate and the hashtag based on different types of contexts which are derived from either side</example>
		<phraseLemma>in np we compute np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are mapped to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our lexicon is constructed from Wikipedia page titles hyperlink anchors redirects and disambiguation pages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are mapped to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corresponding entities</example>
		<phraseLemma>np which be map to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are more relevant to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>It assumes that entities directly linked from more prominent anchors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are more relevant to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hashtag</example>
		<phraseLemma>np be more relevant to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contribute more to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Texts added in the same time period of a trending hashtag &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contribute more to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the context similarity between the entity and the hashtag</example>
		<phraseLemma>np contribute more to np</phraseLemma>
	</can>
	<can>
		<phrase>It measures &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It measures the distance between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two time series by ﬁnding optimal shifting and scaling parameters to match the shape of two time series</example>
		<phraseLemma>it measure np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that lead to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In other words the problem of ranking the most prominent entities becomes identifying the set of entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that lead to the largest number of entities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the candidate set</example>
		<phraseLemma>np that lead to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are depicted in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The general idea is that we ﬁnd an optimal ω such that the average error with respect to the top inﬂuencing entities is minimized The main steps &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are depicted in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1</example>
		<phraseLemma>np be depict in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a sample of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We used the Twitter API to collect from the public stream &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a sample of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 1 tweets from January to April</example>
		<phraseLemma>np a sample of np</phraseLemma>
	</can>
	<can>
		<phrase>Our model performs better than &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our model performs better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the systems that we combined based on manual and automatic evaluations</example>
		<phraseLemma>we model perform better than np</phraseLemma>
	</can>
	<can>
		<phrase>We report &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report ROUGE 1 and ROUGE 1 with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stemming and stopwords included</example>
		<phraseLemma>we report np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is equal to the number of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The sentence importance score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is equal to the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic words divided by the number of words in the sentence</example>
		<phraseLemma>np be equal to the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; divided by the number of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The sentence importance score is equal to the number of topic words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;divided by the number of words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence</example>
		<phraseLemma>np divide by the number of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; be the probability of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Let pλ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;be the probability of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ngram w in distribution λ</example>
		<phraseLemma>np be the probability of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Better summaries should include words or phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; higher importance</example>
		<phraseLemma>np that be of np</phraseLemma>
	</can>
	<can>
		<phrase>Comparing to &lt;NP&gt; achieves &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Comparing to these two SumCombine achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a lower performance on the DUC 1 data and a higher performance on the DUC 1 data</example>
		<phraseLemma>compare to np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>Inspired by &lt;NP&gt; we propose &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Inspired by phrasebased machine translation we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a phrasebased model to simultaneously perform sentence scoring extraction and compression</example>
		<phraseLemma>inspire by np we propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relies on &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This baseline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relies on merely the Englishside information for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; En glish sentence ranking in the original documents</example>
		<phraseLemma>np rely on np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; sets &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>If samples that data are drawn from are independent and differences in correlations are computed on independent data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;sets the Fisher r to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; z transformation is applied to test for signiﬁcant differences in correlations</example>
		<phraseLemma>np set np to np</phraseLemma>
	</can>
	<can>
		<phrase>We collect &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We collect a corpus of indicative tweets with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their associated articles and investigate to what extent they can be derived from the articles using extractive methods</example>
		<phraseLemma>we collect np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; could be applied to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This was done to investigate whether sentence compression techniques &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;could be applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; local context windows to generate the tweet</example>
		<phraseLemma>np could be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the better &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Hence we can say that the more formal the subject or the article &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the better&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tweet can be extracted from the article</example>
		<phraseLemma>np the better np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is obtained by combining &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Their highest performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is obtained by combining&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these visual features with normalized edit distance an orthographic similarity metric</example>
		<phraseLemma>np be obtain by combine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; require &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>However these models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;require document alignments as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; initial bilingual signals</example>
		<phraseLemma>np require np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains a number of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The network &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains a number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; layers starting with ﬁve convolutional layers two fully connected layers and ﬁnally a softmax and has been pretrained on the ImageNet classiﬁcation task using Caffe</example>
		<phraseLemma>np contain a number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the anonymous reviewers for their helpful comments</phrase>
		<frequency>6</frequency>
		<example>We thank Marco Baroni for useful feedback and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the anonymous reviewers for their helpful comments&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np the anonymous reviewer for they helpful comment</phraseLemma>
	</can>
	<can>
		<phrase>The dataset is divided into &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The dataset is divided into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 authorspecific corpora containing 1 1 and documents and each document has accompanying 1 Ratings label</example>
		<phraseLemma>the dataset be divide into np</phraseLemma>
	</can>
	<can>
		<phrase>We also report &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also report 1 fold cross validation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; accuracy on Standard Movie Reviews Dataset given by which contains ve and ve reviews in Hindi</example>
		<phraseLemma>we also report np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in English and &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>After imposing certain restrictions on these embeddings we perform supervised training using labeled sentiment corpora &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in English and a much smaller one in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Hindi to get the final classifier</example>
		<phraseLemma>np in english and np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to get &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>After imposing certain restrictions on these embeddings we perform supervised training using labeled sentiment corpora &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in English and a much smaller one in Hindi to get&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the final classifier</example>
		<phraseLemma>np in np to get np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we use movie reviews for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; opinion summarization task as they often have the following parts</example>
		<phraseLemma>in this paper we use np for np</phraseLemma>
	</can>
	<can>
		<phrase>In contrast &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast the target in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the entity/eventlevel task may be any noun or verb</example>
		<phraseLemma>in contrast np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisting of &lt;NP&gt; respectively</phrase>
		<frequency>6</frequency>
		<example>For each sentence s we deﬁne a set E &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisting of entities events and the writer of s and sets P and N consisting of positive and negative sentiments respectively&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np consist of np respectively</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisting of &lt;NP&gt; consisting of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For each sentence s we deﬁne a set E &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisting of entities events and the writer of s and sets P and N consisting of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive and negative sentiments respectively</example>
		<phraseLemma>np consist of np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; covers &lt;NP&gt; of topics</phrase>
		<frequency>6</frequency>
		<example>MPQA 1 MPQA 1 is a recently developed corpus with entity/eventlevel sentiment annotations 1 It is built on the basis of MPQA 1 which includes editorials reviews news reports and scripts of interviews from different news agencies and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;covers a wide range of topics&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np cover np of topic</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained with &lt;NP&gt; achieves &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our evaluation shows that the classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained with manually labeled data achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; good performance at identifying positive and negative similes</example>
		<phraseLemma>np train with np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is widely used for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Twitter is a popular microblogging platform and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is widely used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiment analysis</example>
		<phraseLemma>np be widely use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which contains &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We count whether the number of positive connotation words is greater in a simile using a Connotation Lexicon &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which contains 1 words with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive connotation and 1 words with negative connotation</example>
		<phraseLemma>np which contain np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; increases &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Finally Row shows that using both types of training instances further improves performance for positive polarity and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;increases precision for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; negative polarity but with some loss of recall</example>
		<phraseLemma>np increase np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which takes advantage of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Moreover the aforementioned work mainly focuses on improving visual contents recognition by introducing text features while our work will take the opposite route &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which takes advantage of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; visual information to improve event coreference resolution</example>
		<phraseLemma>np which take advantage of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In this work we adopt the stateoftheart techniques and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that train robust convolutional neural networks over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; millions of web images to detect 1 semantic categories deﬁned in ImageNet from each image</example>
		<phraseLemma>np that np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; scores of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To compute the similarity between videos associated with two candidate event mentions we sample multiple frames from each video and aggregate the similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;scores of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the few most similar image pairs between the videos</example>
		<phraseLemma>np score of np</phraseLemma>
	</can>
	<can>
		<phrase>We get &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We get the highlevel visual representation Fm = F C 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each frame fm from the output of the 1 nd to the last fully connected layer of CNN model</example>
		<phraseLemma>we get np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; calculate &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Finally we ﬁnd the video frames corresponding to the event mentions remove the anchor frames and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;calculate the visual similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the videos</example>
		<phraseLemma>np calculate np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each paired with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In total this dataset contains photos of basic object types with 1 million labeled instances in 1 k images &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each paired with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 captions</example>
		<phraseLemma>np each pair with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; captures &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We find evidence that the VQA dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;captures more abstract concepts than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other datasets with almost 1 of the words found in our abstract concept resource</example>
		<phraseLemma>np capture np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; predict &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In image description generation work Kulkarni manually map spatial relations to predeﬁned prepositions whilst Yang &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;predict prepositions from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; largescale text corpora solely based on the complement term with the prepositions constrained to describing scenes</example>
		<phraseLemma>np predict np from np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; perform better than &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In general geometric features perform better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline and when combined with text features further improve the results</example>
		<phraseLemma>in np perform better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are seen in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In practice D can be derived in various ways in our experiments we simply deﬁne D to include all French words f such that e and f &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are seen in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a translation pair</example>
		<phraseLemma>np be see in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are weighted with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Cooccurrence counts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are weighted with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PPMI and SVD is applied to the resulting matrix reducing the dimensionality to 1</example>
		<phraseLemma>np be weight with np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 summarizes &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 summarizes our top results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the TOEFL BLESS and also the SimLex 1 similarity test and compares them to a baseline score from the Skipgram model trained on the same data using a window size of 1 negative samples and dimensional vectors</example>
		<phraseLemma>table 1 summarize np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to show that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>Our main contribution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; neural networks can successfully learn unlexicalized models that infer a useful word representation from the character stream</example>
		<phraseLemma>np be to show that np</phraseLemma>
	</can>
	<can>
		<phrase>Results are presented in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Results are presented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 half of Table 1</example>
		<phraseLemma>result be present in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; have shown &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In recent years neural network models have shown&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; impressive performance on many natural language processing tasks such as speech recognition machine translation text classiﬁcation and image description generation</example>
		<phraseLemma>in np have show np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have shown &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In recent years neural network models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have shown impressive performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many natural language processing tasks such as speech recognition machine translation text classiﬁcation and image description generation</example>
		<phraseLemma>np have show np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Let φ be a d dimensional vector representing the word w and φ be a h dimensional vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is the local representation at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word position i</example>
		<phraseLemma>np which be np at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; so that &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To extend this model to a zeroshot learning classiﬁer we use parameter sharing among label hyperplanes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;so that similar labels have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similar hyperplanes</example>
		<phraseLemma>np so that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; where the number of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We tested this SLU model on datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;where the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; attribute types and values is increased and show much better results than the baselines especially in recall</example>
		<phraseLemma>np where the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisting of &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Data We consider the Ferguson rumour data set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisting of tweets on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ru mours around Ferguson unrest</example>
		<phraseLemma>np consist of np on np</phraseLemma>
	</can>
	<can>
		<phrase>We observe &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In our setting &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we observe posts over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a target rumour i for one hour and over reference rumours for two hours</example>
		<phraseLemma>np we observe np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is modeled using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The intensity function λ &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is modeled using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a latent function f sampled from a Gaussian process</example>
		<phraseLemma>np be model use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as outlined in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Assuming the previous tweet occurred at time s we obtain the arrival time of next tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as outlined in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1</example>
		<phraseLemma>np as outline in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by including &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our joint objective function uses a discriminative maxmargin approach to both model the contents of documents and produce good predictions of links in addition it improves prediction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by including lexical terms in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the decision function</example>
		<phraseLemma>np by include np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We modiﬁed six existing highperformance systems to enable lossaugmented decoding and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained these models with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; six different methods</example>
		<phraseLemma>np trained np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each of which is &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Solves a sequence of quadratic programs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each of which is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an approximation to the dual formulation of the marginbased learning problem</example>
		<phraseLemma>np each of which be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is set to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In BINARY weighting entry 1 ≤ i ≤ n in the distributional vector of target word w is set to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 iff vi and w cooccur at a distance of at most ten words in the corpus and to 1 otherwise</example>
		<phraseLemma>np in np be set to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; replacing &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We do this by keeping only θ randomly chosen occurrences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the corpus and replacing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all other occurrences with a different token</example>
		<phraseLemma>np in np replace np</phraseLemma>
	</can>
	<can>
		<phrase>This is true for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is true for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both mixed and separate initialization with the exception of WS for which mixed is better in only 1 of 1 cases</example>
		<phraseLemma>this be true for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; that have &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In such cases distributional initialization makes the learning task easier since in addition to the contexts of the rare word the learner now also has access to the global distribution of the rare word and can take advantage of weight sharing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with other words that have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similar distributional representations to smooth embeddings systematically</example>
		<phraseLemma>np with np that have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>An alternative to using distributional information for initialization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to use syntactic and semantic information for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; initialization</example>
		<phraseLemma>np be to use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be formalized as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>These knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be formalized as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; directed multirelation graphs whose node correspond to entities connected with edges encoding various kind of relationships</example>
		<phraseLemma>np can be formalize as np</phraseLemma>
	</can>
	<can>
		<phrase>We have &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>These embeddings are learnt so that for each fact in the KB &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we have h ≈ t in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embedding space</example>
		<phraseLemma>np we have np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; randomly sampled from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Families are connected among them by marriage links between two members &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;randomly sampled from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same layer of different families</example>
		<phraseLemma>np randomly sample from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was used to assign &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For soft labeling percentage item agreement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was used to assign&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; instance weights</example>
		<phraseLemma>np be use to assign np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that maximized &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>If there were more candidate cutoff values we trained and evaluated a classiﬁer on a development set and chose the value for HighAgree &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that maximized Hard Case performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the development set</example>
		<phraseLemma>np that maximize np on np</phraseLemma>
	</can>
	<can>
		<phrase>Of &lt;NP&gt; 1 were &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Of total instances 1 were&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Hard Cases and were Easy Cases</example>
		<phraseLemma>of np 1 be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using each of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our results on all ﬁve tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using each of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training strategies and variously evaluating on all Easy or Hard Cases can be seen in Table 1</example>
		<phraseLemma>np use each of np</phraseLemma>
	</can>
	<can>
		<phrase>We tried to make &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We tried to make&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the comparison as fair as possible</example>
		<phraseLemma>we try to make np</phraseLemma>
	</can>
	<can>
		<phrase>To do &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To do this the corresponding word vectors of all words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dataset are clustered and the purity of the returned clusters is computed with respect to the labeled dataset</example>
		<phraseLemma>to do np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that occur as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The more recent MEN dataset follows a similar strategy but restricts queries to words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that occur as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; annotations in an image dataset</example>
		<phraseLemma>np that occur as np</phraseLemma>
	</can>
	<can>
		<phrase>We can view &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can view the relatedness task as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of evaluating a set of rankings similar to ranking evaluation in Information Retrieval</example>
		<phraseLemma>we can view np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; seem to capture &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Surprisingly word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;seem to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; even more complex linguistic properties</example>
		<phraseLemma>np seem to capture np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; that use &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a result LDA models that use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior knowledge only work in smallscale scenarios</example>
		<phraseLemma>as np that use np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; for &lt;NP&gt; For &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are many possible candidates for T and M For&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; example T could be ones favorite neural network mapping from Rd to Rd</example>
		<phraseLemma>there be np for np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also makes use of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The algorithm makes use of a training method that can leverage partial dependency structures and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also makes use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; conﬁdence scores from a perceptrontrained model</example>
		<phraseLemma>np also make use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; particularly when &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We empirically show that adding a bilingual dictionary improves parser performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;particularly when&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; target data is limited</example>
		<phraseLemma>np particularly when np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be captured by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our embeddings encode not just crosslingual correspondences but also capture dependency relations which we expect might be beneﬁcial for other NLP tasks based on dependency parsing eg crosslingual semantic role labelling where longdistance relationship &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be captured by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embedding</example>
		<phraseLemma>np can be capture by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are capable of learning &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In order to ﬁnd out whether the characterbased representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are capable of learning&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the morphology of words we applied the parser to morphologically rich languages speciﬁcally the treebanks of the SPMRL shared task</example>
		<phraseLemma>np be capable of learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by comparing &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We tested this speciﬁcally &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by comparing Chars to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a model in which all OOVs are replaced by the string UNK” during parsing</example>
		<phraseLemma>np by compare np to np</phraseLemma>
	</can>
	<can>
		<phrase>We report the results of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For Turkish &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we report the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Koo which only reported unlabeled attachment scores</example>
		<phraseLemma>np we report the result of np</phraseLemma>
	</can>
	<can>
		<phrase>We compare our model against &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our model against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the system of McDonald which also formulates sentence compression as a binary sequence labeling problem</example>
		<phraseLemma>we compare we model against np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are close to 1</phrase>
		<frequency>6</frequency>
		<example>The differences in Fscore between the three versions of LSTM are not signiﬁcant all scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are close to 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be close to 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are challenging for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>And sentences with quotes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are challenging for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parsers which in turn provide important signals for most compression systems</example>
		<phraseLemma>np be challenge for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; choose &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For each iteration we measure the performance on the development data and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;choose best parameters for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁnal tests</example>
		<phraseLemma>np choose np for np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we assume that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For all the experiments we assume that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input is a bag of words without order and the output is a fully ordered sentence</example>
		<phraseLemma>for np we assume that np</phraseLemma>
	</can>
	<can>
		<phrase>In order to study &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to study&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the inﬂuence of parsing accuracy of the training data we also use tenfold jackkniﬁng to construct WSJ training data with different accuracies</example>
		<phraseLemma>in order to study np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; respectively by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Figure 1 shows distributions of distortion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;respectively by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the syntactic and Ngram model</example>
		<phraseLemma>np respectively by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; to capture &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>As shown in Table 1 NLM 1 NLM 1 NLM 1 and NLM 1 respectively &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use 1 1 and 1 bins to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NLM feature values</example>
		<phraseLemma>np use np to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; give &lt;NP&gt; compared with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Syntactic models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;give better performance compared with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Ngram models despite trained with less data</example>
		<phraseLemma>np give np compare with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; compared with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The model shows signiﬁcant performance gains &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the DUC 1 shared task compared with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several strong baselines</example>
		<phraseLemma>np on np compare with np</phraseLemma>
	</can>
	<can>
		<phrase>We will assume that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We will assume that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the words in the summary also come from the same vocabulary V and that the output is a sequence yN</example>
		<phraseLemma>we will assume that np</phraseLemma>
	</can>
	<can>
		<phrase>We set &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Based on the validation set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we set hyperparameters as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; D = 1 H = 1 C = 1 L = 1 and Q = 1</example>
		<phraseLemma>np we set np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is useful for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Note that the additional extractive features bias the system towards retaining more input words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is useful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the underlying metric</example>
		<phraseLemma>np which be useful for np</phraseLemma>
	</can>
	<can>
		<phrase>Examples of &lt;NP&gt; include &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Examples of these techniques include&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LexRank TextRank and the work by</example>
		<phraseLemma>example of np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is better suited for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Summarization based on rhetorical structure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is better suited for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; shorter documents and is highly dependent on the quality of the discourse parser that is used</example>
		<phraseLemma>np be better suit for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; we used &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For expanding the citation vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the related biomedical terminology we used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SNOMED CT 1 ontology by which we added synonyms of the concepts in the citation text to the citation vector</example>
		<phraseLemma>np use np we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; demonstrate that &lt;NP&gt; outperforms &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The results of experiments on the data collected from a real world microblogging service &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;demonstrate that the proposed method outperforms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stateoftheart methods that do not consider these aspects</example>
		<phraseLemma>np demonstrate that np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned as &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>A word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned as an item from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a vocabulary with W distinct words indexed by w = {</example>
		<phraseLemma>np be deﬁned as np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which constructs &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In this section we present GRAW &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which constructs&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a coupled bagofwords model by exploiting the correlation of readability among the words</example>
		<phraseLemma>np which construct np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on the graph</phrase>
		<frequency>6</frequency>
		<example>Secondly we estimate reading levels of documents by applying label propagation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the graph&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on the graph</phraseLemma>
	</can>
	<can>
		<phrase>We compute &lt;NP&gt; between &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compute the similarity between any pair of documents using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Euclidean distance and built the featurebased graph in the same way as above</example>
		<phraseLemma>we compute np between np use np</phraseLemma>
	</can>
	<can>
		<phrase>Due to &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Due to its unstructured and unfocused nature automatic ﬁltering of social media content is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a necessity for further analysis</example>
		<phraseLemma>due to np be np</phraseLemma>
	</can>
	<can>
		<phrase>We focus on &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We focus on tweets as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; suitable example for unstructured textual information shared in social media</example>
		<phraseLemma>we focus on np as np</phraseLemma>
	</can>
	<can>
		<phrase>As features we used &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As features we used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a vector with the frequency of each ngram</example>
		<phraseLemma>as feature we use np</phraseLemma>
	</can>
	<can>
		<phrase>This is followed by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is followed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a results section in which we report differences in performance by means of qualitative and inferential statistics</example>
		<phraseLemma>this be follow by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; follows &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For sufﬁciently many samples the statistic &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;follows a distribution with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; k 1 degrees of freedom</example>
		<phraseLemma>np follow np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; this leads to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Here we use the simpler linear covariance function and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;this leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Bayesian linear ridge regression</example>
		<phraseLemma>np this lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is then given by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The prediction of a new essay represented by x in the target domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is then given by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; xT w¯ t</example>
		<phraseLemma>np be then give by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; extracted from &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Brieﬂy given a topic and a corresponding relevant claim &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;extracted from a Wikipedia article by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human annotators the annotators were asked to mark corresponding evidence – text segments sup porting the claim</example>
		<phraseLemma>np extract from np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are generated as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>All articles are split into sentences and all consecutive segments up to three sentences within a paragraph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are generated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; candidates</example>
		<phraseLemma>np be generate as np</phraseLemma>
	</can>
	<can>
		<phrase>This is motivated by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is motivated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the observation that in most practical use cases it is usually more important to be able to support many claims than to provide all the CDE available for a single claim</example>
		<phraseLemma>this be motivate by np</phraseLemma>
	</can>
	<can>
		<phrase>Comparing &lt;NP&gt; to &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Comparing the pipeline performance to the baseline using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only the context dependent component the results indicate the necessity of the contextfree stage in our pipeline</example>
		<phraseLemma>compare np to np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; unique to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Expanding to additional corpora will probably require development of additional features to capture signals &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;unique to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each corpus</example>
		<phraseLemma>np unique to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is automatically extracted from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is automatically extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; event logs where users reissue their search queries with potentially corrected spelling within the same session</example>
		<phraseLemma>np be automatically extract from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Spelling correction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not a trivial task as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; search queries are often short and lack context</example>
		<phraseLemma>np be not np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as soon as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The manual ﬁltering heuristic calculates a sequence of features for each search query pair and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as soon as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a feature ﬁres the entry is removed</example>
		<phraseLemma>np as soon as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; add &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We experiment with &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;additional phrase features as part of the phrase table and add 1 features based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrase pairspeciﬁc edit operations ie the number of insertions deletions substitutions transpositions and ﬁnal overall edit distance which helps to increase precision</example>
		<phraseLemma>np add np base on np</phraseLemma>
	</can>
	<can>
		<phrase>We calculate &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>As the judges worked on collapsed outputs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we calculate agreement scores for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unexpanded pairs otherwise the high overlap would unfairly increase agreement</example>
		<phraseLemma>np we calculate np for np</phraseLemma>
	</can>
	<can>
		<phrase>However many of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However many of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these deeper models have relied on minibatch training on largescale labeled datasets either using unsupervised pretraining or improved architectural components</example>
		<phraseLemma>however many of np</phraseLemma>
	</can>
	<can>
		<phrase>Consider the case of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Consider the case of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a child learning to discriminate between object categories and mapping them to words given only a small amount of explicitly labeled data and a large portion of unsupervised learning where the child comprehends an adults speech or experiences positive feedback for his or her own utterances regardless of their correctness</example>
		<phraseLemma>consider the case of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can consist of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>A full topdown phase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can consist of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; up to two calls to the ensemble backpropagation procedure</example>
		<phraseLemma>np can consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; similar in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Since we found this simple pseudolabeling approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;similar in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; spirit to to improve the results for all classiﬁers and thus we report all results utilizing this scheme</example>
		<phraseLemma>np similar in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; greater than &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For a dataset like the 1 NewsGroup which contained a number of unlabeled samples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;greater than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training iterations we view our schema as simulating access to a data stream since all models had access to any given unlabeled example only once during a training run</example>
		<phraseLemma>np greater than np</phraseLemma>
	</can>
	<can>
		<phrase>To the best of our knowledge this work is &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To the best of our knowledge this work is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst successful application of word embedding techniques for the task of click prediction in sponsored search</example>
		<phraseLemma>to the best of we knowledge this work be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; rely on &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>Such features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;rely on the assumption that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; queryad overlap is correlated with perceived relevance</example>
		<phraseLemma>np rely on np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tries to identify &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In the BET context BREDS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tries to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a relational pattern based on a shallow heuristic originally proposed in ReVerb</example>
		<phraseLemma>np try to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; shown in Table 1</phrase>
		<frequency>6</frequency>
		<example>We compared BREDS against Snowball &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in 1 relationship types shown in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np show in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learns &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For instance for the founderof relationship BREDS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learns patterns based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words such as founder cofounder cofounders or founded while Snowball only learns patterns that have the word founder like CEO and founder or founder and chairman</example>
		<phraseLemma>np learn np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; storing them in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Extracting relational facts between entities and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;storing them in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; knowledge bases has been a topic of active research in recent years</example>
		<phraseLemma>np store they in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to extract &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>One possible approach to updating KBs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to extract facts from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dynamic Web content such as news</example>
		<phraseLemma>np be to extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as we did for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The features are deﬁned and extracted in the same way &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as we did for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; DIEL and 1 binary classiﬁers are trained with the same method</example>
		<phraseLemma>np as we do for np</phraseLemma>
	</can>
	<can>
		<phrase>We create &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For our proposed NS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we create a negative example from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each nonother instance in the training set 1 in total</example>
		<phraseLemma>np we create np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improving &lt;NP&gt; by 1</phrase>
		<frequency>6</frequency>
		<example>However with similar amount of negative examples treating the reversed dependency paths from objects to subjects as negative examples can achieve a better performance 1 F 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improving random samples by 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np improve np by 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can learn from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This again proves that dependency paths provide useful clues to reveal the assignments of subjects and objects and a model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can learn from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such reversed paths as negative examples to make correct assignments</example>
		<phraseLemma>np can learn from np</phraseLemma>
	</can>
	<can>
		<phrase>As is the case for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As is the case for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other languages social media informality introduces numerous problems for NLP systems such as spelling errors novel words and ungrammatical constructions</example>
		<phraseLemma>as be the case for np</phraseLemma>
	</can>
	<can>
		<phrase>This is the case for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is the case for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; new languages and domains the task we face in this paper</example>
		<phraseLemma>this be the case for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which computes &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>One relevant technique to achieve our goal is selectional preference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which computes the most appropriate types for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a speciﬁc argument of a predicate</example>
		<phraseLemma>np which compute np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are linked to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Relation arguments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are linked to entities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the knowledge base by fuzzy string matching</example>
		<phraseLemma>np be link to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in a group</phrase>
		<frequency>6</frequency>
		<example>Combining all tuples in one group we deﬁne the support of a type pair tp &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a group&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in a group</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; could be used in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>As realtime processing on a large scale gains more attention we investigate features that are both effective and efﬁcient and so &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;could be used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a scalable online novelty scoring engine for making personalized newsfeeds on large web properties like Google News and Yahoo News</example>
		<phraseLemma>np could be use in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; included &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In a followup work Zhou included a longshort term memory in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their convolution neural network to learn the classiﬁcation sequence for the thread</example>
		<phraseLemma>in np include np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; whether it is &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The task asks participants to determine for each answer in the thread &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;whether it is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Good Bad or Potentially useful for the given question</example>
		<phraseLemma>np whether it be np</phraseLemma>
	</can>
	<can>
		<phrase>Our approach is inspired by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our approach is inspired by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Pang and Lee where they model the proximity relation between sentences for ﬁnding subjective sentences in product reviews whereas we are interested in global inference based on local classiﬁers</example>
		<phraseLemma>we approach be inspire by np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we explored &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For the SamevsDifferent problem we explored&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a variant of training with three classes by splitting the Same class into SameGood and SameBad</example>
		<phraseLemma>for np we explore np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as it has &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Graphcut works better than ILP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as it has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; higher precision which helps and accuracy</example>
		<phraseLemma>np as it have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are longer than &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>However typical EHR notes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are longer than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the passages and verbose queries in these systems which makes the graphical model and other learning based models less efﬁcient</example>
		<phraseLemma>np be longer than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; which makes &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>However typical EHR notes are longer than the passages and verbose queries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in these systems which makes&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the graphical model and other learning based models less efﬁcient</example>
		<phraseLemma>np in np which make np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was used to capture &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In our queries sequential dependence model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was used to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dependencies in a multiword query term</example>
		<phraseLemma>np be use to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to use &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our core idea &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to use another modality as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hub to indirectly learn the relevance between two different languages</example>
		<phraseLemma>np be to use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; extracted from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Generally spoken these studies ﬁnd signiﬁcant correlations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between sentiment extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; corporate disclosures and future volatilities</example>
		<phraseLemma>np between np extract from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; achieves &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Feature selection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on document frequency and information gain achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; better results than the ﬁrst one but only when the classiﬁers are trained with the CEO letter collection</example>
		<phraseLemma>np base on np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be modeled in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This paper considers the question as to whether the overall argumentation of web reviews &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be modeled in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a general way in order to increase domain independence in sentiment analysis</example>
		<phraseLemma>np can be model in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which apply to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>There are two sub tasks in targeted sentiment analysis namely entity recognition and sentiment classiﬁcation for each entity mention &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which apply to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both scenarios above</example>
		<phraseLemma>np which apply to np</phraseLemma>
	</can>
	<can>
		<phrase>We denote &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We denote CFOs and opinion words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bold and italic faces respectively</example>
		<phraseLemma>we denote np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be obtained by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>However this advantage is diluted in recommending review text from which effective features for user modeling such as UCFOs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be obtained by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; opinion mining</example>
		<phraseLemma>np can be obtain by np</phraseLemma>
	</can>
	<can>
		<phrase>We model &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We model the extraction for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CFOs as the BIO chunking which labels each token in a sentence as being the beginning inside or outside of a span of interest</example>
		<phraseLemma>we model np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are produced for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Feature functions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are produced for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any possible combinations of the values for the variables used and take 1 if the corresponding combination appears and 1 otherwise</example>
		<phraseLemma>np be produce for np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; of Figure 1 &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the upper part of Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a rectangle and an arrow denote a bunsetstu phrase and a syntactic dependency between two phrases respectively and in each phrase we show Japanese words based on the Hepburn system and their English translations in parentheses</example>
		<phraseLemma>in np of figure 1 np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can see that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>Looking at Figure 1 one &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our complete method outperformed any variation of our method in terms of Fmeasure</example>
		<phraseLemma>np can see that np</phraseLemma>
	</can>
	<can>
		<phrase>We observed that while &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We observed that while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; premise sentences varied considerably in length hypothesis sentences tended to be as 1 We additionally include about 1 k sentence pairs from a pilot study in which the premise sentences were instead drawn from the VisualGenome corpus</example>
		<phraseLemma>we observe that while np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which the &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We observed that while premise sentences varied considerably in length hypothesis sentences tended to be as 1 We additionally include about 1 k sentence pairs from a pilot study &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which the&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; premise sentences were instead drawn from the VisualGenome corpus</example>
		<phraseLemma>np in which the np</phraseLemma>
	</can>
	<can>
		<phrase>We ask &lt;NP&gt; to provide &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Given a sentence and target word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we ask annotators to provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; as many questionanswer pairs as possible where the question comes from a templated space of whquestions and the answer is a phrase from the original sentence</example>
		<phraseLemma>np we ask np to provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by using &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Universal Cognitive Conceptual Annotation is an attempt to create a linguistically universal annotation scheme &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by using general labels such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; argument or scene</example>
		<phraseLemma>np by use np such as np</phraseLemma>
	</can>
	<can>
		<phrase>We annotate &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We annotate verbs with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pairs of questions and answers that provide information about predicateargument structure</example>
		<phraseLemma>we annotate np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; identifies &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In addition CEA uses the reference documents as prior knowledge to model the topiclevel biography of an entity and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;identifies the truly relevant documents from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the candidates based on biographydocument relevance</example>
		<phraseLemma>np identify np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; assigns &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For the ones not included in VR and VC CTM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;assigns a weight zero in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic model no matter what GibbsLDA does</example>
		<phraseLemma>np assign np in np</phraseLemma>
	</can>
	<can>
		<phrase>To reduce &lt;NP&gt; caused by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To reduce EM errors caused by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deceptive names we use name tagging to distinguish deceptive names and true names</example>
		<phraseLemma>to reduce np cause by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was obtained by &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>KBP 1 includes target entities and groundtruth fillers and provenances where the groundtruth data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was obtained by manual verification&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; and annotation on the pool of system outputs</example>
		<phraseLemma>np be obtain by np on np</phraseLemma>
	</can>
	<can>
		<phrase>Our experiments demonstrated that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our experiments demonstrated that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stochastic ListNet method indeed leads to better ranking performance and speeds up the model training remarkably</example>
		<phraseLemma>we experiment demonstrate that np</phraseLemma>
	</can>
	<can>
		<phrase>Recent studies show that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Recent studies show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; listwise learning delivers better performance in general than traditional pairwise learning partly attributed to its capability of learning humanlabelled scores as a full rank list</example>
		<phraseLemma>recent study show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; until &lt;NP&gt; is reach</phrase>
		<frequency>6</frequency>
		<example>The training runs several iterations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;until the convergence criterion is reach&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np until np be reach</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; falls back to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In the case that the number of samples is very large for example for Top the stochastic ListNet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;falls back to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the conventional ListNet and their performance becomes similar</example>
		<phraseLemma>np fall back to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; primarily relies on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>ERMLN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;primarily relies on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the predicates for inference</example>
		<phraseLemma>np primarily rely on np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 compares &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 compares Praline to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline wordbased method on two question sets</example>
		<phraseLemma>table 1 compare np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is O for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For each iteration in optimization the complexity of TransE is O and the complexity of PTransE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is O for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ADD and MUL and O for RNN</example>
		<phraseLemma>np be o for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; which has &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We conduct the evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on FB 1 K which has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 relational triples and 1 1 relation types among which there are rich inference and reasoning patterns</example>
		<phraseLemma>np on np which have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is also useful for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This indicates that encoding relation paths &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is also useful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relation extraction from text</example>
		<phraseLemma>np be also useful for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; focus on &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We note that these methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;focus on modeling relation paths for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relation extraction without considering any information of entities</example>
		<phraseLemma>np focus on np for np</phraseLemma>
	</can>
	<can>
		<phrase>We show that &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show that context and global models for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entity typing provide complementary infor mation and combining them gives the best results</example>
		<phraseLemma>we show that np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; not covered by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The trained models can be applied to large corpora and the resulting scores can be used for learning new types of entities covered in the KB as well as for typing new or unknown entities – ie entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;not covered by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the KB</example>
		<phraseLemma>np not cover by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set that contains &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Thus CM is trained on a noisy training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set that contains&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only a relatively small number of informative contexts</example>
		<phraseLemma>np set that contain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; provided by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We consider all entities in the corpus whose notable types can be mapped to one of the FIGER types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the mapping provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; FIGER</example>
		<phraseLemma>np base on np provide by np</phraseLemma>
	</can>
	<can>
		<phrase>The number of &lt;NP&gt; used in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The number of hidden units used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each model is also reported</example>
		<phraseLemma>the number of np use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can leverage &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The table shows that CM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can leverage&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; larger context sizes well</example>
		<phraseLemma>np can leverage np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are linked to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the disambiguation phase all KBi ∈ KU are linked to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the uniﬁed sense inventory S and added to the set of redeﬁned KBs KS</example>
		<phraseLemma>in np be link to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; comprising &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Finally we evaluated the quality of disambiguation on a publicly available dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;comprising manual annotations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NELL</example>
		<phraseLemma>np comprise np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is higher in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Disagreement between human choice and ranking &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is higher in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NELL and in PATTY</example>
		<phraseLemma>np be higher in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which case &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Most spoken language processing or dialogue systems are based on a ﬁnite vocabulary so occasionally a word used will be out of the vocabulary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which case&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the automatic speech recognition system chooses the best matching invocabulary sequence of words to cover that region</example>
		<phraseLemma>np in which case np</phraseLemma>
	</can>
	<can>
		<phrase>In addition in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an open domain system automaticallylearned lexical context features from one domain may be useless in another</example>
		<phraseLemma>in addition in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is in &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The main failing of the BOW ME model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is in recall on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the General domain though it also has relatively low recall on the HADR domain</example>
		<phraseLemma>np be in np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is very useful in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The ability to learn by assessing only the ﬁnal prediction and not the intermediate steps &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is very useful in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the face of missing labels such as in the case of the labels for the NEC stage</example>
		<phraseLemma>np be very useful in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has to be &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To understand why we can do this recall from Section 1 that in QSLINKs and OLINKs the trigger &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a spatial signal element having a semantic type attribute</example>
		<phraseLemma>np have to be np</phraseLemma>
	</can>
	<can>
		<phrase>As an example for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As an example for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the QSLINK and OLINK sentence in Table 1 exactly one positive instance LINK will be created</example>
		<phraseLemma>as a example for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; concerning &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This problem could be alleviated by exploiting geographical knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;concerning these cities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; external knowledge sources such as Wikipedia</example>
		<phraseLemma>np concern np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; remains &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Despite the seeming simplicity of the question precisely identifying which characters appear in a story &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;remains an open question in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; literary and narrative analysis</example>
		<phraseLemma>np remain np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each associated with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Thus the remaining set of nodes are merged to create sets of names &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a different character</example>
		<phraseLemma>np each associate with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; adding &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Similarly Gärtner developed graph kernels based on random walks and Srivastava used them on dependency trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with Vector Tree Kernels adding&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; node similarity based on word embeddings from SENNA and reporting improvements over SSTK</example>
		<phraseLemma>np with np add np</phraseLemma>
	</can>
	<can>
		<phrase>To train and test &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To train and test&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SVM classiﬁer we used the LibSVM library and employed the onevsone strategy for multiclass tasks</example>
		<phraseLemma>to train and test np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Figure 1 shows the accuracy on the same test set 1 of the dataset when the learning was done on 1 to 1 of the training set 1 of the dataset for the bigram baseline and our bigram PK phrase kernel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependency tree representation on PL 1</example>
		<phraseLemma>np both with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are written with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Although cooking instructions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are written with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an implied temporal order they are not linked in a linear chain</example>
		<phraseLemma>np be write with np</phraseLemma>
	</can>
	<can>
		<phrase>We proposed &lt;NP&gt; to capture &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We proposed a new ingredientinstruction dependency tree representation to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the internal structure of cooking recipes and built a parser for it</example>
		<phraseLemma>we propose np to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be applied in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM signiﬁcantly outperforms stateoftheart DPMM model and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be applied in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a lifelong learning framework</example>
		<phraseLemma>np can be apply in np</phraseLemma>
	</can>
	<can>
		<phrase>We can use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can use different values of α for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior topics with difk ferent conﬁdence levels</example>
		<phraseLemma>we can use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been increasingly focused on for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Recently neural network based sentence modeling approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been increasingly focused on for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their ability to minimize the efforts in feature engineering such as Neural BagofWords Recurrent Neural Network Recursive Neural Network and Convolutional Neural Network</example>
		<phraseLemma>np have be increasingly focus on for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even without &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Generally our model is better than the previous recursive neural network based models which indicates our model can better model the combinations of features with the FBT and our gating mechanism &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even without&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an external syntactic tree</example>
		<phraseLemma>np even without np</phraseLemma>
	</can>
	<can>
		<phrase>In future work we would like to investigate &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In future work we would like to investigate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the other gating mechanisms for better modeling the feature combinations</example>
		<phraseLemma>in future work we would like to investigate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; be &lt;NP&gt; of score</phrase>
		<frequency>6</frequency>
		<example>Let the probability p &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;be a monotonically nondecreasing function of score&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be np of score</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; provides &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the table PAVEM provides higher recall and for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; top 1 and top 1 MeSH terms</example>
		<phraseLemma>in np provide np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; propose &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>We model this as a sequence classiﬁcation problem and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;propose a supervised setting in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data are acquired automatically</example>
		<phraseLemma>np propose np in which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tend to follow &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The problem is modeled as a classiﬁcation task using Conditional Random Fields which are particularly suitable for our study because the biographical sections &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tend to follow&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a chronological order and present typical sequential patterns</example>
		<phraseLemma>np tend to follow np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed at &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>While a simple tokenbased baseline is very difﬁcult to beat when the task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; section level our method performs best when the evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; page level recognizing all sections that describe a persons life</example>
		<phraseLemma>np be perform at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by looking for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Other works focused on the analysis of typical events in selected articles from Wikipedia biographies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by looking for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a particular list of predeﬁned events</example>
		<phraseLemma>np by look for np</phraseLemma>
	</can>
	<can>
		<phrase>To assess the performance of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To assess the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our system we compare it with a baseline approach considering only the most frequent words in section titles</example>
		<phraseLemma>to assess the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relying on &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The results in Table 1 show also that a simple baseline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relying on the 1 most frequent tokens in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; section titles achieves surprisingly good results especially with the intersectionbased metrics</example>
		<phraseLemma>np rely on np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; to replace &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our future work will focus on studying the performance of applying nonlinear kernel function to the QP problem and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the word embedding vector to replace&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; current lexicalized features</example>
		<phraseLemma>np use np to replace np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that map to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We do this by analyzing Urdu alphabets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that map to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single roman form eg words samar sabar and saib all start with different Urdu alphabets that have identical roman representation</example>
		<phraseLemma>np that map to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is comparable to that of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>UrduPhone outperforms Soundex Caverphone and Metaphone while Nysiiss fmeasure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is comparable to that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; UrduPhone</example>
		<phraseLemma>np be comparable to that of np</phraseLemma>
	</can>
	<can>
		<phrase>We can see that both &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can see that both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CBOW and CBOWbi perform worse than the corresponding SkipGram and SkipGrambi</example>
		<phraseLemma>we can see that both np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; 1 over &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We observe C 1 EL to decisively outperform both the existing methods providing a F 1 improvement of around 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over CROCS and 1 over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; EECR</example>
		<phraseLemma>np over np 1 over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; explicitly in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our model can be extended to additionally capture mention heads &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;explicitly in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a joint manner under the same time complexity</example>
		<phraseLemma>np explicitly in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; largely due to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The task of mention detection and tracking has received substantial attention &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;largely due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its important role in conducting several downstream tasks such as relation extraction entity linking and coreference resolution</example>
		<phraseLemma>np largely due to np</phraseLemma>
	</can>
	<can>
		<phrase>We can see that for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can see that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a given sentence consisting of n words there are altogether tn/ 1 possible different mention candidates where t is the total number of possible mention types</example>
		<phraseLemma>we can see that for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have also been used in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Hypergraphs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have also been used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other ﬁelds such as syntactic parsing semantic parsing and machine translation</example>
		<phraseLemma>np have also be use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is given as follows</phrase>
		<frequency>6</frequency>
		<example>Speciﬁcally for a given input sentence x the probability of predicting a possible output y &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is given as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be give as follow</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to encode &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The technique is not directly applicable to our task where a hypergraph representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to encode&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; overlapping mentions</example>
		<phraseLemma>np be use to encode np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; fail to produce &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Implicit extractors are only used when more explicit extractors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;fail to produce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a good type</example>
		<phraseLemma>np fail to produce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is taken by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>An alternative approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is taken by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervised methods which train classiﬁers based on linguistic features</example>
		<phraseLemma>np be take by np</phraseLemma>
	</can>
	<can>
		<phrase>To the best of &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To the best of our knowledge JERL is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst model to jointly optimize NER and linking tasks together completely</example>
		<phraseLemma>to the best of np be np</phraseLemma>
	</can>
	<can>
		<phrase>The disadvantage of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The disadvantage of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the decoupled approach is that each lower level task is not aware of other tasks and thus not able to leverage information provided by others to improve performance</example>
		<phraseLemma>the disadvantage of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is also evaluated on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our baseline model JERLel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is also evaluated on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Stanford NER generated mentions which has comparable performance with Kul and Hof</example>
		<phraseLemma>np be also evaluate on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only focus on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Conventional language models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the word sequence within a sentence</example>
		<phraseLemma>np only focus on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we will explore &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the future we will explore&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; better sentence representation such as distributed sentence representation as input for our sentencelevel language model to better model document coherence</example>
		<phraseLemma>in np we will explore np</phraseLemma>
	</can>
	<can>
		<phrase>After &lt;NP&gt; there were &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;After standard cleaning and tokenization there were&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 k parallel sentences in the newscommentary dataset and 1 sentences each for the tuning and test sets</example>
		<phraseLemma>after np there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is also &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Regularization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in neural networks is also&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an old idea for example Nowland and Hinton mention both 1 and 1 1 regularization</example>
		<phraseLemma>np in np be also np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to determine &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>This reduces the need to conduct expensive multidimensional grid searches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to determine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; optimal sizes</example>
		<phraseLemma>np in order to determine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; obtained from &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Sontag proposed a sophisticated block method called MPLP that considers all values of variable Xi instead of the ones &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;obtained from the best assignments for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the subproblems</example>
		<phraseLemma>np obtain from np for np</phraseLemma>
	</can>
	<can>
		<phrase>We have to deal with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We have to deal with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many sentence fragments nonstandard orthography and sometimes lack of syntax</example>
		<phraseLemma>we have to deal with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; made &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Because annotating full discourse structures is a very complex task experts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;made several passes over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the annotations from the naive annotators improving the data and debugging it</example>
		<phraseLemma>np make np over np</phraseLemma>
	</can>
	<can>
		<phrase>As for &lt;NP&gt; use &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As for documentlevel discourse parsers Subba and Di Eugenio use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a transitionbased approach following the paradigm of Sagae</example>
		<phraseLemma>as for np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by combining &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Our new model jointly predicts different aspects of the structure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by combining the different subtask predictions in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the edge weights of an evidence graph we then apply a standard MST decoding algorithm</example>
		<phraseLemma>np by combine np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; addressed in this paper</phrase>
		<frequency>6</frequency>
		<example>In our discussion of related work we focus on the three subtasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;addressed in this paper&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np address in this paper</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; achieving &lt;NP&gt; of 1</phrase>
		<frequency>6</frequency>
		<example>Palau and Moens used a handwritten contextfree grammar to predict argumentation trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on legal documents achieving an accuracy of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np achieve np of 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is deﬁned as &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The argumentation structure of a text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is deﬁned as a graph with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the text segments as nodes</example>
		<phraseLemma>np be deﬁned as np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; predict &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>As before we ﬁrst tune the hyperparameters in the inner CV train the model on the whole training data and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;predict probabilities on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all items of the test set</example>
		<phraseLemma>np predict np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; in English</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Table 1 we give a summary of how often tree constraints are fulﬁlled showing that without decoding valid trees can only be predicted for 1 of the texts in German and for 1 of the texts in English&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np in english</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; except for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The pure labeled mstparser model performs worse than the base classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on all levels except for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the attachment task</example>
		<phraseLemma>np on np except for np</phraseLemma>
	</can>
	<can>
		<phrase>We consider &lt;NP&gt; to be &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To deal with singleletter spelling errors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we consider T and T to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an exact match if exactly one of the two is correctly spelled and their Levenshtein distance is 1</example>
		<phraseLemma>np we consider np to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can capture &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The two representations are complementary – the entitybased representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can capture equivalences between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mentions of different lengths of a named entity while the wordbased representation allows the use of similarity resources for named entity words</example>
		<phraseLemma>np can capture np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is computed between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For each individual test set the Pearson productmoment correlation coefﬁcient &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is computed between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; system scores and human annotations</example>
		<phraseLemma>np be compute between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both within and across &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Most existing SRL systems model each semantic role as an atomic unit of meaning ignoring ﬁnergrained semantic similarity between roles that can be leveraged to share context between similar labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both within and across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; annotation conventions</example>
		<phraseLemma>np both within and across np</phraseLemma>
	</can>
	<can>
		<phrase>We found that using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We found that using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a straightforward productofexperts model at inference time reduces this variance and results in signiﬁcantly higher performance</example>
		<phraseLemma>we find that use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; extracted from &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The input to our system consists of 1 relational phrases and the associated argument types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;extracted from the Englishlanguage Wikipedia website using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the PATTY system</example>
		<phraseLemma>np extract from np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; above 1</phrase>
		<frequency>6</frequency>
		<example>Ultimately RELLY produced 1 hypernymy links between relational phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with conﬁdence scores above 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np above 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improve performance on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The ultimate goal of producing a highquality hypernymy graph is to deepen our understanding of natural language and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improve performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the many NLP applications</example>
		<phraseLemma>np improve performance on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being introduced into &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>A connection identiﬁes the origin of a given string span as either the output of a previous action or as a new ingredient or entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being introduced into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the recipe</example>
		<phraseLemma>np be introduce into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; then &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>If a span introduces raw ingredient or new location &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into the recipe then&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; o = 1 in Fig 1 this occurs for each of the spans that represent raw ingredients as well as oven” and into loaf pan”</example>
		<phraseLemma>np into np then np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; rank &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;rank the candidate relations based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the captured information and integrate the highly ranked relations into a taxonomic structure</example>
		<phraseLemma>np rank np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the importance of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Using this score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the importance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a page is calculated as follows</example>
		<phraseLemma>np the importance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; described in Sections &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We evaluate the individual methods for trustiness measurement and synonymy identiﬁcation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;described in Sections&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 and 1</example>
		<phraseLemma>np describe in section np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which refers to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Also a constituent in a clue &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which refers to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an entity in a particular problem may not refer to an entity in another problem</example>
		<phraseLemma>np which refer to np</phraseLemma>
	</can>
	<can>
		<phrase>There has been &lt;NP&gt; of work on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There has been a signiﬁcant amount of work on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the representation of puzzle problems in a formal language</example>
		<phraseLemma>there have be np of work on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is largely due to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The instability &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is largely due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the alignment of function words which affects translation performance</example>
		<phraseLemma>np be largely due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are found by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The weights for all old or new models in the loglinear combination &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are found by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tuning on a development set for each PPE iteration</example>
		<phraseLemma>np be find by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being &lt;NP&gt; introduced in</phrase>
		<frequency>6</frequency>
		<example>Since this objective function requires to normalize scores several alternative training objectives have recently been proposed to speed up training and inference a popular and effective choice &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being the Noise Contrastive Estimation introduced in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be np introduce in</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to help &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In SMT the primary role of CTMs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to help&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations where quality is measured using automatic metrics such as BLEU</example>
		<phraseLemma>np be to help np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depends heavily on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Moreover our training procedure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depends heavily on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the loglinear coefﬁcients λ</example>
		<phraseLemma>np depend heavily on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are initialized using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The weights of this replicated feature space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are initialized using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the weights w tuned for the baseline φ</example>
		<phraseLemma>np be initialize use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that includes &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>A derivation r of sentence fi has features that are computed from the combination of the baseline training corpus and a genrespeciﬁc corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that includes all sentence pairs from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tuning corpus as well as from the adaptation corpus with j &amp;lt i sharing fis j genre</example>
		<phraseLemma>np that include np from np</phraseLemma>
	</can>
	<can>
		<phrase>As we do not have &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we do not have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; access to any dataset which provides scores to segments on the basis of translation quality we used the WMT 1 ranks corpus to automatically derive training data</example>
		<phraseLemma>as we do not have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; calculates &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In Algorithm 1 the kendall function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;calculates Kendall tau correlation using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WMT 1 human judgements</example>
		<phraseLemma>np calculate np use np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; acts as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In other words the corpus acts as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a scoring function for the available reference translation pairs which gives a similarity score between a reference and a translation</example>
		<phraseLemma>in np act as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Table 1 shows systemlevel Pearson correlation obtained on different language pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as average Pearson correlation over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all language pairs</example>
		<phraseLemma>np as well as np over np</phraseLemma>
	</can>
	<can>
		<phrase>They reported &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They reported positive results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; simple features larger feature sets did not improve these results</example>
		<phraseLemma>they report np on np</phraseLemma>
	</can>
	<can>
		<phrase>At this point &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At this point&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a state from the search graph is selected the search graph leading from this state is kept and the remainder discarded</example>
		<phraseLemma>at this point np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that have &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We deﬁne a discriminative rule selection model for systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that have syntactic annotation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on the target language side</example>
		<phraseLemma>np that have np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are due to the fact that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>An analysis of rule diversity and an empirical comparison with hierarchical rule selection indicate that the low improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are due to the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ambiguity between stringtotree rules is too small to be improved with a rule selection model</example>
		<phraseLemma>np be due to the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>The hypothesis of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The hypothesis of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our broader vision is that personalized MT or authoraware translation is an important necessity</example>
		<phraseLemma>the hypothesis of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being calculated via &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The labels are provided by the author with scores on ﬁve traits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being calculated via&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; selfassessment responses to the short Big 1 test BFI 1 then normalized between 1 and 1</example>
		<phraseLemma>np be calculate via np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to search for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used the Google YouTube Analytics API 1 to search for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; videos of talks in French</example>
		<phraseLemma>we use np to search for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For the majority of the traits the native results outperform &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both translation settings in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some cases by considerable margin</example>
		<phraseLemma>np both np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; learning approach</phrase>
		<frequency>6</frequency>
		<example>This work explores deception gender and age detection in short texts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using a machine learning approach&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use np learn approach</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that relate to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We further explore the linguistic differences in deceptive content &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that relate to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deceivers gender and age and ﬁnd evidence that both age and gender play an important role in peoples word choices when fabricating lies</example>
		<phraseLemma>np that relate to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are aware of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>English speakers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are aware of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this constraint and judge forms that start with a as impossible English words</example>
		<phraseLemma>np be aware of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>If K is the number of templates that have been posited so far and nK indicate the number of sounds that have been drawn from each template then the probability distribution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over the template zn that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sound sn will be drawn from is given by Since the probability that an existing template generated sn is proportional to the number of segments currently assigned to that template this prior encourages partitions in which a few templates explain most of the sounds which amounts to a parsimony bias</example>
		<phraseLemma>np over np that np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are derived from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In most cases sexpressions are derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; verb function nodes and modifier function nodes while nexpressions are generated from constants and noun function nodes</example>
		<phraseLemma>in np be derive from np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are called &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper equations and inequations are called&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sexpressions because they represent mathematical sentences The semantic interpretation of DOL nodes plays a critical role in the algorithm</example>
		<phraseLemma>in np be call np</phraseLemma>
	</can>
	<can>
		<phrase>Problems on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Problems on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both sites are organized into categories</example>
		<phraseLemma>problem on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>These labeled edges have informative content and we would like to use the alignment procedure of Pourdamghani which aligns words to edges &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; terminal nodes</example>
		<phraseLemma>np as well as to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is highly dependent on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>However whether or not a role is ﬁlled by a string or an instance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is highly dependent on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the kind of role being ﬁlled</example>
		<phraseLemma>np be highly dependent on np</phraseLemma>
	</can>
	<can>
		<phrase>This is &lt;NP&gt; behind &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is the motivation behind&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the following model of an AMR 1 We deﬁne an AMR instance i = where c is a concept and R is a set of roles</example>
		<phraseLemma>this be np behind np</phraseLemma>
	</can>
	<can>
		<phrase>We need to compute &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For many natural language processing tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we need to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; meaning representations for sentences from meaning representations of words</example>
		<phraseLemma>np we need to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which consists of &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To illustrate how a CNN works the following example uses a simpliﬁed model proposed by Collobert &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which consists of one convolutional layer with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the max pooling operation followed by one fully connected layer</example>
		<phraseLemma>np which consist of np with np</phraseLemma>
	</can>
	<can>
		<phrase>This is &lt;NP&gt; because &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is a promising result because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our network used only parse forests unsupervisedly pretrained word embeddings whereas SVMS used heavily engineered resources</example>
		<phraseLemma>this be np because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; according to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The idea that a composition function must be able to change its behaviour &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the ﬂy according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input vectors is explored by Socher Le and Zuidema among others</example>
		<phraseLemma>np on np accord to np</phraseLemma>
	</can>
	<can>
		<phrase>We are able to learn &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>By explicitly modeling both the lowlevel compositional structure of individual actions and the highlevel structure of full plans &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we are able to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both grounded representations of sentence meaning and pragmatic constraints on interpretation</example>
		<phraseLemma>np we be able to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; returns &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Indeed if L is the set of all atomic entities and relations fV &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;returns a unique label for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; every v ∈ V and fE always returns a vector with one active feature we recover the existentiallyquantiﬁed portion of ﬁrst order logic exactly and in this form can implement large parts of classical neoDavidsonian semantics using grounding graphs</example>
		<phraseLemma>np return np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be computed using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>While in principle the normalizing constant &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be computed using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the forward algorithm in practice the state spaces under consideration are so large that even this is intractable</example>
		<phraseLemma>np can be compute use np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁnd that in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ﬁnd that in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English bigram alignment models do perform better than unigram alignment models on the G 1 P task</example>
		<phraseLemma>we ﬁnd that in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; obtained from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Overall we ran experiments in which we trained DirecTL or Phonetisaurus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with alignments of speciﬁc quali ties obtained from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; particularly parametrized aligners</example>
		<phraseLemma>np with np obtain from np</phraseLemma>
	</can>
	<can>
		<phrase>However we note that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However we note that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the surplus over the unsupervised alignments decreases as training set size increases</example>
		<phraseLemma>however we note that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as 1 &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Thus given a phoneme sequence yl = y y &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input the goal of our IM engine is to output a word sequence wm that maximizes the probabil ity P as follows</example>
		<phraseLemma>np as 1 np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is retrieved by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For a character c 1 C the corresponding character embedding vc 1 Rd &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is retrieved by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the lookup table layer</example>
		<phraseLemma>np be retrieve by np</phraseLemma>
	</can>
	<can>
		<phrase>We use Y to denote &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use Y to denote&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set of all possible tag sequences for a given sentence xi and the correct tag sequence for xi is yi</example>
		<phraseLemma>we use y to denote np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; derived from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>introduced a graphbased semisupervised joint model of Chinese word segmentation and partofspeech tagging and regularized the learning of a linear CRF model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the label distributions derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unlabeled data</example>
		<phraseLemma>np base on np derive from np</phraseLemma>
	</can>
	<can>
		<phrase>However &lt;NP&gt; focus on &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However because most of these approaches focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SMT performance they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy</example>
		<phraseLemma>however np focus on np</phraseLemma>
	</can>
	<can>
		<phrase>To guarantee that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To guarantee that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the semantic meanings of the Chinese segmentation match those of the corresponding English sentences as closely as possible we propose to use a feature based on the EnglishChinese semantic gap to ensure the retention of semantic meaning during the segmentation process</example>
		<phraseLemma>to guarantee that np</phraseLemma>
	</can>
	<can>
		<phrase>We performed &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To ensure a fair comparison &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we performed the evaluation in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two steps</example>
		<phraseLemma>np we perform np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; without resorting to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The model can extract a compact rule and phrase table &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;without resorting to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any heuristics by hierarchically backing off to smaller phrases under SCFG</example>
		<phraseLemma>np without resort to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learned by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>A model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learned by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model</example>
		<phraseLemma>np be learn by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is deﬁned as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We use Hiero grammar an instance of an SCFG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is deﬁned as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a contextfree grammar for two languages</example>
		<phraseLemma>np which be deﬁned as np</phraseLemma>
	</can>
	<can>
		<phrase>In contrast we used &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast we used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a = 1 when performing slice sampling without the future score</example>
		<phraseLemma>in contrast we use np</phraseLemma>
	</can>
	<can>
		<phrase>In addition we present &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition we present&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; heuristic extraction from the last 1 sample of Backfuture in Exhaustive</example>
		<phraseLemma>in addition we present np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; correlate better with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We propose to use coverage which reﬂects how well extracted phrases can recover the training data to enable word alignment to model consistency and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;correlate better with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; machine translation</example>
		<phraseLemma>np correlate better with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; result in &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Ayan and Dorr ﬁnd that precisionoriented alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;result in better translation performance than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recalloriented alignments</example>
		<phraseLemma>np result in np than np</phraseLemma>
	</can>
	<can>
		<phrase>We introduce &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We introduce a new alignment search algorithm with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an objective that maximizes both alignment model score and coverage while keeping the training algorithm unchanged</example>
		<phraseLemma>we introduce np with np</phraseLemma>
	</can>
	<can>
		<phrase>Using &lt;NP&gt; shown in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Using the algorithm shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1 we iteratively derive evidence scores for candidate translations</example>
		<phraseLemma>use np show in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to minimize &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>More speciﬁcally the former is to encourage alignmentconsistent generation of substructures while the latter &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to minimize semantic distances between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bilingual subphrases</example>
		<phraseLemma>np be to minimize np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; behind this is that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>The assumption &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;behind this is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a source/target node should be able to reconstruct the entire subtree rooted at its target/source aligned node as they are semantically equivalent</example>
		<phraseLemma>np behind this be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; that contains &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Word embeddings in BRAE are pretrained with toolkit Word 1 Vec 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on largescale monolingual data that contains&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 B words for Chinese and 1 B words for English</example>
		<phraseLemma>np on np that contain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is represented as &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Every phrase pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is represented as a vector where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; every entry in the vector reﬂects its relatedness with each domain</example>
		<phraseLemma>np be represent as np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; followed by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In both cases we ﬁrst present the regularized loss function for the normalized output layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the standard softmax followed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corresponding unnormalized one using the noise contrastive estimation</example>
		<phraseLemma>np with np follow by np</phraseLemma>
	</can>
	<can>
		<phrase>Were obtained by concatenating &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Best results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were obtained by concatenating&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the data together</example>
		<phraseLemma>np be obtain by concatenate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are not &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We demonstrate that sentence length and punctuation usage &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Chinese are not&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sufﬁcient clues for accurately detecting heavy sentences and present a richer classiﬁcation model that accurately identiﬁes these sentences</example>
		<phraseLemma>np in np be not np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; there is &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>When &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;there is a fullstop comma in the sentence there is a higher chance that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence is contentheavy</example>
		<phraseLemma>np there be np that np</phraseLemma>
	</can>
	<can>
		<phrase>This can be seen as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This can be seen as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an approximation upper bound of the current model and decoder which we call guided oracle</example>
		<phraseLemma>this can be see as np</phraseLemma>
	</can>
	<can>
		<phrase>From the results we can see that &lt;CL&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From the results we can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hypergraphMERT is better than MERT by 1 BLEU points verifying the result of</example>
		<phraseLemma>from the result we can see that np</phraseLemma>
	</can>
	<can>
		<phrase>It outperforms &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It outperforms traditional MERT by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 BLEU points on average and better than hypergraphMERT by 1 BLEU points</example>
		<phraseLemma>it outperform np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was induced from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Especially Brown clusters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was induced from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than 1 million line documents making the setting unrealistic for resourcepoor language</example>
		<phraseLemma>np be induce from np</phraseLemma>
	</can>
	<can>
		<phrase>We reduce &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Second since our tokenized version of the English Gigaword corpus contains more than 1 million unique words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we reduce the vocabulary of the dictionary to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the approximately 1 million words having or more occurrences in the corpus</example>
		<phraseLemma>np we reduce np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; whether &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>State generally reﬂects the deﬁniteness &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in nominals and whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a nominal is the head of genitive construction</example>
		<phraseLemma>np in np whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared with using &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We ﬁnd that using a hidden layer speciﬁcally for embedding features gives better results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared with using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no hidden layers</example>
		<phraseLemma>np compare with use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be rewritten as</phrase>
		<frequency>6</frequency>
		<example>In this paper we adopt the secondorder sibling factorization in which each sibling part consists of a tuple of indices where and are a pair of adjacent edges to the same side of the head h By adding labele information to this factorization Score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be rewritten as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np can be rewrite as</phraseLemma>
	</can>
	<can>
		<phrase>Comparing with &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Comparing with the best systems from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CoNLL our parser achieves better performance on both UAS and LAS for 1 languages</example>
		<phraseLemma>compare with np from np</phraseLemma>
	</can>
	<can>
		<phrase>We set &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>By using the development set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we set the dimension of word embedding and the window size for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; cooccurrence counts as and respectively</example>
		<phraseLemma>np we set np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which contains &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We create a sentence treebank called Foreebank &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which contains sentences from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Symantec Norton English and French technical support forums</example>
		<phraseLemma>np which contain np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; involve &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>With the exception of HOO 1 all shared tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;involve errorannotated sentences from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; learner corpora</example>
		<phraseLemma>np involve np from np</phraseLemma>
	</can>
	<can>
		<phrase>To train with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To train with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predicted POS tags we use a CRFbased POS tagger to generate 1 fold jackknifed POS tags on the training set and predicted tags on the dev test and tune sets our tagger gets comparable accuracy to the Stanford POS tagger with 1 on the WSJ test set</example>
		<phraseLemma>to train with np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁnd that for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ﬁnd that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all settings the dense neural network model produces higher POS tagging and parsing accuracy gains than its sparse linear counterpart</example>
		<phraseLemma>we ﬁnd that for np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; obtained from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the same dataset obtained from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ConLL 1 shared task Scoring is performed using the Vmeasure which is used to predict syntactic classes at the word level</example>
		<phraseLemma>we use np obtain from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not perform as well as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We can see that in this task our models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not perform as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CBOW and Skipngram model which hints that our model is learning embeddings that learn more towards syntax</example>
		<phraseLemma>np do not perform as well as np</phraseLemma>
	</can>
	<can>
		<phrase>While most of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While most of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the learnt vectors are semantically oriented work has been done in order to extend the model to learn syntactically oriented embeddings</example>
		<phraseLemma>while most of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is placed at &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>No dummy root token is necessary removing the need to choose whether the to ken &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is placed at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the beginning or end of the buffer</example>
		<phraseLemma>np be place at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are adjacent to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Note that if the two words each seem like the governor of the sentence such that the parser deems all incoming arcs to these words unlikely the transition system is guaranteed to arrive at a conﬁguration where these two words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are adjacent to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each other</example>
		<phraseLemma>np be adjacent to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; described by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We extended the feature set to include Brown cluster features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the cluster preﬁx trick described by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Koo and Collins</example>
		<phraseLemma>np use np describe by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; obtained from &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The semantic representations are learned via a weighted mean value and a minimum distance method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using phrase vector representations obtained from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large scale monolingual corpus</example>
		<phraseLemma>np use np obtain from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; either use &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In order to address this issue researchers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;either use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntactic labels to annotate nonterminal Xs or employ syntactic information from parse trees to reﬁne nonterminals with realvalued vectors</example>
		<phraseLemma>np either use np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; in &lt;NP&gt; we compute &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each node in the generated binary tree we compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Euclidean distance between the original input vectors and the reconstructed vectors to measure the reconstruction error</example>
		<phraseLemma>for np in np we compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We used NIST MT 1 as our development &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set NIST MT 1 as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our development test set and MT 1 as our ﬁnal test set</example>
		<phraseLemma>np set np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was carried out in</phrase>
		<frequency>6</frequency>
		<example>Further recent research on applying NN models for extended context &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was carried out in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be carry out in</phraseLemma>
	</can>
	<can>
		<phrase>Were used for &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For the full data 1 grams &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were used for the IWSLT and WMT tasks and 1 grams for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BOLT</example>
		<phraseLemma>np be use for np for np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; generates &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In concrete details the model ﬁrst generates an aligned position pt for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each target word at time t</example>
		<phraseLemma>in np generate np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; observed &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>While visualized alignments for some sample sentences and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;observed gains in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation quality as an indication of a working attention model no work has assessed the alignments learned as a whole</example>
		<phraseLemma>np observe np in np</phraseLemma>
	</can>
	<can>
		<phrase>Since &lt;NP&gt; consists of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since a document consists of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a list of sentences and each sentence is made up of a list of words the approach models document representation in two stages</example>
		<phraseLemma>since np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produces &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Given the vectors of sentences of variable length as input document composition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produces a ﬁxedlength document vector as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; output</example>
		<phraseLemma>np produce np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; predicted &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For model training we use the crossentropy error between gold sentiment distribution P g and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;predicted sentiment distribution P as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the loss function</example>
		<phraseLemma>np predict np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; metric to measure &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>We use accuracy and M SE as evaluation metrics where accuracy is a standard &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;metric to measure&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the overall sentiment classiﬁcation performance</example>
		<phraseLemma>np metric to measure np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to measure &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use MSE to measure the divergences between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predicted sentiment la bels and ground truth sentiment labels because review labels reﬂect sentiment strengths</example>
		<phraseLemma>we use np to measure np between np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we develop &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we develop&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; neural models in a sequential way and encode sentence semantics and their relations automatically without using external discourse analysis results</example>
		<phraseLemma>in this work we develop np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; encode &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we develop neural models in a sequential way and encode&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence semantics and their relations automatically without using external discourse analysis results</example>
		<phraseLemma>np in np encode np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we present &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we present our experimental settings and results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of opinion target extraction from customer reviews</example>
		<phraseLemma>in this section we present np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>What is remarkable is that RNNs without any handcrafted features outperform featurerich CRF models by a good margin – absolute maximum gains of &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ElmanRNN and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BiLSTMRNN on Laptop</example>
		<phraseLemma>np 1 by np</phraseLemma>
	</can>
	<can>
		<phrase>This can be attributed to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This can be attributed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; RNNs ability to learn better features automatically and to capture longrange sequential dependencies between the output labels</example>
		<phraseLemma>this can be attribute to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as shown in Figure 1 a</phrase>
		<frequency>6</frequency>
		<example>The space of possible extended lexical entries for word xi is deﬁned by i which is expressed with a CFG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as shown in Figure 1 a&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as show in figure 1 a</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by aligning &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To improve efﬁciency we compute a number of thresholds &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by aligning gold CCGbank dependencies with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PropBank</example>
		<phraseLemma>np by align np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allows us to train &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Using latent syntax &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allows us to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model purely from semantic dependencies enabling future work to train against other annotations such as FrameNet Ontonotes or QASRL</example>
		<phraseLemma>np allow we to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which form the basis of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>However although recent studies show that semantic parsing with SCFGs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which form the basis of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; most existing statistical syntaxbased translation models achieves favorable results this approach is still behind the most recent stateoftheart</example>
		<phraseLemma>np which form the basis of np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows an example of &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows an example of converted meaning representation where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each token is in the format of A@B where A is the symbol while B is either s indicating that the symbol is a string or a number indicating the symbols arity</example>
		<phraseLemma>figure 1 show a example of np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allowing for &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The translation then is considered correct if and only if its MRL retrieves the same answers as the gold standard MRL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allowing for a fair comparison between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our systems and previous works</example>
		<phraseLemma>np allow for np between np</phraseLemma>
	</can>
	<can>
		<phrase>We report &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We report the SAT score as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the percentage of correctly answered questions penalized by the wrong answers</example>
		<phraseLemma>we report np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is expressed by &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>The difference is that relation extraction focuses on determining what relationship &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is expressed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a particular sentence while knowledge base completion tries to predict which relationships hold between which entities</example>
		<phraseLemma>np be express by np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to describe &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use onesided path to describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sequence of edges that starts at a source or target node in the data but does not necessarily terminate at a corresponding target or source node as PRA features do</example>
		<phraseLemma>we use np to describe np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are able to perform &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Models that learn to represent textual and knowledge base relations in the same continuous latent space &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are able to perform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion</example>
		<phraseLemma>np be able to perform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learned from &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Following prior work in latent feature models for knowledge base completion every textual relation receives its own continuous representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learned from the pattern of its cooccurrences in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the knowledge graph</example>
		<phraseLemma>np learn from np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; suggests &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Further our models posterior &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;suggests an interesting relationship between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; seniority and author choices</example>
		<phraseLemma>np suggest np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; there are &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>For instance in the English wikipedia dump &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with million sentences there are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; approximately million different lowercased and tokenized word types each of which would need its own vector</example>
		<phraseLemma>np with np there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is set to for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Each LSTM state used in the language model sequence si &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is set to for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both states and cell memories</example>
		<phraseLemma>np be set to for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; belonging to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In a distributional model of meaning the semantic representation of a word is given as a vector in some high dimensional vector space obtained either by explicitly collecting cooccurrence statistics of the target word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with words belonging to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a representative subset of the vocabulary or by directly optimizing the word vectors against an objective function in some neuralnetwork based architecture</example>
		<phraseLemma>np with np belong to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that best &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>To address this problem a prior disambiguation step on the word vectors is often introduced the purpose of which is to ﬁnd the word representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that best&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁt to the given context before composition takes place</example>
		<phraseLemma>np that best np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Each word is associated with a main vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; n vectors denoting cluster centroids and an equal number of sense vectors</example>
		<phraseLemma>np as well as with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in Equations &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>During the training of each model we minimize the hinge loss &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Equations&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 and 1</example>
		<phraseLemma>np in equation np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can contribute to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>One of the problems of the recursive and recurrent compositional architectures especially in grammars with strict branching structure such as in English is that any given composition is usually the product of a terminal and a nonterminal ie a single word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can contribute to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the meaning of a sentence to the same extent as the rest of a sentence on its whole as below</example>
		<phraseLemma>np can contribute to np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; is not &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that the driver topic is not&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a contiguous sequence but present in nonadjacent parts and</example>
		<phraseLemma>note that np be not np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; because of &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a ﬁnal note because of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the annotations required most prior work on forums or IRC chats have typically used few hundred threads</example>
		<phraseLemma>as np because of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; proposed by &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>Rewriting Systems To ﬁnd the most likely parse tree we use the parsing algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;proposed by Kallmeyer and Maier for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; binary PLCFRS</example>
		<phraseLemma>np propose by np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on whether &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>With regard to CLUS and LSEG there is a difference in performance between SHORT and LONG threads and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the desired structure was projective or nonprojective</example>
		<phraseLemma>np base on whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that uses &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>MetropolisHastings is a Markov chain Monte Carlo method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that uses a proposal distribution to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; approximate the true distribution when exact sampling is difﬁcult</example>
		<phraseLemma>np that use np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is represented as &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>6</frequency>
		<example>In the Chinese restaurant process each Dirichlet process in the hierarchical structure &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is represented as a restaurant with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an inﬁnite number of tables each serving the same dish</example>
		<phraseLemma>np be represent as np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to convert &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>An alternative approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to convert text descriptions to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prespeciﬁed representations using annotated training data commonly used in language grounding tasks</example>
		<phraseLemma>np be to convert np to np</phraseLemma>
	</can>
	<can>
		<phrase>Moreover we show that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Moreover we show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the acquired representation can be reused across games speeding up learning and leading to faster convergence of Qvalues</example>
		<phraseLemma>moreover we show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; We represent &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Game Representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We represent a game by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tuple hH A T R Ψi where H is the set of all possible game states A is the set of all commands T is the stochastic transition function between states and R is the reward function</example>
		<phraseLemma>np we represent np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been used successfully in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In recent work LSTMs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been used successfully in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NLP tasks such as machine translation and sentiment analysis to compose vector representations of sentences from wordlevel embeddings</example>
		<phraseLemma>np have be use successfully in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we update &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In each iteration i we update&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parameters to reduce the discrepancy between the predicted value of the current state Q i and the expected Qvalue given the reward rt and the value of the next state maxa Q</example>
		<phraseLemma>in np we update np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; gets &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However this model tends to ﬁnd suboptimal solutions and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;gets an average reward of 1 even worse than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; BOWDQN</example>
		<phraseLemma>np get np than np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 demonstrates that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 demonstrates that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the agent with transferred parameters is able to learn quicker than an agent starting from scratch initialized with random parameters reaching the optimal policy almost epochs earlier</example>
		<phraseLemma>figure 1 demonstrate that np</phraseLemma>
	</can>
	<can>
		<phrase>We also investigate the effects of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also investigate the effects of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different minibatch sampling procedures on the parameter learning</example>
		<phraseLemma>we also investigate the effect of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learnt by &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We analyzed the representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learnt by the LSTMDQN model on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Home world</example>
		<phraseLemma>np learn by np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; more similar to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>While distributional vectors can capture the useful fact that say Italy is in many ways &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;more similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Spain than to Germany as humans we also know a set of objective facts about Italy such as what is its capital its area its ofﬁcial language and GDP that are difﬁcult to express in the language of vector algebra and geometry</example>
		<phraseLemma>np more similar to np</phraseLemma>
	</can>
	<can>
		<phrase>Qualitative analysis of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Qualitative analysis of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the results points both to the inherent difﬁculty of correctly retrieving certain classes of attributes and to some intriguing properties of the conceptual nature of the knowledge encoded in distributional data that bias their predictions about certain objective attributes of geographic entities</example>
		<phraseLemma>qualitative analysis of np</phraseLemma>
	</can>
	<can>
		<phrase>We optimize &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We optimize the parameters with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; gradient descent using the Cross Entropy error function</example>
		<phraseLemma>we optimize np with np</phraseLemma>
	</can>
	<can>
		<phrase>We attribute this to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We attribute this to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the presence of correlations</example>
		<phraseLemma>we attribute this to np</phraseLemma>
	</can>
	<can>
		<phrase>For instance &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance the fertility rate is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a function of both general development status and of speciﬁc social factors</example>
		<phraseLemma>for instance np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; extracted from &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This line of work however does not attempt to connect entity representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;extracted from corpora and from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KBs as we do</example>
		<phraseLemma>np extract from np from np</phraseLemma>
	</can>
	<can>
		<phrase>Our approach is similar to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our approach is similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Gupta who predict numerical attributes for unseen concepts from distributional vectors getting comparably accurate estimates for features such as the GDP or CO 1 emissions of a country</example>
		<phraseLemma>we approach be similar to np</phraseLemma>
	</can>
	<can>
		<phrase>In the following we use &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the following we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a derived gold standard including all 1 quantiﬁed classes in QMR with the annotation set to majority opinion instances</example>
		<phraseLemma>in the follow we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is contained in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The ontology describes a world with everything &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is contained in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that world</example>
		<phraseLemma>np that be contain in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; this means that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>By extension &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;this means that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a model built from distributional data does not support denotation in the standard way and thus precludes the deﬁnition of a truth function</example>
		<phraseLemma>np this mean that np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; is also &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a special case of HRG ERG is also&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a contextfree rewriting grammar to recognize and produce graphs</example>
		<phraseLemma>as np be also np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we deﬁne &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we deﬁne&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a synchronous ERG over dependency graphs as a dependency graphtostring grammar which can be used for MT N is a ﬁnite set of nonterminal symbols</example>
		<phraseLemma>in this paper we deﬁne np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set to tune &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>NIST 1 is taken as a development &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set to tune&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; weights and NIST 1 and NIST 1 are two test sets to evaluate systems</example>
		<phraseLemma>np set to tune np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is implemented in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In addition translation results from a recently opensource dependency treetostring system Dep 1 Str 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is implemented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Moses and improves the dependencybased model in Xie are also reported</example>
		<phraseLemma>np which be implement in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which uses &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Menezes and Quirk and Quirk propose the treelet approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which uses dependency structures on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source side</example>
		<phraseLemma>np which use np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is written as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We obtain permutations in the training data by segmenting every wordaligned sourcetarget pair into minimal phrase pairs the resulting alignment between minimal phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is written as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a permutation 1 and onto on the source side</example>
		<phraseLemma>np be write as np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁrst extract &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For learning preordering &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we ﬁrst extract an initial PCFG from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the latent treebank of PETs over the source sentences only</example>
		<phraseLemma>np we ﬁrst extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>We initialize &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We initialize the nonterminal set of this PCFG to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the prime permutations decorating the PET nodes</example>
		<phraseLemma>we initialize np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained with &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We use a 1 gram language model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained with KenLM 1 tune 1 times with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; kbmira to account for tuner instability and evaluated using Multeval 1 for statistical signiﬁcance on 1 metrics</example>
		<phraseLemma>np train with np with np</phraseLemma>
	</can>
	<can>
		<phrase>We show an example of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show an example of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; JapaneseEnglish translation in Figure 1</example>
		<phraseLemma>we show a example of np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate our approach using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our approach using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard machine translation data in a simultaneous translation setting</example>
		<phraseLemma>we evaluate we approach use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; results in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Our experimental results show that including the rewritten references &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into the learning of a phrasebased MT system results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a better speedaccuracy tradeoff against both the original and the rewritten reference translations</example>
		<phraseLemma>np into np result in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Japanese the standard structure of a sentence is NP 1 NP 1 verb where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; case markers following the verb indicate the voice of the sentence</example>
		<phraseLemma>in np be np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; given by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Such errors could be reduced by skipping nodes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with low inside/outside scores given by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parser or skipping lowfrequency patterns</example>
		<phraseLemma>np with np give by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are based on &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Most segmentation strategies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are based on heuristics such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pauses in speech comma prediction and phrase reordering probability</example>
		<phraseLemma>np be base on np such as np</phraseLemma>
	</can>
	<can>
		<phrase>We count &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We count unigrams in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one year of unﬁltered tweets with nation mentions that contain an emoticon</example>
		<phraseLemma>we count np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that were labeled by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The bootstrapped classiﬁers for sports and concerts were learned without labeled data so we ran the sports and concerts classiﬁers on an unseen portion of our data and manually evaluated the ﬁrst 1 tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that were labeled by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each classiﬁer</example>
		<phraseLemma>np that be label by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in using &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>While the input data is sparse we must center each column to have a 1 mean and perform PCA through a singular value decomposition of that columncentered data using the method of Halko &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in using SVD for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PCA the right singular vectors correspond to the principal directions from these we directly read off a K = 1 dimensional score for each proposition in our data</example>
		<phraseLemma>np in use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being made in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Both provide measures of convergent validity that conﬁrm the distinction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being made in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our models is indeed one of political ideology</example>
		<phraseLemma>np be make in np</phraseLemma>
	</can>
	<can>
		<phrase>Recent research in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Recent research in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; psycholinguistics has shown it is possible to automatically infer personal traits from ones linguistic footprints such as tweets and blogs</example>
		<phraseLemma>recent research in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shows some of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The higher θui is the more likely that user u is interested in topic i Table 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows some of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the topics automatically learned by LDA</example>
		<phraseLemma>np show some of np</phraseLemma>
	</can>
	<can>
		<phrase>We built &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For each brand &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we built threeway classiﬁers using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different classiﬁcation algorithms including AdaBoost Decision Tree Logistic Regression Naive Bayes Random Forest and SVM</example>
		<phraseLemma>np we build np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; exploit &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Recently some works &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;exploit Wikipedia for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; detecting and analyzing events on Twitter</example>
		<phraseLemma>np exploit np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relying on &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This is also the limitation of systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relying on Wikipedia such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entity disambiguation which can only disambiguate popular entities and not the ones in the long tail</example>
		<phraseLemma>np rely on np as np</phraseLemma>
	</can>
	<can>
		<phrase>In this study we focus on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this study we focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the precision and the popular trending hashtags and leave the improvement of recall to future work</example>
		<phraseLemma>in this study we focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are derived from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In the 1 step we compute different similarities between each candidate and the hashtag based on different types of contexts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; either side</example>
		<phraseLemma>np which be derive from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while still maintaining &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Therefore to guarantee a good recall in this step &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while still maintaining&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; feasible computation we apply entity linking only on a random sample of the complete tweet set</example>
		<phraseLemma>np while still maintain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; perform well in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>It can be clearly seen that entity linking methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;perform well in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the endogenous group but then deteriorate in the exogenous group</example>
		<phraseLemma>np perform well in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as seen in Figure 1</phrase>
		<frequency>5</frequency>
		<example>However whenever the hashtag evolves into a meaningful topic a deeper annotation method will produce a signiﬁcant improvement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as seen in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as see in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>Based on &lt;NP&gt; assigned by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Based on the ranks of the input sentences assigned by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different systems methods have been proposed to rerank these sentences</example>
		<phraseLemma>base on np assign by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that compares &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In our work we propose a class of novel features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that compares the candidate summary to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set of the basic summaries where H can be regarded as a hypersummary of I</example>
		<phraseLemma>np that compare np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; denote &lt;NP&gt; in &lt;NP&gt; respectively</phrase>
		<frequency>5</frequency>
		<example>Let DAi &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;denote the set of sentences in S and SAi respectively&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np denote np in np respectively</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the results suggest that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>We also conduct manual evaluation and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the results suggest that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the linguistic quality of produced summaries is not decreased by too much compared with extractive counterparts</example>
		<phraseLemma>np the result suggest that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also holds for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This analogy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also holds for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; crosslanguage document summarization with the only difference that the languages of source documents and the target summary are different</example>
		<phraseLemma>np also hold for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used by &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The outline of this algorithm is very similar to the greedy algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used by Morita for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; subtree extraction except that in our context the increase of cost function when adding a sentence is exactly the cost of that sentence</example>
		<phraseLemma>np use by np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will consist of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We reckon that sentences describing the same events may partially share descriptive bigram patterns thus sentences selected by the algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will consist of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; mostly important patterns that appear repeatedly in the original document cluster</example>
		<phraseLemma>np will consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to compute &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Furthermore when the linguistic quality of summaries has been assessed in parallel with annotations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human coverage scores it has been shown that the two dimensions of quality do not correlate with one another providing evidence that coverage scores alone do not fully represent human judgment of the overall quality of summaries</example>
		<phraseLemma>np use to compute np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provides insight into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Although the approach to evaluation of metrics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provides insight into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accuracy of conclusions drawn from metric/test combinations the evaluation is limited by inclusion of only six variants of ROUGE fewer than 1 of possible ROUGE variants</example>
		<phraseLemma>np provide insight into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are computed at &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The fact that ﬁnal overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries is again a divergence from MT evaluation as ngram counts used to compute BLEU scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are computed at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document as opposed to sentencelevel</example>
		<phraseLemma>np be compute at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are taken into consideration</phrase>
		<frequency>5</frequency>
		<example>For all these analyses the stop words have been eliminated from the tweet as well as the document so that only the informative words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are taken into consideration&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be take into consideration</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was matched with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Apart from the 1 times where the tweet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was matched with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; title in the article we also checked to see if the tweet text matched with the article titles that were separately extracted by the newspaper package in order to determine if tweets could be generated using the headline generation methods</example>
		<phraseLemma>np be match with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be estimated by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We assume that the formality of an article &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be estimated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the formality of the words and phrases in the article</example>
		<phraseLemma>np can be estimate by np</phraseLemma>
	</can>
	<can>
		<phrase>We plan to experiment with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We plan to experiment with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweet normalization systems to account for this factor</example>
		<phraseLemma>we plan to experiment with np</phraseLemma>
	</can>
	<can>
		<phrase>This paper is concerned with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This paper is concerned with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of bilingual lexicon induction using imagebased features</example>
		<phraseLemma>this paper be concern with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to derive &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The standard approach in multimodal semantics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to derive&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single image representation for each word eg by averaging the n images</example>
		<phraseLemma>np be to derive np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into how &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Using the two evaluation datasets can potentially provide some insight &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into how&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; purely visual models for bilingual lexicon induction behave with respect to both abstract and concrete concepts</example>
		<phraseLemma>np into how np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; then applied &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We use the SemiSupervised RAE based classification where we 1 trained a standard RAE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using Hindi monolingual corpora then applied&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervised training procedure as described in</example>
		<phraseLemma>np use np then apply np</phraseLemma>
	</can>
	<can>
		<phrase>Due to &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Due to the explosive growth of data ﬁne grained sentiment analysis as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; summarization on the whole chunk of data can be a very timeconsuming task</example>
		<phraseLemma>due to np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consider &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To show that opinion summarization inherently follow the diminishing return property &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consider the following sentences with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive polarity</example>
		<phraseLemma>np consider np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; should have &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The weights of the partitions as well as the threshold parameters for the A are currently kept proportional to the &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inverse of the depth of that aspect in the ontologytree as sentiment expressed on the concepts at higher level in the ontology tree should have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more weightage</example>
		<phraseLemma>np in np should have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; be satisﬁed with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In the ﬁrst clause the writer is positive toward Imam and Prophet as expressed by may God &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;be satisﬁed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; him and peace be upon him respectively</example>
		<phraseLemma>np be satisﬁed with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicating the importance of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Each rule is associated with a weight &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicating the importance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this rule in the whole rule set</example>
		<phraseLemma>np indicate the importance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is assigned by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Thus for an opinion y if the source s &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is assigned by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a ground atom SOURCE is created with score 1</example>
		<phraseLemma>np be assign by np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; does not appear in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If a word does not appear in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the lexicon we do not treat it as a /effect event and thus assign 1 to both EFFECT and EFFECT</example>
		<phraseLemma>if np do not appear in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shown in &lt;NP&gt; of Table 1</phrase>
		<frequency>5</frequency>
		<example>Generalizations of the inference rules used in are expressed in ﬁrstorder logic &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shown in Part 1 of Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np show in np of table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is trained on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Though the PSL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inference does not need supervision and the SVM classiﬁer for agents and themes in Section 1 is trained on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a separate corpus we still have to train the eTarget SVM classiﬁer to assign local scores as described in Section 1</example>
		<phraseLemma>np in np be train on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are known to be &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Jane swims like a dolphin” is easily understood to be a compliment toward Janes swimming ability because dolphins &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are known to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; excellent swimmers</example>
		<phraseLemma>np be know to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; associated with &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Their focus was on extracting salient properties &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;associated with simile vehicles and the affective perception&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on vehicles that the salient properties bring about</example>
		<phraseLemma>np associate with np on np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; we added &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As our sixth set we added&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the instances recognized from the surrounding words of a simile producing the largest data set of positive and negative similes</example>
		<phraseLemma>as np we add np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is common to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Sometimes a simile explicitly mentions a property &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is common to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the tenor and the vehicle</example>
		<phraseLemma>np that be common to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are not part of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Although the properties &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are not part of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our triples because they are optional components we can still use them as valuable features whenever present in the original corpus</example>
		<phraseLemma>np be not part of np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to indicate &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use 1 binary features to indicate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the presence of a positive sentiment word and 1 binary features to indicate the presence of a negative sentiment word in each simile component</example>
		<phraseLemma>we use np to indicate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 but &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Row in Table 1 shows that this classiﬁer produces reasonable precision &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 but&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recall levels only around 1 for both positive and negative polarity</example>
		<phraseLemma>np 1 but np</phraseLemma>
	</can>
	<can>
		<phrase>We show results for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁers trained only with unigram features and classiﬁers trained with our full feature set for positive similes in Figure 1 and negative similes in Figure 1</example>
		<phraseLemma>we show result for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be achieved with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We have also shown that good performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be achieved with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; automatically acquired training instances when manually labeled data may not be available</example>
		<phraseLemma>np can be achieve with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not perform well on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Such approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not perform well on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CC due to</example>
		<phraseLemma>np do not perform well on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is intended to capture &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Such aggregation method among the top matches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is intended to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similarity between videos that share only partially overlapped content</example>
		<phraseLemma>np be intend to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to compute &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Once the anchor frames are detected they are excluded and only the nonanchor frames &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to compute the visual similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; videos associated with event mentions</example>
		<phraseLemma>np be use to compute np between np</phraseLemma>
	</can>
	<can>
		<phrase>Expanding &lt;NP&gt; to include &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Expanding the detec tion range to include&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; visual events in the temporal neighborhood can also differentiate the events</example>
		<phraseLemma>expand np to include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to adapt &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>One idea &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to adapt&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the α value for different types of events eg we expect some event types are more visually oriented than others and thus use a smaller α value</example>
		<phraseLemma>np be to adapt np</phraseLemma>
	</can>
	<can>
		<phrase>We analyze &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We analyze perplexity for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each dataset against a 1 gram language model learned on a generic 1 B words English dataset</example>
		<phraseLemma>we analyze np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; generated by &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The captions of these datasets are either the original photo title and descriptions provided by online users or the captions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;generated by crowd workers for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; existing images</example>
		<phraseLemma>np generate by np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are mentioned in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To get a rough estimate of the reporting bias in image captioning we determined the percentage of toplevel objects &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are mentioned in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the captions for this dataset out of all the objects that are annotated</example>
		<phraseLemma>np that be mention in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are automatically generated from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Toronto COCOQA Dataset is also a visual question answering dataset where the questions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are automatically generated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; image captions of MS COCO dataset</example>
		<phraseLemma>np be automatically generate from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can help with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Our hypothesis is that the combination of geometric textual and visual features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can help with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of predicting the most appropriate preposition since incorporating geometric and visual information should help generate a relation that is consistent with the image content whilst incorporating textual information should help generate a description that is consistent with natural language</example>
		<phraseLemma>np can help with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; ﬁnding &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Thus we extracted the lemmatised head word of each phrase using a semantic head variant of the head &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;ﬁnding rules of Collins in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Stanford CoreNLP</example>
		<phraseLemma>np ﬁnding np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; like &lt;NP&gt; having &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Future work could include nonprepositional terms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;like verbs having&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prepositions modify verbs adding word 1 vec embeddings to the structured prediction model and providing stronger features – whether textual visual or geometric</example>
		<phraseLemma>np like np have np</phraseLemma>
	</can>
	<can>
		<phrase>Of &lt;NP&gt; it was &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Of the ﬁrst generation wordbased SMT models it was&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the only such model with a concave objective function</example>
		<phraseLemma>of np it be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; e for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We j deﬁne &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;e for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; k = 1 to be a special NULL word</example>
		<phraseLemma>np e for np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we showed how &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we showed how&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; IBM Model 1 can be made into a strictly convex optimization problem via functional composition</example>
		<phraseLemma>in this paper we show how np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; affects &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Figure 1 shows how the redistribution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;affects the different relations in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the BLESS test</example>
		<phraseLemma>np affect np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; also &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Table 1 summarizes our top results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the TOEFL BLESS and also&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SimLex 1 similarity test and compares them to a baseline score from the Skipgram model trained on the same data using a window size of 1 negative samples and dimensional vectors</example>
		<phraseLemma>np on np also np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in this paper is &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Our main focus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in this paper is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; POS tagging yet the proposed approach could be applied to a wide variety of language processing tasks</example>
		<phraseLemma>np in this paper be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; of both &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The corpus described &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in details in contains a training set of sentences a development and a test set of both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences</example>
		<phraseLemma>np in np of both np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is also included in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The sentence end token &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is also included in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the evaluation whereas the sentence start token is only used as context in the input layer</example>
		<phraseLemma>np be also include in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by mapping &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>After mapping the utterance into a vector space the model exploits the structure of the output labels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by mapping each label to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a hyperplane that separates utterances with and without that label</example>
		<phraseLemma>np by map np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are initialised with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Both these mappings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are initialised with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unsupervised word embeddings so they can be computed even for words or concepts which were not in the SLU training data</example>
		<phraseLemma>np be initialise with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to form &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Usually unsupervised knowledge sources &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to form&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic codes of the labels that helps us to generalize to unseen labels</example>
		<phraseLemma>np be use to form np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are a form of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In order to represent words and concepts we use word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are a form of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vector space model</example>
		<phraseLemma>np which be a form of np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we split &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For our ﬁrst experiment we split&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each dataset into about 1 for the testing set and 1 for the training set</example>
		<phraseLemma>for np we split np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is designed for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In this paper we describe a new SLU model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is designed for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; improved domain adaptation</example>
		<phraseLemma>np that be design for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has application in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Interarrival time prediction is a type of such modeling and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has application in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many settings featuring continuous time streaming text corpora including journalism for event monitoring realtime disaster monitoring and advertising on social media</example>
		<phraseLemma>np have application in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at &lt;NP&gt; in time</phrase>
		<frequency>5</frequency>
		<example>Tweets associated with an event stream arrive &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at different rates at different points in time&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np at np in time</phraseLemma>
	</can>
	<can>
		<phrase>We propose to address &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose to address&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; interarrival time prediction problem with logGaussian Cox process an inhomogeneous Poisson process which models tweets to be generated by an underlying intensity function which varies across time</example>
		<phraseLemma>we propose to address np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed &lt;NP&gt; comparing to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Evaluation on a set of rumours from Ferguson riots &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed efﬁcacy of our methods comparing to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; baselines</example>
		<phraseLemma>np show np compare to np</phraseLemma>
	</can>
	<can>
		<phrase>Training was performed with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Training was performed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an inhouse toolkit using stochastic gradient descent</example>
		<phraseLemma>training be perform with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the issue of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Meanwhile if an entity has few facts the description will provide information for embedding thus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the issue of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KB sparsity is also well handled</example>
		<phraseLemma>np the issue of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; requiring &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This brings convenience to tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;requiring computation between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; knowledge bases and text</example>
		<phraseLemma>np require np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; cannot be applied to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However it completely relies on the special data source of Wikipedia anchors and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;cannot be applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other general data settings</example>
		<phraseLemma>np can not be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Weston &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that combing scores from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TransE and some text side base extractor achieved much better precisionrecall curve compared to the base extractor</example>
		<phraseLemma>np show that np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be applied to &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Compared to the method of alignment using Wikipedia anchors Wang our method has no dependency on special data sources of anchors and hence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be applied to any knowledge bases with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text descriptions for entities</example>
		<phraseLemma>np can be apply to np with np</phraseLemma>
	</can>
	<can>
		<phrase>Evaluation on &lt;NP&gt; shows that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Evaluation on word similarity shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this initialization signiﬁcantly increases the quality of embeddings for rare words</example>
		<phraseLemma>evaluation on np show that np</phraseLemma>
	</can>
	<can>
		<phrase>There is &lt;NP&gt; available in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There is no usable information available in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this inputlayer representation except for the identity of the word</example>
		<phraseLemma>there be np available in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are signiﬁcant for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Looking only at results for θ ∈ 1 of improvements are signiﬁcant 1 for mixed initialization and of improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are signiﬁcant for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; separate initialization</example>
		<phraseLemma>np be signiﬁcant for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which the number of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Recall that each value of θ effectively results in a different training corpus – a training corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; occurrences of the words in the evaluation data sets has been reduced to ≤ θ</example>
		<phraseLemma>np in which the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; in terms of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We present experimental results that show the superiority of RTRANSE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over TRANSE in terms of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; link prediction</example>
		<phraseLemma>np over np in term of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; so that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This ranking loss effectively trains &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;so that the embedding of the tail is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the nearest neighbor of the translated head but it does not guarantee that the distance between the tail and the translated head is small</example>
		<phraseLemma>np so that np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For ﬁve natural language tasks we pass item agreement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task classiﬁer via soft labeling and lowagreement ﬁltering of the training dataset</example>
		<phraseLemma>np on to np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we follow &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we follow&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Beigman Klebanov and Beigman in using the nominal agreement categories Hard Cases and Easy Cases to separate instances by item agreement</example>
		<phraseLemma>in this paper we follow np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; s &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However unlike Beigman Klebanov and Beigman who use &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;simple percentage agreement we calculate itemspeciﬁc agreement via Krippendorff s α item agreement with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Nominal Ordinal or Ratio distance metrics as appropriate</example>
		<phraseLemma>np s np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; including &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Features used are combinations of the characters after the removal of the longest common substring &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between the word pair including&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 additional characters from the substring word boundaries are marked</example>
		<phraseLemma>np between np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provided by &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We use the crowdsourced annotation for a 1 headline sample of this dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provided by Snow 1 with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; annotations per emotion per headline</example>
		<phraseLemma>np provide by np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are the same as for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Training strategies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are the same as for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Biased Language experiments except</example>
		<phraseLemma>np be the same as for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; failed to outperform &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Soft labeling &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;failed to outperform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; integrated labels for 1 of the 1 complete test sets</example>
		<phraseLemma>np fail to outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; there has been &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Despite the growing interest in vector representations of semantic information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;there has been relatively little work on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; direct evaluations of these models</example>
		<phraseLemma>np there have be np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; providing &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In particular we perform a comprehensive analysis of evaluation methods and introduce novel methods that can be implemented through crowdsourcing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;providing better insights into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the relative strengths of different embeddings</example>
		<phraseLemma>np provide np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the dataset &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For each of the query words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the dataset&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the nearest neighbors at ranks k ∈ for the six embeddings were retrieved</example>
		<phraseLemma>np in the dataset np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is necessary in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We take the two observations above as evidence that a more ﬁnegrained analysis &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is necessary in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; discerning different embedding methods</example>
		<phraseLemma>np be necessary in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; chose &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To normalize for frequencybased effects we computed the average frequency avg of the three words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in this set and chose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the intruder word to be the ﬁrst word that had a frequency of avg ± 1 starting at rank of the list of nearest neighbors</example>
		<phraseLemma>np in np choose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at &lt;NP&gt; as follows</phrase>
		<frequency>5</frequency>
		<example>We evaluate the relative performance of word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at this task as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np at np as follow</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate the relative performance of word embeddings at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task as follows</example>
		<phraseLemma>we evaluate np at np</phraseLemma>
	</can>
	<can>
		<phrase>We found &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We found a strong correlation between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the frequency of a word and its position in the ranking of nearest neighbors in our experiments</example>
		<phraseLemma>we find np between np</phraseLemma>
	</can>
	<can>
		<phrase>Factors such as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Factors such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word frequency play a signiﬁcant and previously unacknowledged role</example>
		<phraseLemma>factor such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that use &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As a result LDA models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that use prior knowledge only work in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; smallscale scenarios</example>
		<phraseLemma>np that use np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; to improve &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Therefore its often necessary to incorporate prior knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into topic models to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the models performance</example>
		<phraseLemma>np into np to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is independent from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>LDA assumes that the hidden topic assignment of a word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is independent from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other hidden topics given the documents topic distribution θ</example>
		<phraseLemma>np be independent from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in Equation 1</phrase>
		<frequency>5</frequency>
		<example>SparseLDA efﬁciently computes the s r q bins &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in Equation 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as in equation 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; decreases as the number of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Because document labels apply sparsity to the documenttopic counts the average running time per iteration &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;decreases as the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; labeled document increases</example>
		<phraseLemma>np decrease as the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been proposed to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Numerous methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been proposed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; leverage path information for knowledge base completion and question answering</example>
		<phraseLemma>np have be propose to np</phraseLemma>
	</can>
	<can>
		<phrase>Results on &lt;NP&gt; show &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Results on several languages show&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an absolute improvement of 1 in average dependency accuracy over the stateoftheart method of</example>
		<phraseLemma>result on np show np</phraseLemma>
	</can>
	<can>
		<phrase>We demonstrate the utility of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We demonstrate the utility of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dense projected structures when training the targetlanguage parser</example>
		<phraseLemma>we demonstrate the utility of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that builds on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We describe a training algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that builds on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the deﬁnitions of dense structures</example>
		<phraseLemma>np that build on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; returns &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>TRAIN is a function that takes a set of dependency structures D as input and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;returns a model θ as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its output</example>
		<phraseLemma>np return np as np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; of learning &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the ﬁrst stage of learning&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model is initialized by training on</example>
		<phraseLemma>in np of learn np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; for &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In these experiments for a given target language we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all other languages in our data as source languages</example>
		<phraseLemma>in np for np we use np</phraseLemma>
	</can>
	<can>
		<phrase>We have described &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We have described a densitydriven method for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the induction of dependency parsers using parallel data and sourcelanguage parsers</example>
		<phraseLemma>we have describe np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are a series of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The key ideas &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are a series of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; increasingly relaxed deﬁnitions of density together with an iterative training procedure that makes use of these deﬁnitions</example>
		<phraseLemma>np be a series of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained on &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This work is motivated by the idea of delexicalized parsing in which a parser is built without any lexical features and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained on a treebank for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a resourcerich source language</example>
		<phraseLemma>np train on np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as presented in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We compare our approach to a baseline interlingual model based on the same parsing algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as presented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; section 1 but with cascaded training</example>
		<phraseLemma>np as present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consistently outperformed &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The supervised neural network dependency parser performed worst as expected and the baseline cascade model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consistently outperformed the supervised model on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all languages by an average margin of 1 The joint model also consistently outperformed both baselines giving a further 1 average improvement over the cascade</example>
		<phraseLemma>np consistently outperform np on np</phraseLemma>
	</can>
	<can>
		<phrase>With &lt;NP&gt; we see &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;With more training data we see&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; interesting changes to the relative performance of the different models</example>
		<phraseLemma>with np we see np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; needs to consider &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To understand this pattern of performance differences for the cascade versus the joint model one &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;needs to consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the cascade model formulation</example>
		<phraseLemma>np need to consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; enabling &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We do this by augmenting its transition operations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a SWAP operation enabling&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parser to produce nonprojective dependencies which are often found in morphologically rich languages</example>
		<phraseLemma>np with np enable np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; depending on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To do this Dyer use a recursive neural network gr that composes the representations of the two subtrees popped from S resulting &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a new vector gr or gr depending on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the direction of attachment</example>
		<phraseLemma>np in np depend on np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; is calculated using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For these datasets evaluation is calculated using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; evalpl which includes punctuation</example>
		<phraseLemma>for np be calculate use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; corresponding to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We present an LSTM approach to deletionbased sentence compression where the task is to translate a sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into a sequence of zeros and ones corresponding to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; token deletion decisions</example>
		<phraseLemma>np into np correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; processing such as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Indeed core problems in natural language &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;processing such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation parsing image captioning or learning to execute small programs employed virtually the same principles—the use of Recurrent Neural Networks</example>
		<phraseLemma>np processing such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is processed with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Consequently and for the sake of being able to successfully train the model with largescale data the learning procedure is implemented as a distributed structured perceptron with iterative parameter mixing where each shard &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is processed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MIRA and K is set to 1</example>
		<phraseLemma>np be process with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which gives &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We take the system of Liu &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which gives stateoftheart performance and efﬁciencies in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard word ordering benchmark</example>
		<phraseLemma>np which give np in np</phraseLemma>
	</can>
	<can>
		<phrase>On the other hand when &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On the other hand when&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data scale increases syntactic models can become much slower to train compared with Ngram models</example>
		<phraseLemma>on the other hand when np</phraseLemma>
	</can>
	<can>
		<phrase>Note that in contrast to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that in contrast to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; related tasks like machine translation we will assume that the output length N is ﬁxed and that the system knows the length of the summary before generation</example>
		<phraseLemma>note that in contrast to np</phraseLemma>
	</can>
	<can>
		<phrase>They extract &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They extract tree transduction rules from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; aligned parsed texts and learn weights on transfomations using a maxmargin learning algorithm</example>
		<phraseLemma>they extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>For reference &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For reference&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the best human evaluator scores 1 ROUGE 1</example>
		<phraseLemma>for reference np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁrst note that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We ﬁrst note that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baselines COMPRESS and IR do relatively poorly on both datasets indicating that neither just having article information or language model information alone is sufﬁcient for the task</example>
		<phraseLemma>we ﬁrst note that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Of these features the biggest impact is from using a more powerful encoder &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; beam search to generate summaries</example>
		<phraseLemma>np as well as use np</phraseLemma>
	</can>
	<can>
		<phrase>use &lt;NP&gt; for ﬁnding &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use a search oriented approach for ﬁnding&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relevant parts of the reference paper to citations</example>
		<phraseLemma>use np for ﬁnding np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; except for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>All terms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in citation except for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stopwords numeric values and citation markers ie name of authors or numbered citations</example>
		<phraseLemma>np in np except for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by averaging over &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We also considered the oracles performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by averaging over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ROUGE scores of all human summaries calculated by considering one human summary against others in each topic</example>
		<phraseLemma>np by average over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to evaluate &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We use a dataset collected from Sina Weibo which provides the Twitterlike service and is one of the most popular one &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in China to evaluate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the proposed approach and alternative methods</example>
		<phraseLemma>np in np to evaluate np</phraseLemma>
	</can>
	<can>
		<phrase>Were used to establish &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In discriminativetermweights &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were used to establish&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topicterm relationships of which users perception were context in cluster selection</example>
		<phraseLemma>np be use to establish np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we propose &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we propose a graphbased method using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word coupling which combines the merits of both word frequencies and text features for readability assessment</example>
		<phraseLemma>in this paper we propose np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; focus on extracting &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The featurebased methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;focus on extracting&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text features from a document and training a classiﬁcation model to classify its readability</example>
		<phraseLemma>np focus on extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; representing &lt;NP&gt; among &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Firstly we construct a graph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;representing the readability relationship among&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; documents by using the coupled bagofwords model to compute the relations among these documents</example>
		<phraseLemma>np represent np among np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are common in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For each node v we ﬁrstly select the neighbors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are common in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the three graphs ∩ N lex ∩ N syn</example>
		<phraseLemma>np which be common in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; It has &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>On ENCT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It has relatively good performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; grade levels 1 and 1 while on the Chinese dataset CPT the performance is not satisfactory</example>
		<phraseLemma>np it have np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>On ENCT It has relatively good performance on grade levels 1 and 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Chinese dataset CPT the performance is not satisfactory</example>
		<phraseLemma>np while on np</phraseLemma>
	</can>
	<can>
		<phrase>From Figure 1 &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the three word coupling matrices greatly outperform the TFIDF matrix especially on the Chinese dataset</example>
		<phraseLemma>from figure 1 np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are mostly based on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Feature groups &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are mostly based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; wordngrams such as unigrams bigrams or the combination of unigrams and bigrams</example>
		<phraseLemma>np be mostly base on np</phraseLemma>
	</can>
	<can>
		<phrase>We collected &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As ground truth data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we collected several cityspeciﬁc datasets using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Twitter Search API</example>
		<phraseLemma>np we collect np use np</phraseLemma>
	</can>
	<can>
		<phrase>Various approaches to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Various approaches to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this task have been proposed and used in the context of NLP</example>
		<phraseLemma>various approach to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponds to &lt;NP&gt; while &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Two other common values for ρ are 1 and 1 the former &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponds to a straightforward concatenation of the source and target data while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the latter is the sharedhyperparameter setting which shares α and λ between the source and target domain</example>
		<phraseLemma>np correspond to np while np</phraseLemma>
	</can>
	<can>
		<phrase>As we shall see in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we shall see in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 such a representation eases the estimation of the parameters</example>
		<phraseLemma>as we shall see in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are chosen based on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>These pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are chosen based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the similarities in their genres score ranges and median scores</example>
		<phraseLemma>np be choose base on np</phraseLemma>
	</can>
	<can>
		<phrase>In addition since &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition since&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ASAP data has at least 1 human annotators for each essay we also calculate the human agreement score</example>
		<phraseLemma>in addition since np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; outperforms that of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However for pair 1 &amp;gt 1 the QWK score for domain adaptation with target essays &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;outperforms that of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the indomain albeit only by 1</example>
		<phraseLemma>np outperform that of np</phraseLemma>
	</can>
	<can>
		<phrase>To &lt;NP&gt; we estimate &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To this end we estimate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the the same performance for prompts 1 to 1 slightly contribution of bagofwords features to the overpoorer performance for prompts 1 to 1 and much all prediction by computing the ratio better performance for prompt 1</example>
		<phraseLemma>to np we estimate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also has &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Question answering &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also has some similar aspects to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the proposed task although aiming at a very different goal which is to provide an explicit – typically unique and concise – answer to a question</example>
		<phraseLemma>np also have np to np</phraseLemma>
	</can>
	<can>
		<phrase>In general we observe that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In general we observe that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a text segment should satisfy three criteria to be considered CDE of a speciﬁc type</example>
		<phraseLemma>in general we observe that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is assigned &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Each candidate &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is assigned a score by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two components in the contextfree stage and their scores are averaged</example>
		<phraseLemma>np be assign np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; containing &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Next each sentence was represented by a concatenation of two feature vectors – a bagofwords representation limited to a handcrafted subjectivity lexicon &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;containing words a bagofpatterns representation based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; patterns observed as frequent in the subjective sentences detected by a modiﬁcation of the SPM algorithm</example>
		<phraseLemma>np contain np base on np</phraseLemma>
	</can>
	<can>
		<phrase>Were generated by combining &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Negative examples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were generated by combining&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; claims and CDEs detected in the same topic and article but that were not linked in our labeled data</example>
		<phraseLemma>np be generate by combine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is not related to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Noise in this context is everything &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is not related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; misspelled queries</example>
		<phraseLemma>np that be not relate to np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows results of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this ﬁltering step in terms of data sizes and the accuracy on the dev set</example>
		<phraseLemma>table 1 show result of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to evaluate &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Nevertheless exponential reservoir sampling helps us to focus on the head of that distribution and our main goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to evaluate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the capabilities of the spelling correction framework not the overall system integration</example>
		<phraseLemma>np be to evaluate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produces &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Note that for the case where the gold reference demands a correction and the speller &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produces a wrong correction different from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source query we have to increase both false positives FP and false negatives FN</example>
		<phraseLemma>np produce np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is due to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Our previous experiments prefer recall &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 split of misspelled vs correct queries in the dev set</example>
		<phraseLemma>np which be due to np</phraseLemma>
	</can>
	<can>
		<phrase>Were given &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Participating teams &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were given training data with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; manually annotated corrections of grammatical errors and were allowed to use publicly available resources for training</example>
		<phraseLemma>np be give np with np</phraseLemma>
	</can>
	<can>
		<phrase>However it should be noted that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However it should be noted that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; judges never saw the repeated outputs within one ranking which probably decreases agreement compared to the MTspeciﬁc task</example>
		<phraseLemma>however it should be note that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; to predict &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Again we turn to Bojar who choose their rankings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the ranking models ability to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pairwise rankings</example>
		<phraseLemma>np base on np to predict np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is reached for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The switch to β = 1 from β = 1 for the CoNLL 1 shared task was a good choice but a higher correlation can be achieved for β = 1 the maximum &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is reached for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; β = 1</example>
		<phraseLemma>np be reach for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be directly applied to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In contrast to the DBN which stacks restricted Boltzmann machines and is often used to initialize a deep multilayer perceptron the SBEN model is constructed by composing hybrid restricted Boltzmann machines and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be directly applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the discriminative task in a single learning phase</example>
		<phraseLemma>np can be directly apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being constructed in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We hypothesize that holistic ﬁnetuning ensures that discriminative information is incorporated into the generative features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being constructed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the bottomup learning step</example>
		<phraseLemma>np be construct in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allows us to incorporate &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The probability threshold p¯ for the potential 1 call to the ensemble backpropagation routine &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allows us to incorporate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a tunable form of pseudolabeling into the BottomUpTopDown learning algorithm</example>
		<phraseLemma>np allow we to incorporate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; followed by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This is the &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;online formulation of the SVM trained via subgradient descent on the primal objective followed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a projection step</example>
		<phraseLemma>np on np follow by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in and &lt;NP&gt; in</phrase>
		<frequency>5</frequency>
		<example>The HRBMs metaparameters were tuned using a similar setup to with learning rate varied in α &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in and β in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in and np in</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we explore the use of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we explore the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings for click prediction</example>
		<phraseLemma>in this paper we explore the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to measure &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To the best of our knowledge there is no previous work adopting semanticlevel text features for the purpose of click prediction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in particular word embeddings to measure&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; queryad relevance</example>
		<phraseLemma>np in np to measure np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; include the number of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>These features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;include the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; overlapping words and characters in queryad URL queryad title and queryad description and the number of words and characters in the query</example>
		<phraseLemma>np include the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be done with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We argue that domain adaptation for the extraction of temporal expressions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be done with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; limited efforts and should cover preprocessing as well as temporal speciﬁc tasks</example>
		<phraseLemma>np can be do with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that our method outperforms &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The experimental results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that our method outperforms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline bootstrapping system based on the ideas of Agichtein and Gravano which relies on TFIDF representations</example>
		<phraseLemma>np show that we method outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; instead of relying on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>It differs however in that it attempts to ﬁnd similar relationships using word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;instead of relying on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TFIDF representations</example>
		<phraseLemma>np instead of rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; computing &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Next it iterates through the list of instances &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;computing the similarity between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an instance in and every cluster Clj</example>
		<phraseLemma>np computing np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; differs from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As a result clustering &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Algorithm 1 differs from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original Snowball method which instead computes similarities towards cluster centroids</example>
		<phraseLemma>np in np differ from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is associated with &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The pattern which has the highest similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is associated with i along with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corresponding similarity score</example>
		<phraseLemma>np be associate with np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is applied to &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Tree pattern matching &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is applied to lemmatised syntax trees using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Tregex engine which supports a compact language for writing regular expressions over trees see Table 1 for examples of patterns and matching phrases</example>
		<phraseLemma>np be apply to np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be exploited as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We show how KB tags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be exploited as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a useful complement to traditional NER supervision</example>
		<phraseLemma>np can be exploit as np</phraseLemma>
	</can>
	<can>
		<phrase>We encode &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We encode each phrase using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the BMEOW scheme described above and use the ﬁlename of each gazetteer as its type</example>
		<phraseLemma>we encode np use np</phraseLemma>
	</can>
	<can>
		<phrase>This is important because &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is important because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; existing KBs are mostly static</example>
		<phraseLemma>this be important because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We show that this labeling approach leads to good performance even when offtheshelf classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the distantlylabeled data</example>
		<phraseLemma>np be use on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by introducing &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Distant supervision is often coupled with learning methods that allow for this sort of noise &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by introducing latent variables for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each entity mention by carefully selecting the entity mentions from contexts likely to include speciﬁc KB facts by careful ﬁlter ing of the KB strings used as seeds or by making use of namedentity linking methods and coreference to improve the matching phase of distant learning</example>
		<phraseLemma>np by introduce np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; in combination with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Here we describe a pipelined system which identiﬁes lists of semanticallyrelated items using lexicosyntactic patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses distant supervision in combination with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a labelpropagation method to ﬁnd entity mentions that can be conﬁdently labeled and from this data uses ordinary classiﬁer learners to classify entity mentions by their semantic type</example>
		<phraseLemma>np use np in combination with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be viewed as &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The extracted lists and their items as well as entity mentions and their corresponding NPs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be viewed as a bipartite graph where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one set of vertices are identiﬁers for the lists and entity mentions and the other set of vertices are the strings that occur as items of those lists or as NPs of those mentions</example>
		<phraseLemma>np can be view as np where np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; cooccur in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>It seems intuitive to assume that if two items &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;cooccur in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a coordinateterm list they are very likely to have the same type so it seems plausible to use label propagation on this graph to propagate types from NPs with known types to lists and then from lists to NPs across this graph</example>
		<phraseLemma>np cooccur in np</phraseLemma>
	</can>
	<can>
		<phrase>We calculate &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For evaluation metric &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we calculate recall on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the instances in the heldout set</example>
		<phraseLemma>np we calculate np on np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; are obtained by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For DSbaseline the testing NP examples are obtained by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distant labeling with validating instances and on average 1 examples are collected for each run</example>
		<phraseLemma>for np be obtain by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; achieving &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By using ENTICE we are able to increase NELLs knowledge density by a factor of 1 while achieving&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 accuracy</example>
		<phraseLemma>np by np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; for improving &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As previously mentioned recent proposals for improving&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; density of KGs such as those reported in focus on extracting facts of one of the 1 extraction classes mentioned in Table 1 viz KRKE</example>
		<phraseLemma>as np for improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are also added to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In order to make the query speciﬁc especially in case of ambiguous entities a few keywords &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are also added to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the query</example>
		<phraseLemma>np be also add to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are frequent in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Also if the NPs present in the triple &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are frequent in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the cluster then it makes the corresponding triple more like to become a representative</example>
		<phraseLemma>np be frequent in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also experiment with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>While these initial results are encouraging we hope to apply ENTICE on other knowledge graphs and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also experiment with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other normalization and entity linking algorithms as part of future work</example>
		<phraseLemma>np also experiment with np</phraseLemma>
	</can>
	<can>
		<phrase>This is because &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is because if and are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; arguments of the same predicate then their shortest path should pass through that predicate if and belong to different predicateargument structures their shortest path will pass through a sequence of predicates and any consecutive predicates will share a common argument</example>
		<phraseLemma>this be because np be np</phraseLemma>
	</can>
	<can>
		<phrase>Note that &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Note that the order of the predicates on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the path indicates the proper assignments of subjects and objects for that relation</example>
		<phraseLemma>note that np on np</phraseLemma>
	</can>
	<can>
		<phrase>We choose &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>And for the rest cases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we choose the nonother relation with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; highest conﬁdence as the output since ideally for a nonother instance our model will output the correct label for the right subject/object direction and an other label for the wrong direction</example>
		<phraseLemma>np we choose np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; in total</phrase>
		<frequency>5</frequency>
		<example>For our proposed NS we create a negative example from each nonother &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;instance in the training set 1 in total&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np in total</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; requires &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As input HeidelTime &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;requires linguistic preprocessed documents with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence token and partofspeech information – although it is possible to create language resources not making use of pos constraints</example>
		<phraseLemma>np require np with np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; of languages &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For a subset of languages&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; temporally annotated corpora exist</example>
		<phraseLemma>for np of language np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to utilize &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>One approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to utilize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lexical embeddings to improve NER systems including for Twitter</example>
		<phraseLemma>np be to utilize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The baseline system for our task is our own implementation of Mao &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is the current stateoftheart on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SIGHAN 1 shared task</example>
		<phraseLemma>np which be np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used to represent &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>hGoel Grey played in Cabareti hTom Brady play in National Football Leaguei Informally the goal of our system is to automatically infer a set of schemas such as ht play in t 1 i where and are two semantic types drawn from a standard knowledge base such as WordNet Yago Freebase and Probase and each such schema &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used to represent&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of play in” relation instances</example>
		<phraseLemma>np can be use to represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is generated from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Besides each group has a representative relation pattern &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is generated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the patterns within the group</example>
		<phraseLemma>np which be generate from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we remove &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>First since adjectives adverbs and modal verbs can hardly change the type distribution of arguments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a relation we remove&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these words from a pattern</example>
		<phraseLemma>np in np we remove np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows the MRR scores by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; using both baseline model and our approach</example>
		<phraseLemma>table 1 show np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; as compared to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>An extensive set of experiments has been conducted on TRECKBA 1 dataset and the results demonstrate that this model can yield a signiﬁcant performance gain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in recommendation quality as compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart</example>
		<phraseLemma>np in np as compare to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; where m is the number of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We adopt Akaike Information Criteria to determine the number of latent variables which is calculated as 1 m 1 L &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;where m is the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parameters in the model</example>
		<phraseLemma>np where m be the number of np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>According to different granularity settings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we evaluate the proposed models in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two scenarios</example>
		<phraseLemma>np we evaluate np in np</phraseLemma>
	</can>
	<can>
		<phrase>We study &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We study CCR as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a classiﬁcation problem and propose a latent document type model through introducing a latent layer in a discriminative model to capture the correlations between documents and their intrinsic types</example>
		<phraseLemma>we study np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was computed by using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>After this graph is constructed the eigenvector centrality score for each sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was computed by using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a power method</example>
		<phraseLemma>np be compute by use np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; from &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the CQAQL corpus from Subtask A of SemEval 1 Task 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Answer Selection in CQA</example>
		<phraseLemma>we use np from np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains data from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains data from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Qatar Living forum and is publicly available on the tasks website</example>
		<phraseLemma>np contain datum from np</phraseLemma>
	</can>
	<can>
		<phrase>This can be attributed to the fact that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This can be attributed to the fact that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Wikipedia articles outnumbered the EHR data by 1 times</example>
		<phraseLemma>this can be attribute to the fact that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as given in Table 1</phrase>
		<frequency>5</frequency>
		<example>We deﬁne IDs for each modality in each division &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as given in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as give in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provided by &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We apply the CNN model pretrained using the ILSVRC 1 dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provided by Caffe a standard deep learning software package in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁeld of visual recognition</example>
		<phraseLemma>np provide by np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are then used for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The labeled data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are then used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; training machine learning algorithms</example>
		<phraseLemma>np be then use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to increase &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This paper considers the question as to whether the overall argumentation of web reviews can be modeled in a general way &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to increase&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; domain independence in sentiment analysis</example>
		<phraseLemma>np in order to increase np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that play a role in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Especially topical web review domains differ widely regarding the terms and phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that play a role in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their argumentation</example>
		<phraseLemma>np that play a role in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we determine &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each variant we determine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all sentiment ﬂows that represent at least 1 of all reviews in a given training set</example>
		<phraseLemma>for np we determine np</phraseLemma>
	</can>
	<can>
		<phrase>We obtain &lt;NP&gt; of 1</phrase>
		<frequency>5</frequency>
		<example>In the movie domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we obtain an overall accuracy of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np we obtain np of 1</phraseLemma>
	</can>
	<can>
		<phrase>In addition we propose &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition we propose&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a novel integration of neural and discrete features which combines their relative advantages leading to signiﬁcantly higher results compared to both baselines</example>
		<phraseLemma>in addition we propose np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; represented by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Each node in the input takes a real value &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between 1 and 1 as represented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; grey nodes in Figure 1</example>
		<phraseLemma>np between np represent by np</phraseLemma>
	</can>
	<can>
		<phrase>Given &lt;NP&gt; extracted from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given those standardized units extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a corpus it is feasible to overview the distribution of values for each element or a combination of elements</example>
		<phraseLemma>give np extract from np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; consists of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If the opinion word consists of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than one phrase we take the minimum difference</example>
		<phraseLemma>if np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; usually consists of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>A CFO &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;usually consists of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sequence of Condphrases where each phrase modiﬁes the next phrase as in Figure 1</example>
		<phraseLemma>np usually consist of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; excluding &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Thus the value of feature takes 1 for the ﬁrst phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a sentence excluding&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a conjunction and 1 otherwise</example>
		<phraseLemma>np in np exclude np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as the value for</phrase>
		<frequency>5</frequency>
		<example>We use the existence of words that are strongly associated with UCFO &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as the value for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as the value for</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by Equation 1</phrase>
		<frequency>5</frequency>
		<example>Finally we calculated a mutual information like score Score between a restrictive word r and labels u Condphrases for UCFOs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by Equation 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np by equation 1</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to train &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used CRF 1 to train a classiﬁer for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each phrase and regularized the parameters using norm</example>
		<phraseLemma>we use np to train np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; identifying &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>While it is important to increase the vocabulary size of our dictionary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;identifying synonymous expressions with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; partial matching is also important</example>
		<phraseLemma>np identify np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is associated with &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We need to identify whether an expression for a person &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is associated with userrelated attributes such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the bed is small for a person who is tall” which indicates a physical attribute of a user</example>
		<phraseLemma>np be associate with np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We proposed thirteen features associated with lexical and syntactic information of Japanese and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed their effectiveness using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reviews for hotels</example>
		<phraseLemma>np show np use np</phraseLemma>
	</can>
	<can>
		<phrase>Some of &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Some of these examples have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; qualities reminiscent of Winograd schemas</example>
		<phraseLemma>some of np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; captures &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>While our annotation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;captures different information from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PropBank it is closely related</example>
		<phraseLemma>np capture np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be transformed into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Therefore a gold questionanswer pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be transformed into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple positive training samples</example>
		<phraseLemma>np can be transform into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; of 1 in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The experimental results demonstrate that our model outperforms stateoftheart methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with an average gain of 1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; accuracy and 1 in the measure on three datasets in different domains</example>
		<phraseLemma>np with np of 1 in np</phraseLemma>
	</can>
	<can>
		<phrase>Based on &lt;NP&gt; we build &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Based on these assumptions we build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a graph to represent the documents and their relations and we perform a TrustRanklike algorithm on the graph</example>
		<phraseLemma>base on np we build np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be obtained in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Combining these two indexes the total outneighbors of a document &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be obtained in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; constant time as follows</example>
		<phraseLemma>np can be obtain in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a group of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We collected car brands of General Motors and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a group of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets that contain at least one mention of these brands via the Twitter API</example>
		<phraseLemma>np a group of np</phraseLemma>
	</can>
	<can>
		<phrase>On &lt;NP&gt; we employ &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On one side we employ&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the topics in a smallscale highquality relevant documents to summarize the life slices of a target entity and on the other side we use the biography as a reliable reference material to detect new truly relevant documents from a largescale partially complete pseudofeedback</example>
		<phraseLemma>on np we employ np</phraseLemma>
	</can>
	<can>
		<phrase>Towards &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Towards a target entity of a specific type such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ones discussed in this paper a person or an organization the goal of entity archiving is to search and collect all relevant documents from largescale data sets under limited prior knowledge of the entity</example>
		<phraseLemma>towards np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; conditioned on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Generally Relevance Model refers to the probability distribution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over all words conditioned on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their occurrences in a set of previouslyknown relevant documents ie w V P where V is the vocabulary R is the document set and P can be estimated by TFIDF</example>
		<phraseLemma>np over np condition on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; occurs in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Due to the separate topic modeling procedures for the reference and candidate sources the probability P ̶a topic tR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the reference source occurs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a candidate document D ̶cannot be obtained directly</example>
		<phraseLemma>np in np occur in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set as well as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>GibbsLDA makes it easy to parse the topics in a document &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; estimate topic models P</example>
		<phraseLemma>np set as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; measures &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For each target entity CEA &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;measures the biographydocument relevance for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all documents in the candidate source</example>
		<phraseLemma>np measure np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; probably due to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Meanwhile better performance was obtained with the stochastic ListNet approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;probably due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the learning of partial rank information</example>
		<phraseLemma>np probably due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is possible with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Although any k &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is possible with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the proposed stochastic ListNet we will show that simply increasing the model order k does not improve performance</example>
		<phraseLemma>np be possible with np</phraseLemma>
	</can>
	<can>
		<phrase>This conﬁrms &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This conﬁrms our argument that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rank information can be learned from a subset of the permutation classes that are randomly selected and the partial rank learning can lead to even better performance than the full rank learning the case of conventional ListNet</example>
		<phraseLemma>this conﬁrms np that np</phraseLemma>
	</can>
	<can>
		<phrase>It can be regarded as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It can be regarded as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a general framework that treats both the pairwise learning and the full rank learning as two special cases</example>
		<phraseLemma>it can be regard as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; pose &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We ﬁnd that unique characteristics of this domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;pose new challenges in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; efﬁcient inference</example>
		<phraseLemma>np pose np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is turned into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>A multiple choice question with k answer options &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is turned into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; k truefalse questions each of which asserts some known facts and posits a query</example>
		<phraseLemma>np be turn into np</phraseLemma>
	</can>
	<can>
		<phrase>This reduces the number of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This reduces the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; groundings while retaining the crux of the reasoning problem deﬁned over generalities</example>
		<phraseLemma>this reduce the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; rather than &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We deﬁne rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over prototypical entity/event constants rather than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁrstorder variables</example>
		<phraseLemma>np over np rather than np</phraseLemma>
	</can>
	<can>
		<phrase>To handle &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To handle incomplete matches for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each KB derived MLN rule of the form &amp;gt R we also add k soft rules i of the form Li &amp;gt R</example>
		<phraseLemma>to handle np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; involving &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Although drop” entails fall” and ball” entails object” ERMLN cannot reliably bridge the structural difference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;involving object and agent as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these two relationships typically arent equivalent</example>
		<phraseLemma>np involve np as np</phraseLemma>
	</can>
	<can>
		<phrase>Instead of using &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Instead of using edges for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inference we use them as factors inﬂuencing alignment</example>
		<phraseLemma>instead of use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was performed using &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Marginal inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was performed using MCSAT with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; default parameters and ﬂips per sample to generate samples for marginal estimation</example>
		<phraseLemma>np be perform use np with np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; we adopt &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a baseline we adopt&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a competitive unsupervised collective EL system utilizing structured KBs</example>
		<phraseLemma>as np we adopt np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁrst &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For each of the mentions extracted from the source context &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we ﬁrst select a list of entity candidates from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Gk with heuristic rules such as fuzzy string matching synonyms Wikipedia redirect etc</example>
		<phraseLemma>np we ﬁrst np from np</phraseLemma>
	</can>
	<can>
		<phrase>While many of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While many of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these approaches have been proved to be effective the dependency on deep linguistic knowledge makes it difﬁcult to migrate them to a new language or domain</example>
		<phraseLemma>while many of np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we demonstrated &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we demonstrated&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a highperformance EL approach that can be easily migrated to new languages and domains due to the minimal reliance on linguistic analysis and the deep utilization of structured KBs</example>
		<phraseLemma>in this paper we demonstrate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by summing up &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The addition operation obtains the vector of a path &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by summing up&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vectors of all relations which is formalized as</example>
		<phraseLemma>np by sum up np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is also related to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The difference is that here we consider the reliability of a path p &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is also related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the inference strength given r which is quantiﬁed as Pr = Pr/ Pr obtained from the training data</example>
		<phraseLemma>np be also relate to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For comparison we select all methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our baselines and use their reported results directly since the evaluation dataset is identical</example>
		<phraseLemma>np in as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; due to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Since our implementation of TransE has achieved the best performance among all baselines for entity prediction here we only compare PTransE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with TransE due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; limited space</example>
		<phraseLemma>np with np due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in these &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In contrast many testing triples in this task correspond to nonrelation and there are usually several relation paths between two entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in these&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nonrelation triples</example>
		<phraseLemma>np in these np</phraseLemma>
	</can>
	<can>
		<phrase>This paper addresses the problem of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This paper addresses the problem of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; corpuslevel entity typing ie inferring from a large corpus that an entity is a member of a class such as food” or artist”</example>
		<phraseLemma>this paper address the problem of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; present our conclusions</phrase>
		<frequency>5</frequency>
		<example>Finally we discuss remaining challenges and possible future work and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;present our conclusions&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np present we conclusion</phraseLemma>
	</can>
	<can>
		<phrase>The hypothesis is that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The hypothesis is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words with similar meanings tend to occur in similar contexts and therefore cooccur with similar context words</example>
		<phraseLemma>the hypothesis be that np</phraseLemma>
	</can>
	<can>
		<phrase>We test &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We test this hypothesis for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the simplest possible joint model which adds the scores of the two individual models</example>
		<phraseLemma>we test np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been replaced by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To learn entity embeddings for GM we run word 1 vec on a version of the corpus in which entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been replaced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their Freebase IDs based on the FACC 1 annotation</example>
		<phraseLemma>np have be replace by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is at &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>BEP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is at the point in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ranked list at which precision and recall have the same value</example>
		<phraseLemma>np be at np in np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we brieﬂy describe &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we brieﬂy describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our choice for these two components</example>
		<phraseLemma>in this section we brieﬂy describe np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; We selected &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Currently BabelNet contains around 1 M synsets and represents the largest single multilingual repository of entities and concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We selected PATTY and WISENET as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; linked resources</example>
		<phraseLemma>np we select np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tend to have &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Alignment reliability decreases for lower δalign as relation pairs where ri is a generalization of rj &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tend to have similar centroids in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; VS The same holds for pairs where ri is the negation of rj</example>
		<phraseLemma>np tend to have np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; out of which &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>Our alignment algorithm produced 1 conﬁdent alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;out of which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 relation synsets were derived with an average size of 1 individual relations per synset</example>
		<phraseLemma>np out of which np</phraseLemma>
	</can>
	<can>
		<phrase>As a result we obtained &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a result we obtained&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a uniﬁed KB comprising 1 disambiguated triples deﬁned over 1 distinct entities and 1 distinct relations</example>
		<phraseLemma>as a result we obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; requires &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Since language model training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;requires a normalization operation each time over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the whole vocabulary which is computationally intensive we further speed up training by using noise contrastive estimation</example>
		<phraseLemma>np require np over np</phraseLemma>
	</can>
	<can>
		<phrase>We collect data from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We collect data from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; subreddits that cover different kinds of topics and vary in community size</example>
		<phraseLemma>we collect datum from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has to be made</phrase>
		<frequency>5</frequency>
		<example>This sentence contains two relation candidates Steven Spielberg” and Alfred Hitchcock” between which the decision for the ﬁnal prediction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has to be made&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np have to be make</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Because the context around Steven Spielberg” is stronger NEC features alone are more likely to indicate that as the correct candidate and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also likely overpower relation features for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁnal prediction as the latter tend to be sparser</example>
		<phraseLemma>np also np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are predicted by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Imitation learning algorithms for structured prediction decompose the prediction task into a sequence of actions these actions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are predicted by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classiﬁers which are trained to take into account the effect of their predictions on the whole sequence by assessing their effect using a loss function on the complete structure predicted</example>
		<phraseLemma>np be predict by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is extracted by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Speciﬁcally as a spatial element for a MOVELINK &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is extracted by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sieve it will be added to the structured feature for the classiﬁer associated with the following sieve</example>
		<phraseLemma>np be extract by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; express &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Five of the optional participants &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;express different aspects of the mover in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; space namely source midpoint goal path and landmark</example>
		<phraseLemma>np express np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as is</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Using this decomposition for MOVELINK instances we can generate instances for each classiﬁer using the aforementioned joint approach as is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np use np as be</phraseLemma>
	</can>
	<can>
		<phrase>We tune &lt;NP&gt; to maximize &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We tune the C and J parameters to maximize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fscore on the development data using the hillclimbing algorithm described earlier</example>
		<phraseLemma>we tune np to maximize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in decreasing order of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Speciﬁcally we compute the precision of each sieve on the development data then add sieves into the pipeline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in decreasing order of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision</example>
		<phraseLemma>np in decrease order of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; in combination</phrase>
		<frequency>5</frequency>
		<example>To evaluate the results for the two SpaceEval tasks we employ the ofﬁcial SpaceEval scoring program which reports results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of recall precision and Fscore on the three types of spatial relations in isolation and in combination&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np in combination</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; We had &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Consider the sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We had only more km to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Cluj taking this way but if getting back to Ciucea and on to Cluj the normal way would have been km longer”</example>
		<phraseLemma>np we have np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are applied by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Together Stage 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are applied by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁrst linking all nodes by edges and following identifying pairs prohibited from being connected and remove the edges along the shortest path between those two nodes effectively creating two new disconnected components in the name graph</example>
		<phraseLemma>np be apply by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was categorized as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The Project Gutenberg ﬁction corpus was dependency parsed to identify all verbs in a dependency relation with nouns where each noun &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was categorized as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a named entity having its ﬁrst sense in WordNet refer to an animate entity or neither of the above</example>
		<phraseLemma>np be categorize as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; creating &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Here coreference resolution frequently creates incorrect links between the similar names of different characters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;creating a drop in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision for most systems</example>
		<phraseLemma>np create np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been considered as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Traditionally words and phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been considered as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document features and subsequently fed to a classiﬁer such as an SVM</example>
		<phraseLemma>np have be consider as np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; that relies on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the SVM dual formulation that relies on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; kernels i e similarity measures between documents a linear kernel can be interpreted as the number of exact matching ngrams between two documents</example>
		<phraseLemma>in np that rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by w</phrase>
		<frequency>5</frequency>
		<example>We denote the embedding of a word w &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by w&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np by w</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; so &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For CK we do not require the two phrases to be of the same length &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;so the kernel has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a desirable property of being able to compare Berlin with capital of Germany for instance</example>
		<phraseLemma>np so np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that maps &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Furthermore we build a parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that maps recipes into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the proposed representation</example>
		<phraseLemma>np that map np into np</phraseLemma>
	</can>
	<can>
		<phrase>We create &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We create a corpus in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our representation by converting the recipes in the CURD corpus from MILK to SIMMR</example>
		<phraseLemma>we create np in np</phraseLemma>
	</can>
	<can>
		<phrase>This is comparable to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is comparable to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the attachment score in dependency parsing</example>
		<phraseLemma>this be comparable to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which are available in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Clearly these prior topics can be represented as sets of words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which are available in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many realworld applications</example>
		<phraseLemma>np which be available in np</phraseLemma>
	</can>
	<can>
		<phrase>We extend &lt;NP&gt; to incorporate &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We extend the PU scheme to incorporate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior topics which gives the sPU scheme</example>
		<phraseLemma>we extend np to incorporate np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; datasets we use &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For both Newsgroups and Reuters datasets we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior knowledge learned by DPMM from the previous days dataset</example>
		<phraseLemma>for np dataset we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; needs &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However NMI &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;needs true class labels for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; documents and can only be applied to our benchmark news datasets</example>
		<phraseLemma>np need np for np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; achieves &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition TSDPMME achieves lower performance than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TSDPMMP due to its lower quality of prior topics directly obtained from CFP” compared to higher quality topics from past learning</example>
		<phraseLemma>in np achieve np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which can be regarded as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Paragraph Vector learns continuous distributed vector representations for pieces of texts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which can be regarded as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a long term memory of sentences as opposed to short memory in recurrent neural network</example>
		<phraseLemma>np which can be regard as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as opposed to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Paragraph Vector learns continuous distributed vector representations for pieces of texts which can be regarded as a long term memory of sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as opposed to short memory in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recurrent neural network</example>
		<phraseLemma>np as oppose to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; are used to train &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Then the documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with category label are used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; classiﬁers</example>
		<phraseLemma>np with np be use to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; this indicates that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>While PAVEM and LDA can be implemented in parallel computation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;this indicates that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PAVEM may be more efﬁcient to obtain themes for a larger set of PubMed documents</example>
		<phraseLemma>np this indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>We focus only on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Since our goal is to distinguish between biographical and non biographical sections &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we focus only on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Wikipedia pages describing persons</example>
		<phraseLemma>np we focus only on np</phraseLemma>
	</can>
	<can>
		<phrase>On the other hand we use &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On the other hand we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the lemma part of speech tag and the dependence type related to the word within a widow around a number as features</example>
		<phraseLemma>on the other hand we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; separately on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We empirically evaluate UrduPhone and our complete method involving LexC &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;separately on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two realworld datasets</example>
		<phraseLemma>np separately on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; back into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To cope with these variations and recover the semantics we match all the radical variants &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;back into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their original forms</example>
		<phraseLemma>np back into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; differently from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We make prediction using a 1 KT jV jdimensional matrix O Different from the original CBOW model the extra parameter introduced in the matrix O allows us to maintain the relative order of the components and treat the radical &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;differently from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the rest components</example>
		<phraseLemma>np differently from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicates that &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Such a pattern &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicates that a character has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a component of X</example>
		<phraseLemma>np indicate that np have np</phraseLemma>
	</can>
	<can>
		<phrase>When training &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When training a classiﬁer for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one label the predictionsasfeatures methods can model dependencies between former labels and the current label but they cant model dependencies between the current label and the latter labels</example>
		<phraseLemma>when training np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; one by one</phrase>
		<frequency>5</frequency>
		<example>Put it another way CC/LEAD provide the partial order structure of classiﬁers and train these classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;one by one&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np one by one</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; most similar to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As described in Section 1 when each node in P represents an NSF program our model can easily identify the programs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;most similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a given program</example>
		<phraseLemma>np most similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; then constructs &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For each of the mention groups Mi C 1 EL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;then constructs&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a context summary using</example>
		<phraseLemma>np then construct np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; enriched with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>So only the mention Wolverine” is added to the SE class and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;enriched with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; KB features</example>
		<phraseLemma>np enriched with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; involve &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However none of these methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;involve the incorporation of CR results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NEL</example>
		<phraseLemma>np involve np for np</phraseLemma>
	</can>
	<can>
		<phrase>The idea is to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The idea is to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; further tune the weight of a single parameter – mention penalty based on the development set after the training process completes</example>
		<phraseLemma>the idea be to np</phraseLemma>
	</can>
	<can>
		<phrase>Our experiments indicate that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our experiments indicate that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; FINET outperforms stateoftheart methods in terms of recall precision and granularity of extracted types</example>
		<phraseLemma>we experiment indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; is associated with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>A popular approach is to train an extractor on a corpus of sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which each named entity is associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all its types in a KB</example>
		<phraseLemma>np in which np be associate with np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we experimented with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Sec 1 we experimented with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both variants and found that KB lookups generally helped</example>
		<phraseLemma>in np we experiment with np</phraseLemma>
	</can>
	<can>
		<phrase>We extend &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We extend the above procedure for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entities tagged as location</example>
		<phraseLemma>we extend np for np</phraseLemma>
	</can>
	<can>
		<phrase>For instance from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Ted Kennedy was elected to Congress” we infer that Ted Kennedy” is a person who can be elected</example>
		<phraseLemma>for instance from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represents &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>A word vector is a semantic rep resentation of a phrase and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represents the semantic context in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the phrase occurs</example>
		<phraseLemma>np represent np in which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are most similar to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Given an integer k word 1 vec outputs the set of k phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are most similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the query</example>
		<phraseLemma>np that be most similar to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; all of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our example query all of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top persons share type hcoach 1 i a strong indication that Maradona may also be of type hcoach 1 i</example>
		<phraseLemma>in np all of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is based on &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Our type selection phase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is based on WSD a classiﬁcation task where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words or phrases are disambiguated against senses from some external resource such as WordNet</example>
		<phraseLemma>np be base on np where np</phraseLemma>
	</can>
	<can>
		<phrase>However we believe that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However we believe that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; joint optimization is a promising direction for improving performance for NLP tasks since it is closer to how human beings process text information</example>
		<phraseLemma>however we believe that np</phraseLemma>
	</can>
	<can>
		<phrase>As shown in Figure 1 &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As shown in Figure 1 our models factor graph is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a tree which means the calculation of the gradient is tractable</example>
		<phraseLemma>as show in figure 1 np be np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; also uses &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Addition NereL also uses&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; UIUC NER to generate mentions</example>
		<phraseLemma>in np also use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have also been applied to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In addition to the applications mentioned above many neural network based methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have also been applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural language processing tasks with great success</example>
		<phraseLemma>np have also be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>They train &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They train a Latent Dirichlet Allocation model using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; documents consisting of about sentences long text from Penn Treebank training data</example>
		<phraseLemma>they train np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which captures &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>And then it stores the sentence history &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which captures coherence of sentences in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a realvalued history vector</example>
		<phraseLemma>np which capture np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which look at &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Current language models are usually ngram models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which look at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the previous words to predict the nth word in a sequence based on counts of ngrams collected from training data</example>
		<phraseLemma>np which look at np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate our model using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our model using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the opensource NPLM toolkit released by Vaswani extending it to use the additional regularizers as described in this paper</example>
		<phraseLemma>we evaluate we model use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; can have</phrase>
		<frequency>5</frequency>
		<example>Discourse parsing is a difﬁcult multifaceted problem involving the understanding and modeling of various semantic and pragmatic phenomena as well as understanding the structural properties &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that a discourse graph can have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np that np can have</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as given in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Further all computational work on the PDTB takes the attachments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as given in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; discourse parsing tasks</example>
		<phraseLemma>np as give in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been used to train &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The dialogue act annotations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an automatic classiﬁer for EDUs</example>
		<phraseLemma>np have be use to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; both &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Li use &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;both the Eisner algorithm as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MST algorithm from McDonald</example>
		<phraseLemma>np both np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; for &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train base classiﬁers for the role function and central claim level using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same learning regime as described in Section 1</example>
		<phraseLemma>we train np for np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; predict &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As before we ﬁrst tune the hyperparameters in the inner CV train the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the whole training data and predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; probabilities on all items of the test set</example>
		<phraseLemma>np on np predict np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; slightly better than &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Note that the EG model with equal weighting scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;slightly better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the one with optimized weighting for German but not for English</example>
		<phraseLemma>np slightly better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; would beneﬁt from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In contrast all levels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;would beneﬁt from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a better function classiﬁcation most importantly even the attachment classiﬁcation</example>
		<phraseLemma>np would beneﬁt from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; discussed &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We mentioned major standalone monolingual aligners and brieﬂy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;discussed their working principles in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np discuss np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are shared across &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In contrast a FrameNet frame often associates with multiple lexical units and the frame lexicon contains several hundred core and noncore roles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are shared across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; frames</example>
		<phraseLemma>np that be share across np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that does not rely on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Very recently Zhou and Xu proposed a deep bidirectional LSTM model for SRL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that does not rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntax trees as input their approach achieves the best results on CoNLL 1 and corpora to date but unlike this work they do not report results on FrameNet and CoNLL 1 dependencies and do not investigate joint learning approaches involving multiple annotation conventions</example>
		<phraseLemma>np that do not rely on np</phraseLemma>
	</can>
	<can>
		<phrase>To ensure &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To ensure a fair comparison with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the closest linear model baseline we ensured that the preprocessing steps the argument candidate generation algorithm for the spanbased datasets and the frame identiﬁcation methods are identical to Täckström</example>
		<phraseLemma>to ensure np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are closely related to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In this paper we will make use of several concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are closely related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hypernymy which we deﬁne below</example>
		<phraseLemma>np that be closely related to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which we describe in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The collection of rules deﬁnes the PSL model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which we describe in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 and Table 1</example>
		<phraseLemma>np which we describe in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are omitted from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>When the YAGO type hierarchy rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are omitted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model coverage is reduced dramatically the resulting hypernymy graph contains only 1 hypernymy links in contrast to the 1 links in the original model</example>
		<phraseLemma>np be omit from np</phraseLemma>
	</can>
	<can>
		<phrase>The experiments were performed on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The experiments were performed on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a multicore 1 GHz server with 1 GB of RAM</example>
		<phraseLemma>the experiment be perform on np</phraseLemma>
	</can>
	<can>
		<phrase>We collected &lt;NP&gt; consisting of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We collected a dataset consisting of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; movie plot summaries from two different websites Wikipedia and the Internet Movie Database</example>
		<phraseLemma>we collect np consist of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are represented by &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the bigram model documents are represented by vectors in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the bagofbigrams model with bigram frequency weights</example>
		<phraseLemma>in np be represent by np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are given &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We propose a crowdsourcing task where Turkers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Amazon Mechanical Turk platform are given&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sarcastic utterances and are asked to rephrase those messages so that they convey the authors intended meaning</example>
		<phraseLemma>np in np be give np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to determine &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To avoid any bias during experiments we removed the target words from the tweets as well as any hashtag &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to determine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sense of the tweet</example>
		<phraseLemma>np use to determine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; give &lt;NP&gt; compared to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Among the word embedding models word 1 vec models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;give marginally better results compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; GloVe and WTMF and GloVe outperforms marginally WTMF</example>
		<phraseLemma>np give np compare to np</phraseLemma>
	</can>
	<can>
		<phrase>We did &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We did a small manual validation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on a dataset of tweets from the Lsent class using 1 annotators</example>
		<phraseLemma>we do np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is considered as &lt;NP&gt; of</phrase>
		<frequency>5</frequency>
		<example>If the number of Web search results is greater than a threshold Ψ t 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is considered as a synonym of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be consider as np of</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; as follows</phrase>
		<frequency>5</frequency>
		<example>Similar to the collective synonym evidence the contrastive evidence score of taxonomic relation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between and is boosted with the contrastive evidence scores of taxonomic relations between the two terms and their synonyms as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np between np as follow</phraseLemma>
	</can>
	<can>
		<phrase>However &lt;NP&gt; is limited to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However their coverage is limited to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; common wellknown areas and many speciﬁc domains like Finance and AI are not well covered in those structures</example>
		<phraseLemma>however np be limit to np</phraseLemma>
	</can>
	<can>
		<phrase>In our experiments we found that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our experiments we found that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the threshold values for Ψ between and and those for ScoreF inal between 1 and 1 generally help C ombined the system achieve the best performance</example>
		<phraseLemma>in we experiment we find that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been successfully applied in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Maximum entropy classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been successfully applied in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many natural language processing applications and allows the inclusion of various sources of information without necessarily assuming any independence between the features</example>
		<phraseLemma>np have be successfully apply in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; arises due to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The difﬁculty &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;arises due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the restrictions enforced in the translation models used by the existing semantic parsers</example>
		<phraseLemma>np arise due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; separates &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Also the proposed approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;separates the representation from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reasoning</example>
		<phraseLemma>np separate np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; being indexed as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In Fig 1 the original English sentence had words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;being indexed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 1 1 1 1 1 1 1 1</example>
		<phraseLemma>np be index as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is described in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The rev function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1 determines whether the subsequence from sstart to send should be reversed by examining the related alignment Asub</example>
		<phraseLemma>np which be describe in np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; for training &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For GermanEnglish translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we used the Europarl corpus for training&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WMT 1 / WMT 1 test sets for development / testing respectively</example>
		<phraseLemma>np we use np for training np</phraseLemma>
	</can>
	<can>
		<phrase>We set &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For the the proposed approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we set δ = 1 and M = 1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1</example>
		<phraseLemma>np we set np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; comparable with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>With the proposed approach fast align obtained results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;comparable with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; GIZA and its efﬁciency is retained</example>
		<phraseLemma>np comparable with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in a variety of &lt;NP&gt; including &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As a pragmatic solution human intervention is commonly used for improving automatic draft translations in socalled postediting but is also studied earlier in the translation process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a variety of interactive strategies including&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; eg completion assistance and local translation choices</example>
		<phraseLemma>np in a variety of np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may be attributed to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The lesser amplitude of the gains obtained after 1 iterations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may be attributed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the higher initial quality of the translations in the medical task</example>
		<phraseLemma>np may be attribute to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will focus on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In terms of usability our future work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two important questions</example>
		<phraseLemma>np will focus on np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; the size of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In such architecture the size of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; output vocabulary is a bottleneck when normalized distributions are expected</example>
		<phraseLemma>in np the size of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be written as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Our loss function L is deﬁned with respect to this critical set and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be written as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the following expresion</example>
		<phraseLemma>np can be write as np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are used to train &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the adaptation scenario large outofdomain corpora are used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline SMT system while the CTM is trained on a much smaller indomain corpus and only serves for rescoring</example>
		<phraseLemma>in np be use to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For both baselines backbone hypotheses are selected sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by sentence based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; systemweighted consensus among translation of all MT systems</example>
		<phraseLemma>np by np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; e to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For each f in a corpus the machine generates a hypothesis e then a human provides a corrected translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;e to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the machine</example>
		<phraseLemma>np e to np</phraseLemma>
	</can>
	<can>
		<phrase>On &lt;NP&gt; we observed &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On all tasks we observed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; statistically signiﬁcant improvements over the baseline 1 conﬁdence level in the genre TM doc</example>
		<phraseLemma>on np we observe np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; predict &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We represent both the reference and the translation using an LSTM and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;predict the similarity score y based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a neural network which considers both distance and angle between href and htra</example>
		<phraseLemma>np predict np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; acts as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In other words the corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;acts as a scoring function for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the available reference translation pairs which gives a similarity score between a reference and a translation</example>
		<phraseLemma>np act as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shows the results on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Dimensions are shown in brackets eg L &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows the results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; set L with the hidden dimension and the memory dimension</example>
		<phraseLemma>np show the result on np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; have &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For TreeLSTM models different parameter settings have only a minor impact on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; performance and LSick results are statistically signiﬁcantly different</example>
		<phraseLemma>for np have np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; improves over &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>More notably the addition of the newly proposed features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;improves over the best QE systems in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WMT 1 and WMT 1 by a signiﬁcant margin</example>
		<phraseLemma>np improve over np in np</phraseLemma>
	</can>
	<can>
		<phrase>We propose the use of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we propose the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CSLM features for QE</example>
		<phraseLemma>np we propose the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as features along with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In this paper we propose to estimate the probabilities of source and target segments with continuous space language models based on a deep architecture and to use these estimated probabilities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as features along with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard feature sets in a supervised learning framework</example>
		<phraseLemma>np as feature along with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to extract &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The resources &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these features are also available as part of the WMT shared tasks on QE</example>
		<phraseLemma>np use to extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produce &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>They proved complementary when used together with other feature sets and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produce comparable results to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; high performing baseline features when used alone for prediction</example>
		<phraseLemma>np produce np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not used as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However the triangulated phrase table T is obtained without using the sourcetarget bilingual data which suggests that the sourcetarget data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fully as it could be</example>
		<phraseLemma>np be not use as np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we let &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the normalization factor Zs we let&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; t range only over possible translations of s suggested by either w or the triangulated word probabilities</example>
		<phraseLemma>in np we let np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only over &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In the normalization factor Zs we let t range &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; possible translations of s suggested by either w or the triangulated word probabilities</example>
		<phraseLemma>np only over np</phraseLemma>
	</can>
	<can>
		<phrase>We are left with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This is because after taking the log &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we are left with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a weighted sum of linear and concave terms in A and h</example>
		<phraseLemma>np we be leave with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shall know &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The distributional hypothesis perhaps best stated as You &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shall know a word by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the company it keeps” has had a long and productive history as well as a recent revival in neuralnetworkbased models</example>
		<phraseLemma>np shall know np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that outperform &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We show experimentally that this results in vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that outperform prior work on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multilingual tasks and match the performance of prior work on monolingual tasks</example>
		<phraseLemma>np that outperform np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is performed during &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In the 1 type of strategy which we will call incremental decoding the segmentation process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is performed during&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the decoding of the input stream</example>
		<phraseLemma>np be perform during np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been extracted from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>As an example suppose that rule introduced in Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been extracted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training example in Figure 1</example>
		<phraseLemma>np have be extract from np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; implemented in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the version implemented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Moses open source toolkit with standard parameters</example>
		<phraseLemma>we use np implement in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by randomly selecting &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Since this is a parallel corpus only we ﬁrst removed duplicate sentences and then constructed development and test sets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by randomly selecting&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence pairs</example>
		<phraseLemma>np by randomly select np</phraseLemma>
	</can>
	<can>
		<phrase>We measured &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We measured the overall translation quality with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 gram BLEU which was computed on tokenized and lowercased data for all systems</example>
		<phraseLemma>we measure np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may refer to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The term domain” has a wide interpretation in the MT literature and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may refer to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic dialect genre or style</example>
		<phraseLemma>np may refer to np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; described by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the multilingual parser described by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; AitMokhtar to preprocess the texts and extract a wide range of features</example>
		<phraseLemma>we use np describe by np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we download &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each talk we download&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the manual French and English subtitles ie the transcript and the translation respectively</example>
		<phraseLemma>for np we download np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; should be used for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This suggests that alternative methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;should be used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our task</example>
		<phraseLemma>np should be use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is detrimental to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Although linguistic signals of traits are weaker in the latter case so far it appears that machine translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is detrimental to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the automatic recognition of these traits</example>
		<phraseLemma>np be detrimental to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; across languages</phrase>
		<frequency>5</frequency>
		<example>The idea is to separately train two sets of wordembeddings for each language and then to do a parametric estimation of the mapping &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between wordembeddings across languages&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np between np across language</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as computed by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Figure 1 compares the markedness of the PDTB level 1 relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as computed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Mexp and Mall measures</example>
		<phraseLemma>np as compute by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that while &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Our ﬁndings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deception detection can be performed in short texts even in the absence of a predetermined domain gender and age prediction in deceptive texts is a challenging task</example>
		<phraseLemma>np show that while np</phraseLemma>
	</can>
	<can>
		<phrase>We aim to build &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We aim to build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deception age and gender classiﬁers using short texts and also explore the prediction of gender and age in deceptive content</example>
		<phraseLemma>we aim to build np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showed that the use of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Psycholinguistics lexicons such as Linguistic Inquiry and Word Count have also been used to build deception models using machine learning approaches and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showed that the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic information is helpful for the automatic identiﬁcation of deceit</example>
		<phraseLemma>np show that the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performed poorly in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However our classiﬁers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performed poorly in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the age prediction task with accuracies below the majority class baseline</example>
		<phraseLemma>np perform poorly in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that suggested that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>For instance spontaneous lies often include negation certain and you words which is in line with previous work on domainspeciﬁc deception &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that suggested that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; liars try to reinforce their lies through the use of stronger wording and detachment from the self</example>
		<phraseLemma>np that suggest that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is expressed as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Phonotactic knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is expressed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of wordform templates represented as sequences of phonological classes</example>
		<phraseLemma>np be express as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are not available in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To the best of our knowledge template classes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are not available in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other semantic representation languages</example>
		<phraseLemma>np be not available in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; plays a critical role in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In this paper equations and inequations are called sexpressions because they represent mathematical sentences The semantic interpretation of DOL nodes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;plays a critical role in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the algorithm</example>
		<phraseLemma>np play a critical role in np</phraseLemma>
	</can>
	<can>
		<phrase>This approach leads to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This approach leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more useful rules with fewer undesirable properties</example>
		<phraseLemma>this approach lead to np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne the probability of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We deﬁne the probability of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a fullsentence AMR i as PAMR where ROOT in this case serves as both parent concept and role label</example>
		<phraseLemma>we deﬁne the probability of np</phraseLemma>
	</can>
	<can>
		<phrase>According to the principle of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;According to the principle of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; compositionality the meaning of a sentence is computed from the meaning of its parts and the way they are syntactically combined</example>
		<phraseLemma>accord to the principle of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; corresponding to &lt;NP&gt; containing &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For instance if the network is used for sentiment analysis local features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;corresponding to kgrams containing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the word like” should receive high values in order to be propagated to the top layer</example>
		<phraseLemma>np correspond to np contain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to give &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Therefore a good solution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to give&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the system a set of parses and let it decide which parse is the best or to combine some of them</example>
		<phraseLemma>np be to give np</phraseLemma>
	</can>
	<can>
		<phrase>At &lt;NP&gt; we can see that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At the lexical level we can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the FCN can discriminate important words from the others</example>
		<phraseLemma>at np we can see that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by taking into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Ma extend the work of Kim &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by taking into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; acount dependency relations so that long range dependencies could be captured</example>
		<phraseLemma>np by take into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not able to capture &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However because only one gate is allowed to open in a cell the network &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not able to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an arbitrary forest</example>
		<phraseLemma>np be not able to capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; has been &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Learning edit distance/monotone alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in an unsupervised manner has been&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the topic of eg Ristad and Yianilos Cotterell besides the works already mentioned</example>
		<phraseLemma>np in np have be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; possibly because &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>However we have also found that the importance of good alignments on G 1 P accuracy appears to dimish as data set size increases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;possibly because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the translation modules can accomodate more noisy data in this scenario</example>
		<phraseLemma>np possibly because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; by using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Finally we report experimental results showing that our method increases the accuracy of a word segmenter &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on Twitter texts by using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; logs collected from a browser addon version of our IM</example>
		<phraseLemma>np on np by use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; extend it to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For that purpose we adopt the notion of a stochastically segmented corpus and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;extend it to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pronunciation annotation to words</example>
		<phraseLemma>np extend it to np</phraseLemma>
	</can>
	<can>
		<phrase>This means that &lt;NP&gt; have &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This means that the IM logs have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a potential to reduce errors caused by OOV words</example>
		<phraseLemma>this mean that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provides an alternative to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Intuitively the MaxMargin criterion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provides an alternative to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; probabilistic likelihood based estimation methods by concentrating directly on the robustness of the decision boundary of a model</example>
		<phraseLemma>np provide a alternative to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the rest &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In addition we use the 1 1 sentences of the training data as training set and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the rest&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 sentences as development set for PKU and MSRA datasets</example>
		<phraseLemma>np the rest np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which suffer from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Characterbased Alignment To account for named entities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which suffer from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sparsity and thus make it difﬁcult to calculate the probabilities discussed above we introduce a transliteration feature to evaluate the similarities between the pronunciations of Chinese and English words because many NEs are translated via transliteration</example>
		<phraseLemma>np which suffer from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; demonstrating that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>It can be seen that we achieved signiﬁcant improvement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in performance when we combined the characterlevel and phraselevel features in the inner loglinear model demonstrating that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the proposed phraselevel features can be used to efﬁciently obtain bilingual segmenting information</example>
		<phraseLemma>np in np demonstrate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can provide &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>During the sampling process each training iteration draws a different derivation tree for each sentence pair and the combination of those different derivation trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can provide multiple possible phrase boundaries to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model</example>
		<phraseLemma>np can provide np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; relied on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Surprisingly exhaustive extraction had no gains probably because of the word alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in each Hiero rules relied on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the IBM Model 1</example>
		<phraseLemma>np in np rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; which result in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Unfortunately optimizing with respect to phrase count is prone to yield alignments with very few links &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a biased way which result in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a large number of bilingual phrases extracted from a small fraction of the training data</example>
		<phraseLemma>np in np which result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; aligns &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In contrast BCorrRAE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;aligns sourceside nodes to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their corresponding targetside nodes according to word alignments</example>
		<phraseLemma>np align np to np</phraseLemma>
	</can>
	<can>
		<phrase>We randomly extracted &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We randomly extracted 1 bilingual phrases from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the abovementioned training data as training set 1 1 as development set and another 1 1 as test set</example>
		<phraseLemma>we randomly extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; on average</phrase>
		<frequency>5</frequency>
		<example>BCorrRAEST is better than BCorrRAESM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by 1 BLEU points on average&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np by np on average</phraseLemma>
	</can>
	<can>
		<phrase>In a way &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In a way&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the regularizers in our loss functions are inspired from the data selection methods of Axelrod where they use cross entropy between the in and the outdomain LMs to score outdomain sentences</example>
		<phraseLemma>in a way np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; causing &lt;NP&gt; to fail</phrase>
		<frequency>5</frequency>
		<example>The error gradients propagated by the backpropagation may sometimes become very small or very large which can lead to undesired values in weight matrices &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;causing the training to fail&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np cause np to fail</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on which &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>To score outdomain sequences using a model we need to generate the sequences using the same vocabulary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model was trained</example>
		<phraseLemma>np base on which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that come from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>A solution to this problem is to have an indomain model that can differentiate between its own unk class resulted from the reduced indomain vocabulary and actual unknown words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that come from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the outdomain data</example>
		<phraseLemma>np that come from np</phraseLemma>
	</can>
	<can>
		<phrase>We carried out &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We carried out further experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the EnglishtoGerman pair to validate our models</example>
		<phraseLemma>we carry out np on np</phraseLemma>
	</can>
	<can>
		<phrase>We were able to obtain &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We were able to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an average gain of 1 BLEU points by training an NDAMv 1 model over the selected data</example>
		<phraseLemma>we be able to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>The difference in &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The difference in BLEU scores is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a strong indicator whether these sentences are challenging for MT systems</example>
		<phraseLemma>the difference in np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using the number of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Therefore as our baseline we train a decision tree &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words in a Chinese sentence</example>
		<phraseLemma>np use the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shown to improve &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Dependency grammar captures both syntactic and semantic relationship between words and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shown to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reordering in MT</example>
		<phraseLemma>np be show to improve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we construct &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To capture the transition of each phrase and clause &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the sentence we construct&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; functional POS trigrams for each sentence by removing all nouns verbs adjectives adverbs numbers and pronouns in the sentence</example>
		<phraseLemma>np in np we construct np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We train a logistic regression model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in the parallel method in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 using features illustrated above</example>
		<phraseLemma>np as in np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shows that &lt;NP&gt; yields &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This overall performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shows that forward selection yields&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a suboptimal feature set suggesting that the other features are also informative</example>
		<phraseLemma>np show that np yield np</phraseLemma>
	</can>
	<can>
		<phrase>In order to incorporate &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to incorporate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reordering into outside string we propose a new topdown algorithm in the next subsection</example>
		<phraseLemma>in order to incorporate np</phraseLemma>
	</can>
	<can>
		<phrase>If we only consider &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If we only consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the partial derivations in the beam it is still hard to promote it</example>
		<phraseLemma>if we only consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; playing the role of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Each such language takes a turn &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;playing the role of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target language for testing purposes</example>
		<phraseLemma>np play the role of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is in Figure 1</phrase>
		<frequency>5</frequency>
		<example>The distribution of token in each bible in the unit of a language &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in terms of both &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We demonstrate improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; full morphological analysis choice as well as word diacritization</example>
		<phraseLemma>np in term of both np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; s &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Inspired by Habash &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;s simple set of rules for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; determining case on gold dependency trees we reimplemented these rules to work with our different dependency representation and extended them to include state assignment in a manner similar to Alkuhlani 1 These rules improve over the baseline by 1 absolute in Nominal Diac accuracy but produce no gains in All Words setting</example>
		<phraseLemma>np s np for np</phraseLemma>
	</can>
	<can>
		<phrase>An investigation of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;An investigation of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the error patterns reveals the main reason to be the rules inability to predict the problematic u case</example>
		<phraseLemma>a investigation of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; 1 and &lt;NP&gt; respectively</phrase>
		<frequency>5</frequency>
		<example>Our logistic regression linear parser and reimplementation of Chen and Manning give comparable accuracies to the perceptron ZPar &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;1 and Stanford NN Parser respectively&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np 1 and np respectively</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are converted to &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>WSJ constituent trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are converted to dependency trees using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Penn 1 Malt 1</example>
		<phraseLemma>np be convert to np use np</phraseLemma>
	</can>
	<can>
		<phrase>We studied &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We studied the combination of discrete and continuous features for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deterministic transitionbased dependency parsing comparing several methods to incorporate word embeddings and traditional sparse features in the same model</example>
		<phraseLemma>we study np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that achieved &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>It should be noted that the best results for languages in CoNLL are not from one single system but different systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that achieved best results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different languages</example>
		<phraseLemma>np that achieve np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; feature &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The problem is formalized as tree node classiﬁcation and we ﬁnd that the path &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;feature the sequence of node labels from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current node to the root is highly effective</example>
		<phraseLemma>np feature np from np</phraseLemma>
	</can>
	<can>
		<phrase>Experiments show that &lt;NP&gt; outperforms &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Experiments show that the proposed method outperforms&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the previous stateof the art method by 1 to 1 in terms of Fmeasure</example>
		<phraseLemma>experiment show that np outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; there have been &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In the last ﬁve years &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;there have been several shared tasks in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; grammatical error correction including the Helping Our Own shared tasks of and and the CoNLL 1 and shared tasks</example>
		<phraseLemma>np there have be np in np</phraseLemma>
	</can>
	<can>
		<phrase>We present &lt;NP&gt; to improve &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We present a semisupervised approach to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependency parsing accuracy by using bilexical statistics derived from autoparsed data</example>
		<phraseLemma>we present np to improve np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For outofdomain data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use the Brown portion of the PTB as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the testsets of different domains available in the Google Web Treebank</example>
		<phraseLemma>np we use np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as they are &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We focus on ﬁrstorder parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as they are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most practical graphbased parsers in terms of running time in realistic parsing scenarios</example>
		<phraseLemma>np as they be np</phraseLemma>
	</can>
	<can>
		<phrase>Among &lt;NP&gt; that use &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Among the words that use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unannotated data the dominant approach is to derive either word clusters or word vectors based on unparsed data and use these as additional features for a supervised parsing model</example>
		<phraseLemma>among np that use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; see &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For details &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on our experimental setup see&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1</example>
		<phraseLemma>np on np see np</phraseLemma>
	</can>
	<can>
		<phrase>It is worth pointing out that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is worth pointing out that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Gesmundo is itself a neural net parser</example>
		<phraseLemma>it be worth point out that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to normalize &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Given these scores a natural way to obtain weights &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to normalize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the probabilities</example>
		<phraseLemma>np be to normalize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by 1 on average</phrase>
		<frequency>5</frequency>
		<example>For the selftrained BLLIP parser indomain increases by 1 and outofdomain increases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by 1 on average&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np by 1 on average</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; with &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate fusion&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; with the Berkeley parser on Arabic French and German from the SPMRL 1 shared task but did not observe any improvement</example>
		<phraseLemma>we evaluate np with np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; act as &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>It is possible that a parsers nlist and its scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;act as a weak approximation to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the full parse forest</example>
		<phraseLemma>np act as np to np</phraseLemma>
	</can>
	<can>
		<phrase>This means that for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This means that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the remaining 1 fusion picks an existing parse from the rest of the nlist acting similar to a reranker</example>
		<phraseLemma>this mean that for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by summing over &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>On the other hand the continuous bagofwords model requires no additional parameters as it builds the context representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by summing over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embeddings in the window and its performance is an order of magnitude higher than of other models</example>
		<phraseLemma>np by sum over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; which uses &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The work in attempts to infer POS tags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a standard bigram hmm which uses&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings to infer POS tags without supervision</example>
		<phraseLemma>np with np which use np</phraseLemma>
	</can>
	<can>
		<phrase>It has been shown in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It has been shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that word embeddings learnt from structured skipngrams tend to work better at this task mainly because it is less sensitive to larger window sizes</example>
		<phraseLemma>it have be show in np</phraseLemma>
	</can>
	<can>
		<phrase>These results are consistent with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;These results are consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our observations found in Table 1 in rows Skipngram” and SSkipngram”</example>
		<phraseLemma>these result be consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; generally leads to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The gap between the usage of different embeddings is not as large as in POS induction as this is a supervised task where pretraining &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;generally leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; smaller improvements</example>
		<phraseLemma>np generally lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that with &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Goldberg and Nivre &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the monotonic arceager actions the following arcs are reachable from an arbitrary conﬁguration</example>
		<phraseLemma>np show that with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; similar to &lt;NP&gt; where &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We employ the dynamic oracle in an online learning strategy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;similar to imitationbased learning where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the examples are conﬁgurations produced by following the current models predictions</example>
		<phraseLemma>np similar to np where np</phraseLemma>
	</can>
	<can>
		<phrase>After &lt;NP&gt; we have &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;After paraphrase extraction we have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; paraphrase pairs and a score S we can induce new translation rules for OOV phrases using the steps in Algo</example>
		<phraseLemma>after np we have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to assign &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Considering the translation candidates of known phrases in the SMT phrase table as the labels” we apply a soft label propagation algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to assign&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation candidates to unlabeled” nodes in the graph which include our OOV phrases</example>
		<phraseLemma>np in order to assign np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; can be viewed as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Optimization for the Modiﬁed Adsorption objective function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Sec 1 can be viewed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a controlled random walk</example>
		<phraseLemma>np in np can be view as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compare our results to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We use their setup and data and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compare our results to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their reported results</example>
		<phraseLemma>np compare we result to np</phraseLemma>
	</can>
	<can>
		<phrase>We propose &lt;NP&gt; to obtain &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose a weighted mean value and a minimum distance method to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nonterminal representations from representations of their phrasal substitutions</example>
		<phraseLemma>we propose np to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We incorporate both source/targetside semantic nonterminal reﬁnement model and their combination &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on learned nonterminal representations into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation system</example>
		<phraseLemma>np base on np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; applied &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>They use these distributions to decorate nonterminal Xs in SCFG rules with a realvalued feature vectors and utilize these vectors to measure the similarities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between source phrases and applied&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rules</example>
		<phraseLemma>np between np apply np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we detail &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we detail&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the essential component of our approach ie how to learn semantic vectors for nonterminals and how to project source semantic vectors onto target language semantic space</example>
		<phraseLemma>in this section we detail np</phraseLemma>
	</can>
	<can>
		<phrase>By minimizing &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By minimizing the total reconstruction error over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all nonterminal nodes we can learn parameters of RAE</example>
		<phraseLemma>by minimize np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by learning &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Different from mapping representations from the source side to the target side &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by learning a linear matrix on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word alignments we project source multiword phrase representations onto the target semantic space in a nonlinear manner as we believe that nonlinear relations between languages are more reasonable</example>
		<phraseLemma>np by learn np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are tuned by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The weights of these two features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are tuned by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Minimum Error Rate Training together with weights of other submodels on a development set</example>
		<phraseLemma>np be tune by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is extracted from &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The monolingual corpus which was used to pretrain word embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is extracted from the above parallel corpus in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SMT</example>
		<phraseLemma>np be extract from np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which was used by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In order to compare our proposed models with previous methods on nonterminal reﬁnement we reimplemented a syntax mismatch model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which was used by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Huang and integrated it into hierarchical phrasebased system</example>
		<phraseLemma>np which be use by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; to measure &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>SynMis model decorates each nonterminal with a distribution of head POS tags and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses this distribution to measure&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the degree of syntactic compatibility of translation rules with corresponding source spans</example>
		<phraseLemma>np use np to measure np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to exploit &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>While the focus of this work is on the development and comparison of the models the longterm goal is to decode using JTR models without the limitations introduced by phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to exploit&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the full potential of JTR models</example>
		<phraseLemma>np in order to exploit np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; combine &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Nevertheless JTR models utilize linear sequences of dependencies and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;combine the translation of bilingual word pairs and reoderings into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single model</example>
		<phraseLemma>np combine np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; jointly in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Contrastingly JTR models incorporate target information as well and predict both translations and reorderings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;jointly in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single framework</example>
		<phraseLemma>np jointly in np</phraseLemma>
	</can>
	<can>
		<phrase>We achieve &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>With local attention &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we achieve a signiﬁcant gain of 1 BLEU points over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nonattentional systems that already incorporate known techniques such as dropout</example>
		<phraseLemma>np we achieve np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are effective in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Experimentally we demonstrate that both of our approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are effective in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the WMT translation tasks between English and German in both directions</example>
		<phraseLemma>np be effective in np</phraseLemma>
	</can>
	<can>
		<phrase>Given &lt;NP&gt; provided by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given the gold alignment data provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; RWTH for EnglishGerman Europarl sentences we force” decode our attentional models to produce translations that match the references</example>
		<phraseLemma>give np provide by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; to produce &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Speciﬁcally we try CNN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with multiple convolutional ﬁlters of different widths to produce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence representation</example>
		<phraseLemma>np with np to produce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in this part</phrase>
		<frequency>5</frequency>
		<example>We present a gated recurrent neural network approach for document composition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in this part&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in this part</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; followed by &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The transition function is typically a linear layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;followed by pointwise nonlinearity layer such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tanh</example>
		<phraseLemma>np follow by np such as np</phraseLemma>
	</can>
	<can>
		<phrase>We can see that &lt;NP&gt; yield &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can see that the proposed method ConvGRNN and LSTMGRNN yield&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the best performance on all 1 datasets in two evaluation metrics</example>
		<phraseLemma>we can see that np yield np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; each of which has &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Each word in the input layer is represented by M features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;each of which has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an embedding vector associated with it in a lookup table</example>
		<phraseLemma>np each of which have np</phraseLemma>
	</can>
	<can>
		<phrase>Therefore the number of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Therefore the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parameters remains the same as the unidirectional one</example>
		<phraseLemma>therefore the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are 1 on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Similar gains are also observed on the folds in Table 1 where the maximum gains &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Laptop and 1 on Restaurant</example>
		<phraseLemma>np be 1 on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; before training</phrase>
		<frequency>5</frequency>
		<example>We also ﬁlter infrequent features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;before training&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np before training</phraseLemma>
	</can>
	<can>
		<phrase>Previous studies show that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Previous studies show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic parsing with synchronous contextfree grammars achieves favorable performance over most other alternatives</example>
		<phraseLemma>previous study show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is inferior to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Surprisingly gdfa alignment which is widely adopted in SMT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is inferior to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tgt 1 src alignment</example>
		<phraseLemma>np be inferior to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; increases the size of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Firstly splitting nonterminal X into enriched ones &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;increases the size of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrase tables</example>
		<phraseLemma>np increase the size of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are problematic for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Again both diagrams and small corpora &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are problematic for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this body of work</example>
		<phraseLemma>np be problematic for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are shown in Table 1 for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The features for the unary and binary models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are shown in Table 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the text t and the relation ri</example>
		<phraseLemma>np be show in table 1 for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are modeled in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Figure 1 shows how implications and coordinating conjunctions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are modeled in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the intermediate representation</example>
		<phraseLemma>np be model in np</phraseLemma>
	</can>
	<can>
		<phrase>To account for &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To account for the different scales between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; A and H we use the tradeoff parameter λ in Equation 1 learned on the validation dataset</example>
		<phraseLemma>to account for np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; with &lt;NP&gt; of 1</phrase>
		<frequency>5</frequency>
		<example>Finally we present a computational model which disambiguates the sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in our corpus with an accuracy of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np with np of 1</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we also provide &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For syntactic and discourse ambiguities we also provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an additional ambiguity type speciﬁc encoding as described below</example>
		<phraseLemma>for np we also provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; computes the score of &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Provided an interpretation and its corresponding formula composed of P predicates and V variables along with a collection of object detections b in each frame of a video of length T the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;computes the score of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the videosentence pair by ﬁnding the optimal detection for each participant in every frame</example>
		<phraseLemma>np compute the score of np</phraseLemma>
	</can>
	<can>
		<phrase>We show experimentally that &lt;CL&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show experimentally that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this technique gives substantially better performance than PRA and its variants improving mean average precision from to on a knowledge base completion task using the NELL KB</example>
		<phraseLemma>we show experimentally that np</phraseLemma>
	</can>
	<can>
		<phrase>We tuned &lt;NP&gt; for &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We tuned the and parameters for each method on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a random development split of the data then used a new split of the data to run the ﬁnal tests presented here</example>
		<phraseLemma>we tune np for np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is treated as &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Each such path &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is treated as a separate relation in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a combined knowledge graph including both KB and textual relations</example>
		<phraseLemma>np be treat as np in np</phraseLemma>
	</can>
	<can>
		<phrase>Following &lt;NP&gt; in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Following prior work in latent feature models for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; knowledge base completion every textual relation receives its own continuous representation learned from the pattern of its cooccurrences in the knowledge graph</example>
		<phraseLemma>follow np in np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was shown to outperform &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This model was proposed in Yang and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was shown to outperform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior work on the FB 1 k dataset</example>
		<phraseLemma>np be show to outperform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is assigned &lt;NP&gt; In &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In this model each entity e and each relation r &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is assigned a latent feature vector of dimension K In&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the basic models knowledge base and textual relations are treated uniformly and each textual relation receives its own latent representation of dimensionality K</example>
		<phraseLemma>np be assign np in np</phraseLemma>
	</can>
	<can>
		<phrase>We rank &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We rank all entities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training knowledge base in order of their likelihood of ﬁlling the argument position</example>
		<phraseLemma>we rank np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will be represented by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>In the following a document d &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will be represented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a vector</example>
		<phraseLemma>np will be represent by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provides &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Additionally the corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provides metadata such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; authors venue and incommunity citation networks</example>
		<phraseLemma>np provide np such as np</phraseLemma>
	</can>
	<can>
		<phrase>In Figure 1 we plot &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Figure 1 we plot&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the median of κ for authors at different ages”</example>
		<phraseLemma>in figure 1 we plot np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; measured &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>For instance Gerrish and Blei &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;measured scholarly impact using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dynamic topic models while Hall analyzed the output of topic models to study the history of ideas”</example>
		<phraseLemma>np measure np use np</phraseLemma>
	</can>
	<can>
		<phrase>This approach is similar to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This approach is similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our earlier work where authors are rational agents writing texts to maximize the chance of a favorable decision by a judicial court</example>
		<phraseLemma>this approach be similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; are used</phrase>
		<frequency>5</frequency>
		<example>Evaluation is performed by computing the perplexities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over the test data and the parameters that yield the highest perplexity over the development data are used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np over np be use</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; that uses &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Once again we replace OOV words with an unknown token &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the setup that uses&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word lookup tables and the same with OOV characters in the C 1 W model</example>
		<phraseLemma>np in np that use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been presented by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>A ﬁrst attempt to test these observations in a deep compositional setting &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been presented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Cheng with promising results</example>
		<phraseLemma>np have be present by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; takes place in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>While treating disambiguation as only a preprocessing step is a strategy less than optimal for a neural setting one would expect that the beneﬁts should be greater for an architecture in which the disambiguation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;takes place in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dynamic fashion during training</example>
		<phraseLemma>np take place in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is covered in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The issue of lexical ambiguity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is covered in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 Section 1 below deals with generic training and syntactic awareness</example>
		<phraseLemma>np be cover in np</phraseLemma>
	</can>
	<can>
		<phrase>To avoid &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To avoid the exploding or vanishing gradient problem for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; long sentences we employ a long shortterm memory network</example>
		<phraseLemma>to avoid np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; obtained when using &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>We see that despite the large size of the training set the results are much lower than the ones &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;obtained when using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pretraining step</example>
		<phraseLemma>np obtain when use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even across &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>By categorizing posts into such topics we can provide a useful division of content in a thread and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple threads</example>
		<phraseLemma>np even across np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; proposed by &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Rewriting Systems To ﬁnd the most likely parse tree &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use the parsing algorithm proposed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Kallmeyer and Maier for binary PLCFRS</example>
		<phraseLemma>np we use np propose by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; suggests that in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>This ﬁnding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;suggests that in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any thread of reasonable size which we wish to summarize or categorize nonprojective edges will be common</example>
		<phraseLemma>np suggest that in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is required in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>An optimized sampler &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is required in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; such situations because common topics in weaklycorrelated collections are usually found in the tail of the documenttopic distribution of a sufﬁciently large set of topics</example>
		<phraseLemma>np be require in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; which use &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>Prioritizing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;inmodel solutions for documenttopic asymmetry has been explored elsewhere such as in hierarchical Dirichlet processes which use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an additional level to account for collection variations in documenttopic distributions</example>
		<phraseLemma>np in np which use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; assumes &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>To achieve this CLDA &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;assumes document d in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; collection c has a multinomial documenttopic distribution with an asymmetric Dirichlet prior for K topics where the ﬁrst K are common across collections</example>
		<phraseLemma>np assume np in np</phraseLemma>
	</can>
	<can>
		<phrase>We propose to use &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose to use a feature mapping operation based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tensor products instead of linear operations on stacked vectors</example>
		<phraseLemma>we propose to use np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; compared to &lt;NP&gt;</phrase>
		<frequency>5</frequency>
		<example>The best hyperparameter conﬁguration in this task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses less feature layers and lower ngram order compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentiment classiﬁcation task</example>
		<phraseLemma>np use np compare to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; that capture &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This framework enables us to map text descriptions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into vector representations that capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the semantics of the game states</example>
		<phraseLemma>np into np that capture np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that act as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The ﬁrst one converts textual descriptions into vector representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that act as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; proxies for states</example>
		<phraseLemma>np that act as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in learning &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this section we describe our model and describe its use &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in learning good Qvalue approximations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; games with stochastic textual descriptions</example>
		<phraseLemma>np in learn np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that converts &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The ﬁrst module is a representation generator &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that converts the textual description of the current state into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a vector</example>
		<phraseLemma>np that convert np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; converts it to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Representation Generator The representation generator reads raw text displayed to the agent and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;converts it to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a vector representation vs A bagofwords representation is not sufﬁcient to capture higherorder structures of sentences and paragraphs</example>
		<phraseLemma>np convert it to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not sufﬁcient to capture &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Representation Generator The representation generator reads raw text displayed to the agent and converts it to a vector representation vs A bagofwords representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not sufﬁcient to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; higherorder structures of sentences and paragraphs</example>
		<phraseLemma>np be not sufﬁcient to capture np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; that provide &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For instance rare transitions that provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive rewards can be used more often to learn optimal Qvalues faster</example>
		<phraseLemma>for np that provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The motivation behind Home world &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to abstract away highlevel planning and focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the language understanding requirements of the game</example>
		<phraseLemma>np be to np on np</phraseLemma>
	</can>
	<can>
		<phrase>At the end of &lt;NP&gt; we have &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At the end of this training we have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a testing phase of running M episodes of the game for T steps</example>
		<phraseLemma>at the end of np we have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uniformly at random from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The ﬁrst is a Random agent that chooses both actions and objects &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uniformly at random from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all available choices</example>
		<phraseLemma>np uniformly at random from np</phraseLemma>
	</can>
	<can>
		<phrase>From Figure 1 we can see that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From Figure 1 we can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Random baseline does poorly in terms of both average perepisode reward and quest completion rates</example>
		<phraseLemma>from figure 1 we can see that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; performs very well on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The BIDQN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;performs very well on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; quest completion by ﬁnishing 1 of quests</example>
		<phraseLemma>np perform very well on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; learnt from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We initialized the LSTM part of an LSTMDQN agent &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with parameters θR learnt from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original home world and trained it on the new world</example>
		<phraseLemma>np with np learn from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained it on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We initialized the LSTM part of an LSTMDQN agent with parameters θR learnt from the original home world and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained it on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the new world</example>
		<phraseLemma>np train it on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; produced by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Table 1 shows some examples of descriptions from Fantasy world and their nearest neighbors using cosine similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between their corresponding vector representations produced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LSTMDQN</example>
		<phraseLemma>np between np produce by np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows a sample of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows a sample of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the attributes that FreeBase records for countries</example>
		<phraseLemma>table 1 show a sample of np</phraseLemma>
	</can>
	<can>
		<phrase>It has &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It has range 1 with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; smaller numbers indicating better ranking</example>
		<phraseLemma>it have np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; match those of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The relative patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;match those of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accuracybased evaluation well Countries</example>
		<phraseLemma>np match those of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In NLP Mikolov &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that a linear mapping between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information</example>
		<phraseLemma>np show that np between np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; which consists of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As a source corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use a concatenation of the ukWaC a dump of the English Wikipedia and the BNC 1 which consists of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; about 1 billion tokens</example>
		<phraseLemma>np we use np which consist of np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we do not have &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our account we do not have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an a priori model of the world</example>
		<phraseLemma>in np we do not have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; tells us that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>This vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;tells us that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set of horses includes the set of mammals and that the set of horses and the set of things that are scaly are disjoint</example>
		<phraseLemma>np tell we that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is close to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We discover that in many cases the mapped vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is close to a similar concept in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the gold standard but not &amp;gt to itself</example>
		<phraseLemma>np be close to np in np</phraseLemma>
	</can>
	<can>
		<phrase>In Table 1 we provide &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Table 1 we provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top weighted features for a small set of concepts</example>
		<phraseLemma>in table 1 we provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; reaching &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We found that &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a relatively cheap” linear function – cheap in that it is easy to learn and requires modest training data – we can reproduce the quantiﬁers in our gold annotation with high correlation reaching&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human performance on a domainspeciﬁc test set</example>
		<phraseLemma>np with np reach np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; out of context</phrase>
		<frequency>4</frequency>
		<example>The focus of this paper was conceptpredicate pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;out of context&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np out of context</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in terms of &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The resulting translation model has the same order of magnitude &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of time complexity with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hierarchical phrasebased model under a certain restriction</example>
		<phraseLemma>np in term of np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as in &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this paper we build a dependency graphtostring model so we only use one nonterminal symbol X &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as in HPB on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target side</example>
		<phraseLemma>np as in np on np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However on the source side &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we deﬁne nonterminal symbols over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PartofSpeech tags which can be easily obtained as a byproduct of dependency parsing</example>
		<phraseLemma>np we deﬁne np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; which have &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The translation of a large span can be obtained by combining translations from its subspan &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using rules which have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nonterminals</example>
		<phraseLemma>np use np which have np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; results from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition translation results from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a recently opensource dependency treetostring system Dep 1 Str 1 which is implemented in Moses and improves the dependencybased model in Xie are also reported</example>
		<phraseLemma>in np result from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learned for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Subsequently we split these coarse labels in the same way as latent variable splitting &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learned for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; treebank parsing</example>
		<phraseLemma>np be learn for np</phraseLemma>
	</can>
	<can>
		<phrase>Extract &lt;NP&gt; from &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Extract a PCFG from the PETs with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; initial nonterminals taken from the PETs</example>
		<phraseLemma>extract np from np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We decompose the permutation πm into a forest of permutation trees P EF in O following algorithms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; trivial modiﬁcations</example>
		<phraseLemma>np in with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to translate &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A common strategy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to translate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; independently translatable segments as soon as possible</example>
		<phraseLemma>np be to translate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is hampered by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A datadriven approach to learning these rewriting rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is hampered by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dearth of parallel data</example>
		<phraseLemma>np be hamper by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; superior to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Nevertheless RW is comparable to GD on gold references and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;superior to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline on rewritten references</example>
		<phraseLemma>np superior to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; identify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We present a novel application of contextual sentiment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with this task and identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several semisupervised learning algorithms that are needed to address the reference resolution challenge inherent to country names</example>
		<phraseLemma>np with np identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; to measuring &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To our knowledge this paper is the largest computational approach months &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with 1 billion tweets to measuring&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; international relations on social media</example>
		<phraseLemma>np with np to measure np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is required to make &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>It seems reasonable to infer that they are negative toward the country as a whole but a deeper reasoning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is required to make&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the connection</example>
		<phraseLemma>np be require to make np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicate if &lt;NP&gt; appears in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Two binary features postivemood and negativemood &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicate if a token appears in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sentiment lexicons positive or negative list</example>
		<phraseLemma>np indicate if np appear in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on how &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Each year GlobeScan/PIPA releases polling data of nations in a ranked ordering &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on how&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 people view their positive contribution” to the world</example>
		<phraseLemma>np base on how np</phraseLemma>
	</can>
	<can>
		<phrase>We compare &lt;NP&gt; to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our sentiment ratios to these pairs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top of Table 1</example>
		<phraseLemma>we compare np to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; attains &lt;NP&gt; of 1</phrase>
		<frequency>4</frequency>
		<example>In a tenfold crossvalidation this classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;attains an accuracy rate of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np attain np of 1</phraseLemma>
	</can>
	<can>
		<phrase>In order to establish &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to establish&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; realvalued scores for propositions we follow the same method as for the single membership model described above using the log likelihood ratio of the probability of the proposition under each condition where that probability is given as the count of the proposition among users classiﬁed as liberals divided by the total number of propositions used by them overall</example>
		<phraseLemma>in order to establish np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we pick &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For ten iterations we pick&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a random sample U 1 of 1 data points from the full dataset U and classify each using the two classiﬁers each classiﬁer then adds up to of the highestconﬁdence predictions to the training set retaining the class distribution balance of the initial training set after training the ﬁnal predictive probability for an item is the product of the two trained classiﬁers</example>
		<phraseLemma>for np we pick np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; as for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We calculate scores for propositions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the same way as for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the fully supervised case above</example>
		<phraseLemma>np in np as for np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to compare &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the absolute value of ρ to compare&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the degree to which the ranked propositions of two lists are linearly correlated a perfect correlation would have ρ = 1 no correlation would have ρ = 1</example>
		<phraseLemma>we use np to compare np</phraseLemma>
	</can>
	<can>
		<phrase>In addition &lt;NP&gt; used &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition most previous studies used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; psychometric surveys which are impractical in mass marketing since it is unlikely that a large number of customers would take the time to answer lengthy survey questions</example>
		<phraseLemma>in addition np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we asked &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For each brand &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in each category we asked&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; users to rate their preferences using a 1 point scale</example>
		<phraseLemma>np in np we ask np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are inferred from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Since personal traits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are inferred from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the text authored by a user we discarded all the retweets</example>
		<phraseLemma>np be infer from np</phraseLemma>
	</can>
	<can>
		<phrase>Were then used to build &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The LIWC counts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were then used to build&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prediction models to correlate ones word usage with his ground truth personal traits obtained via a prior psychometric survey</example>
		<phraseLemma>np be then use to build np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; varied from &lt;NP&gt; to another</phrase>
		<frequency>4</frequency>
		<example>But their effectiveness &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;varied from one brand to another&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np vary from np to another</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we trained &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To predict the rank of a product &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in each category we trained&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a multiclass classiﬁer to estimate how 1 Experiment 1 likely a user will like a brand</example>
		<phraseLemma>np in np we train np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; is &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If the coefﬁcient ρ is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 there is a perfect positive correlation between the predict rank and the ground truth</example>
		<phraseLemma>if np be np be np</phraseLemma>
	</can>
	<can>
		<phrase>We compute &lt;NP&gt; across &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compute the average ρ across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the users and all the product categories to represent the baseline performance</example>
		<phraseLemma>we compute np across np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; compared to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By exploiting temporal information from the Wikipedia edit history and page view logs we have improved the annotation performance by 1 as compared to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the competitive baselines</example>
		<phraseLemma>np by np compare to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; leave &lt;NP&gt; to future work</phrase>
		<frequency>4</frequency>
		<example>In this study we focus on the precision and the popular trending hashtags and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;leave the improvement of recall to future work&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np leave np to future work</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learned through &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The ranking function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learned through&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an unsupervised model and needs no humandeﬁned labels</example>
		<phraseLemma>np be learn through np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; mentions of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This measure relies on the explicit &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;mentions of entities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets</example>
		<phraseLemma>np mention of np in np</phraseLemma>
	</can>
	<can>
		<phrase>We build &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For the hashtags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we build the time series based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the volume of tweets adopting the hashtag h on each day in T</example>
		<phraseLemma>np we build np base on np</phraseLemma>
	</can>
	<can>
		<phrase>It measures &lt;NP&gt; between &lt;NP&gt; by ﬁnding &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It measures the distance between two time series by ﬁnding&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; optimal shifting and scaling parameters to match the shape of two time series</example>
		<phraseLemma>it measure np between np by ﬁnding np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; especially for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This is another advantage of noncontent measures such as ft For the 1 group of baselines we also observe the reduction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in precision especially for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Kauri</example>
		<phraseLemma>np in np especially for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as to whether &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To investigate this case further we reexamined the hashtags and divided them by their semantics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as to whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the hashtags are spurious trends of memes inside social media or whether they reﬂect external events</example>
		<phraseLemma>np as to whether np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we address &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we address&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the new problem of topically annotating a trending hashtag using Wikipedia entities which has many important applications in social media analysis</example>
		<phraseLemma>in this work we address np</phraseLemma>
	</can>
	<can>
		<phrase>This is related to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the growing body of research in global optimization which selects the most informative subset of sentences towards a global objective</example>
		<phraseLemma>this be related to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; within &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This system optimizes the coverage of bigrams weighted by their document frequency &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;within the input using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Integer Linear Programming</example>
		<phraseLemma>np within np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; appears in</phrase>
		<frequency>4</frequency>
		<example>We also include features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that compute the information density of the ﬁrst sentence that each word appears in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np that np appear in</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using the method described in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We derive TWs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the method described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the LLRSum system in Section 1</example>
		<phraseLemma>np use the method describe in np</phraseLemma>
	</can>
	<can>
		<phrase>We compute &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For each ngram t &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we compute its probability TFIDF 1 document frequency and χsquare statistic from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LLR test</example>
		<phraseLemma>np we compute np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; normalized by the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Another feature is set to be equal to DF &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;normalized by the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input documents</example>
		<phraseLemma>np normalize by the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been used for &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In order to ﬁnd a better learning method we have experimented with support vector regression 1 and SVMRank 1 SVR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been used for estimating sentence or document importance in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; summarization</example>
		<phraseLemma>np have be use for np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieve &lt;NP&gt; by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>On the TAC 1 data the top performing systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieve the stateoftheart performance by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentence compression</example>
		<phraseLemma>np achieve np by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; comparable to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Overall our combination model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves very competitive performance comparable to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart on multiple benchmarks</example>
		<phraseLemma>np achieve np comparable to np</phraseLemma>
	</can>
	<can>
		<phrase>Second we show the performance of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Second we show the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a single feature class</example>
		<phraseLemma>1 we show the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that combines &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this paper we present a pipeline &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that combines the summaries from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 portable unsupervised summarizers</example>
		<phraseLemma>np that combine np from np</phraseLemma>
	</can>
	<can>
		<phrase>We describe &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Summarization Inspired by the general idea of phrasebased machine translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we describe our proposed phrasebased model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; crosslanguage summarization in this section</example>
		<phraseLemma>np we describe np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; along with &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the context of crosslanguage summarization here we assume that we can also have phrases in both source and target languages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;along with phrase alignments between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two sides</example>
		<phraseLemma>np along with np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are inherited from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Based on the deﬁnition over sentences we deﬁne our summary scoring measure over a summary S where d is a predeﬁned constant damping factor to penalize repeated occurrences of the same phrases count is the number of occurrences in the summary S for phrase p All other terms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are inherited from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence score deﬁnition</example>
		<phraseLemma>np be inherit from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; measured by the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We use C to denote the cost of a summary S &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;measured by the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Chinese characters contained in total</example>
		<phraseLemma>np measure by the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is designed to be &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The scoring function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is designed to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document frequencies of English bigrams which is similar to the 1 term in our proposed sentence scoring function in Section 1 and is submodular</example>
		<phraseLemma>np be design to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which gives &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We reimplement the graphbased CoRank algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which gives the stateoftheart performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same DUC 1 dataset for comparison</example>
		<phraseLemma>np which give np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; to improve &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Their following work incorporate various constraints &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on constituent parse trees to improve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the linguistic quality of the compressed sentences</example>
		<phraseLemma>np on np to improve np</phraseLemma>
	</can>
	<can>
		<phrase>We provide an analysis of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We provide an analysis of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; current evaluation methodologies applied to summarization metrics and identify the following areas of concern</example>
		<phraseLemma>we provide a analysis of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when used for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As described in Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the evaluation of metrics linguistic quality is commonly omitted however with metrics only assessed by the degree to which they correlate with human coverage scores</example>
		<phraseLemma>np when use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; as follows</phrase>
		<frequency>4</frequency>
		<example>Human coverage scores are computed by combining Matching PUs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with coverage estimates as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np as follow</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 is &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 is a scatterplot of human coverage scores and corresponding linguistic quality scores for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all humanassessed summaries from DUC 1 where for the purpose of comparison each of the 1 linguistic quality ratings are converted to a corresponding percentage quality</example>
		<phraseLemma>figure 1 be np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;CL&gt; that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>Contrary to prior belief the vast majority of optimal ROUGE variants are precisionbased showing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that the assumption that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recallbased metrics are superior for evaluation of summarization systems to be inaccurate and a likely presence of bias in favor of recallbased metrics in evaluations by correlation with human coverage scores alone</example>
		<phraseLemma>np that np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to rank &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Since we have established that the variants of ROUGE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to rank&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stateoftheart and baseline summarization systems in Hong have signiﬁcantly weaker correlations with human assessment than several other ROUGE variants this motivates our replication of the evaluation</example>
		<phraseLemma>np use to rank np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; achieves &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition BLEU achieves strongest correlation with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human assessment overall but does not signiﬁcantly outperform the bestperforming ROUGE variant</example>
		<phraseLemma>in np achieve np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; who use &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Other studies using Twitter data include OConnor &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;who use topic summarization for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a given search for better browsing</example>
		<phraseLemma>np who use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as it contains &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The dataset of Lloret and Palomar is an exception &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as it contains&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets and the news articles they link to but it only contains English tweetarticle pairs</example>
		<phraseLemma>np as it contain np</phraseLemma>
	</can>
	<can>
		<phrase>Since we are interested in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Since we are interested in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text articles that can serve as the source text for summarization algorithms we needed to remove photos and video links such as those from Instagram and YouTube</example>
		<phraseLemma>since we be interested in np</phraseLemma>
	</can>
	<can>
		<phrase>In addition we consider &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; locality within the article when computing the overlap</example>
		<phraseLemma>in addition we consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; above with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To tie in the results of the ﬁndings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;above with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some intuitive notions about the text and see how formality interacts with the results we also calculated the formality of the articles</example>
		<phraseLemma>np above with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; a series of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>They tend to be relatively deep consisting of a number of rectiﬁed linear unit layers and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;a series of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; convolutional layers</example>
		<phraseLemma>np a series of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has led to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Although the idea of transferring CNN features is not new the simultaneous availability of massive amounts of data and cheap GPUs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has led to considerable advances in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; computer vision similar in scale to those witnessed with SIFT and HOG descriptors a decade ago</example>
		<phraseLemma>np have lead to np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; are grounded in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>It has been found &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that semantic knowledge from a very early age relies heavily on perceptual information and there exists substantial evidence that many concepts are grounded in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the perceptual system</example>
		<phraseLemma>np that np be ground in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; ranked &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We use Google Images to extract the top n &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;ranked images for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each lexical item in the evaluation datasets</example>
		<phraseLemma>np rank np for np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we specify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each Google search we specify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the target language corresponding to the lexical items language</example>
		<phraseLemma>for np we specify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is accompanied with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The test set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is accompanied with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; comparable data for training for the three language pairs ES/IT/NLEN on which textbased models for bilingual lexicon induction were trained</example>
		<phraseLemma>np be accompany with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compare the results to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We evaluate the 1 similarity metrics on the BERGSMA 1 dataset and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compare the results to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the systems of Bergsma and Van Durme who report results for the AVGMAX function having concluded that it performs better than MAXMAX on EnglishSpanish translations</example>
		<phraseLemma>np compare the result to np</phraseLemma>
	</can>
	<can>
		<phrase>The results in Table 1 indicate that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results in Table 1 indicate that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the perimage CNNAVGMAX metric outperforms the aggregated visual representationbased metrics of CNNMEAN and CNNMAX despite the fact that Kiela and Bottou achieved optimal performance using the latter metrics on a wellknown conceptual relatedness dataset</example>
		<phraseLemma>the result in table 1 indicate that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; rather than in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If however we are interested in relatedness related properties may just as well be encoded deeper in the network so in the layers preceding FC 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;rather than in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; FC 1 itself</example>
		<phraseLemma>np rather than in np</phraseLemma>
	</can>
	<can>
		<phrase>Although &lt;NP&gt; are available for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Although concreteness ratings are available for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English words this is not the case for other languages so in order to examine the concreteness of the datasets we use a substitute method that has been shown to closely mirror how abstract a concept is</example>
		<phraseLemma>although np be available for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; are used</phrase>
		<frequency>4</frequency>
		<example>For Languages where labeled data is not present approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on crosslingual sentiment analysis are used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np base on np be use</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as features for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>present an alternative approach to Cross Lingual Sentiment Analysis &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using WordNet senses as features for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervised sentiment classification</example>
		<phraseLemma>np use np as feature for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was tested for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A document in Resource Poor Language &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was tested for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; polarity through a classifier trained on sense marked and polarity labeled corpora in Resource rich language</example>
		<phraseLemma>np be test for np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; were generated using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this approach features were generated using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntagmatic property based word clusters created from unlabeled monolingual corpora thereby eliminating the need for Bilingual Dictionaries</example>
		<phraseLemma>in np be generate use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; serves as &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This matrix is learned using DNN and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;serves as input to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; further stages of RAE</example>
		<phraseLemma>np serve as np to np</phraseLemma>
	</can>
	<can>
		<phrase>To assess how well &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To assess how well&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parent vector represents its children the autoencoder reconstructs the chilGiven y 1 Eq is used again to compute by setting the children to be =</example>
		<phraseLemma>to assess how well np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; between them</phrase>
		<frequency>4</frequency>
		<example>Sentiments At the end of previous Training procedure we obtain high quality phrase embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in both source and target language and transformation function between them&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np between they</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to perform &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In each training phase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use SGD algorithm to perform&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parameter optimization</example>
		<phraseLemma>np we use np to perform np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used as features in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Each word in a review was then mapped to its cluster identifier and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used as features in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an SVM</example>
		<phraseLemma>np use as feature in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was calculated by averaging &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Final accuracy for each size &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was calculated by averaging&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the accuracies obtained on all samples</example>
		<phraseLemma>np be calculate by average np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by comparing it with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Since we restricted the movement in semantic vector space our model was able to infer the sentiment for a unknown word/phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by comparing it with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantically similar words/phrases</example>
		<phraseLemma>np by compare it with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are suboptimal for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However twostage approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are suboptimal for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text summarization</example>
		<phraseLemma>np be suboptimal for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that there exists &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Another advantage of choosing monotone submodular function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that there exists&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a polynomialtime greedy algorithm for constrained monotone submodular objective</example>
		<phraseLemma>np be that there exist np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are clustered in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the algorithm the sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are clustered in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different partitions corresponding to different aspects in the ontology tree using the clue words</example>
		<phraseLemma>np be cluster in np</phraseLemma>
	</can>
	<can>
		<phrase>While &lt;NP&gt; was at &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While much early work was at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the document or sentence level to fully understand and utilize opinions researchers are increasingly carrying out more ﬁnegrained sentiment analysis to extract components of opinion frames</example>
		<phraseLemma>while np be at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; would be valuable in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A system that could recognize sentiments toward entities and events &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;would be valuable in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an application such as Automatic Question Answering to support answering questions such as Who is negative/positive toward X?”</example>
		<phraseLemma>np would be valuable in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; would be valuable in &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A system that could recognize sentiments toward entities and events &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;would be valuable in an application such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Automatic Question Answering to support answering questions such as Who is negative/positive toward X?”</example>
		<phraseLemma>np would be valuable in np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as revealed by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Imam is negative toward Salman Rushdie and the insulting event &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as revealed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the expression issued the fatwa against</example>
		<phraseLemma>np as reveal by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; above in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Using the source and the target we summarize the positive opinions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;above in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set P and the negative opinions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;above in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; another set N</example>
		<phraseLemma>np above in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are often &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While the targets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in aspectbased sentiment analysis are often&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entity targets they are mainly product aspects which are a predeﬁned set</example>
		<phraseLemma>np in np be often np</phraseLemma>
	</can>
	<can>
		<phrase>In fact under &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In fact under&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our new rules the targets of sentiments may be other sentiments we model such novel sentiment toward sentiment” structures in Section 1</example>
		<phraseLemma>in fact under np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that employs &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Probabilistic Soft Logic is a variation of Markov Logic Networks which is a framework for probabilistic logic &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that employs weighted formulas in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁrstorder logic to compactly encode complex undirected probabilistic graphical models</example>
		<phraseLemma>np that employ np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; takes as input &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>PSL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;takes as input&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the local scores as well as the constraints deﬁned by the rules among atoms so that it is able to jointly resolve all the ambiguities</example>
		<phraseLemma>np take as input np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are &lt;NP&gt; according to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The score of EFFECT is the fraction of that words senses &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are effect senses according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the lexicon and the score of EFFECT is the fraction of that words senses &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are effect senses according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the lexicon</example>
		<phraseLemma>np that be np accord to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into &lt;NP&gt; so &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If an opinion expression in the gold standard is not extracted by any spanbased system it is not input &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into PSL so&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PSL cannot possibly ﬁnd its eTargets</example>
		<phraseLemma>np into np so np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; according to the scores</phrase>
		<frequency>4</frequency>
		<example>We rank the eTargets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;according to the scores&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np accord to the score</phraseLemma>
	</can>
	<can>
		<phrase>We consider it as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If the top N eTargets of an opinion contain the head of target span &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we consider it as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a correct hit</example>
		<phraseLemma>np we consider it as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; annotated with &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We also present a data set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;annotated with affective polarity in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similes and experiment with both manually annotated and automatically acquired training data</example>
		<phraseLemma>np annotated with np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; ranging from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our ﬁrst training data set is created using the AFINN sentiment lexicon containing 1 manually labeled words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with integer values ranging from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; to 1</example>
		<phraseLemma>np with np range from np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; is labeled as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If the simile is labeled as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive or negative then it is assigned that label</example>
		<phraseLemma>if np be label as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained only with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Row shows the results for a baseline classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained only with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unigram features</example>
		<phraseLemma>np train only with np</phraseLemma>
	</can>
	<can>
		<phrase>With &lt;NP&gt; we created &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;With the crawler we created&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a benchmark dataset which is fully annotated with crossdocument coreferential events</example>
		<phraseLemma>with np we create np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; associated with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To compute the similarity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between videos associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two candidate event mentions we sample multiple frames from each video and aggregate the similarity scores of the few most similar image pairs between the videos</example>
		<phraseLemma>np between np associate with np</phraseLemma>
	</can>
	<can>
		<phrase>It is reasonable to assume that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is reasonable to assume that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most frequent face cluster is the one corresponding to the anchor faces</example>
		<phraseLemma>it be reasonable to assume that np</phraseLemma>
	</can>
	<can>
		<phrase>We apply &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As CC consists of capitalized letters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we apply the truecasing tool from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Standford CoreNLP on CC</example>
		<phraseLemma>np we apply np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that provides &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We also build a crawler &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that provides a benchmark dataset of videos with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; aligned closed captions</example>
		<phraseLemma>np that provide np with np</phraseLemma>
	</can>
	<can>
		<phrase>We categorize &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We categorize the available datasets into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three major classes and evaluate them against these criteria</example>
		<phraseLemma>we categorize np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; found in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We find evidence that the VQA dataset captures more abstract concepts than other datasets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with almost 1 of the words found in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our abstract concept resource</example>
		<phraseLemma>np with np find in np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; map &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In image description generation work Kulkarni manually map spatial relations to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predeﬁned prepositions whilst Yang predict prepositions from largescale text corpora solely based on the complement term with the prepositions constrained to describing scenes</example>
		<phraseLemma>in np map np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; found &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We explored the role of geometric textual and visual features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in learning to predict a preposition given two bounding box instances in an image and found&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; clear evidence that all three features play a part in the task</example>
		<phraseLemma>np in np find np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; play &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We explored the role of geometric textual and visual features in learning to predict a preposition given two bounding box instances in an image and found clear evidence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that all three features play&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a part in the task</example>
		<phraseLemma>np that np play np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even though there is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Distributional Semantic Models have become standard paraphernalia in the natural language processing toolbox and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even though there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a wide variety of models available the basic parameters of DSMs are now well understood</example>
		<phraseLemma>np even though there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; tagging &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This approach achieves state oftheart performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a German POS tagging&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; task</example>
		<phraseLemma>np on np tagging np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as reﬂected by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This task is difﬁcult because German is a morphologically rich language &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as reﬂected by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the large number of morphological tags in our study yielding a grand total of more than POSMORPH tags</example>
		<phraseLemma>np as reﬂected by np</phraseLemma>
	</can>
	<can>
		<phrase>Experiments are carried out on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Experiments are carried out on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the PartofSpeech and Morphological tagging tasks using the German corpus TIGER Treebank</example>
		<phraseLemma>experiment be carry out on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; respectively on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We further analyze the results by looking at the error rates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;respectively on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; known and unknown words</example>
		<phraseLemma>np respectively on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that were unseen in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>These models can therefore efﬁciently process words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that were unseen in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data</example>
		<phraseLemma>np that be unseen in np</phraseLemma>
	</can>
	<can>
		<phrase>We plan to extend &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the future &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we plan to extend&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these models to other tasks such as syntactic parsing and machine translation</example>
		<phraseLemma>np we plan to extend np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; needed to store &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The initial experiments give promising results indicating that the method is able to increase language modelling accuracy while also decreasing the parameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;needed to store&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model along with the computation required at each step</example>
		<phraseLemma>np need to store np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that occur less than times in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Any words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that occur less than times in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data were replaced by a special token for unknown words leaving a vocabulary of 1 unique words</example>
		<phraseLemma>np that occur less than time in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was ﬁxed at &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>General learning rate was set to 1 and decreased during training whereas the learning rate of the document vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was ﬁxed at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 for both training and testing</example>
		<phraseLemma>np be ﬁxed at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; also has &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The graph of perplexity with respect to additional operations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the model also has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a very similar shape</example>
		<phraseLemma>np in np also have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was achieved in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The lowest perplexity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was achieved in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the region of D = 1 and M = 1 and making the document vectors much smaller or larger led to a decrease in performance</example>
		<phraseLemma>np be achieve in np</phraseLemma>
	</can>
	<can>
		<phrase>In addition to &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition to words we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bigrams since they have been shown previously to be effective features for this task</example>
		<phraseLemma>in addition to np we use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that transforms &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>W is a d × h matrix &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that transforms the word embedding to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hidden representation inputs</example>
		<phraseLemma>np that transform np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as where &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We deﬁne the hyperplane of the label a with its normal vector W &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as where φ is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same mapping to d dimensional word vectors that is used above in the utterance representation Wih is a 1 d × h matrix and Who is a h × h matrix</example>
		<phraseLemma>np as where np be np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we randomly select &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In each iteration of the stochastic training algorithm we randomly select&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an utterance and its labels as positive examples and choose randomly another utterance with a different label as a negative example</example>
		<phraseLemma>in np we randomly select np</phraseLemma>
	</can>
	<can>
		<phrase>We randomly select &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In each iteration of the stochastic training algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we randomly select an utterance and its labels as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; positive examples and choose randomly another utterance with a different label as a negative example</example>
		<phraseLemma>np we randomly select np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; since it provides &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This biased negative sampling speeds up the training process &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;since it provides&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more difﬁcult training examples to the learner</example>
		<phraseLemma>np since it provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to address &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to address&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the adaptivity issues because the utterance and the dialogue act representations are in the same space using the same shared parameters φ which are initialised with unsupervised word embeddings</example>
		<phraseLemma>np be able to address np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is very important in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>SLU recall &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is very important in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the overall dialogue system performance as the effect of a missed dialogue act is hard to handle for the Interaction Manager</example>
		<phraseLemma>np be very important in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisting of a total of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This results in rumours &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisting of a total of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tweets</example>
		<phraseLemma>np consist of a total of np</phraseLemma>
	</can>
	<can>
		<phrase>We query &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We query the model for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a complete set of times of posts about rumour i in the future one hour time interval</example>
		<phraseLemma>we query np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be solved using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The problem of modeling the interarrival times of tweets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be solved using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Poisson processes</example>
		<phraseLemma>np can be solve use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is obtained by taking &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The probability density function of the random variable Tn &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is obtained by taking&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the derivative of with respect to u</example>
		<phraseLemma>np be obtain by take np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; predicted by &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This is because the interarrival times &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;predicted by GPLIN for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; several rumours become smaller as time grows resulting in a large number of arrival times</example>
		<phraseLemma>np predict by np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; cannot be used for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However this technique &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;cannot be used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hidden layers beyond the ﬁrst</example>
		<phraseLemma>np can not be use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are mapped into &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In a standard feedforward embeddingbased neural network the input tokens &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are mapped into a continuous vector using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an embedding table and this embedding vector is fed into the ﬁrst hidden layer</example>
		<phraseLemma>np be map into np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when using &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For evaluation we present both the model perplexity and the BLEU score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when using the model as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an additional MT feature</example>
		<phraseLemma>np when use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; integrating &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We introduce a topic model for link prediction based on the intuition that linked documents will tend to have similar topic distributions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;integrating a maxmargin&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; learning criterion and lexical term weights in the loss function</example>
		<phraseLemma>np integrate np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; prior over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Speciﬁcally we propose a joint model that uses link structure to deﬁne clusters of documents and following the intuition that documents in the same cluster are likely to have similar topic distributions assigns each cluster its own separate Dirichlet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;prior over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the clusters topic distribution</example>
		<phraseLemma>np prior over np</phraseLemma>
	</can>
	<can>
		<phrase>The results conﬁrm that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results conﬁrm that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our model outperforms both LDA and MRTF and that its use of user interactions holds promise</example>
		<phraseLemma>the result conﬁrm that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; without &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this paper we propose a new alignment model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on text descriptions of entities without&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependency on anchors</example>
		<phraseLemma>np base on np without np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; via &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train their models via&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our own implemen tities in KB ew” means the tail side is a word out of KB entity vocabulary similarly for we” and ww”</example>
		<phraseLemma>we train np via np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; released by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the same public dataset NYTFB released by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Riedel and used in and</example>
		<phraseLemma>we use np release by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by selecting &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The system forms the output summary &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by selecting a subset of the sentences in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the input collection that does not exceed a ﬁxed wordlength limit</example>
		<phraseLemma>np by select np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are rare in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This procedure – corpus downsampling – ensures that all words in the six data sets &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are rare in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the corpus and that our setup directly evaluates the impact of distributional initialization on rare words</example>
		<phraseLemma>np be rare in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also achieves &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>RTRANSE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also achieves signiﬁcantly better performances than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TRANSE on this new dataset</example>
		<phraseLemma>np also achieve np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; only applies to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The ﬁrst criterion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;only applies to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; original facts of the KB while the 1 term applies to quadruples</example>
		<phraseLemma>np only apply to np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; to initialize &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>On FB 1 K &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we used the embeddings of TRANSE to initialize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; RTRANSE and we set a learning rate of 1 to ﬁnetune RTRANSE</example>
		<phraseLemma>np we use np to initialize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; suggest that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>All in all the results show that considering paths during training very signiﬁcantly improves performances and the results on triples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with composition suggest that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; RTRANSE is indeed capable of capturing the evidence of links that exist in longer paths</example>
		<phraseLemma>np with np suggest that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; removes &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Filtering training instances &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by item agreement removes&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; low agreement instances from the training set</example>
		<phraseLemma>np by np remove np</phraseLemma>
	</can>
	<can>
		<phrase>We found &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We found a statistically signiﬁcant beneﬁt from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lowagreement training ﬁltering in 1 of our ﬁve tasks and strongest improvements for Hard Cases</example>
		<phraseLemma>we find np from np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 presents the results on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 presents the results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different datasets for the six embedding models</example>
		<phraseLemma>table 1 present the result on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that is encoded in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Pairwise similarities seem to be only part of the information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that is encoded in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word embeddings so looking at more global measures is necessary for a better understanding of differences between embeddings</example>
		<phraseLemma>np that be encode in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are asked to identify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>One example is the word intrusion task in which annotators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are asked to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a random word inserted into the set of high probability words for a given topic</example>
		<phraseLemma>np be ask to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which has &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We also use the 1 Newsgroup dataset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which has document labels for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; document label experiments</example>
		<phraseLemma>np which have np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is also in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Hu and BoydGraber use WordNet 1 to obtain synsets for word types and then if a synset &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is also in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vocabulary they add a mustlink correlation between the word type and the synset</example>
		<phraseLemma>np be also in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we ﬁrst obtain &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For a noun word type we ﬁrst obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its synsets from WordNet 1</example>
		<phraseLemma>for np we ﬁrst obtain np</phraseLemma>
	</can>
	<can>
		<phrase>We also conduct experiments on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also conduct experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SCLDA with varying numbers of word correlations</example>
		<phraseLemma>we also conduct experiment on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by embedding &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Recent models for knowledge base completion impute missing facts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by embedding knowledge graphs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vector spaces</example>
		<phraseLemma>np by embed np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that have been used for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this paper we present a scheme to answer path queries on knowledge bases by compositionalizing” a broad class of vector space models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that have been used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; knowledge base completion</example>
		<phraseLemma>np that have be use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can also be seen as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Therefore compositional training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can also be seen as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a new form of structural regularization for existing models</example>
		<phraseLemma>np can also be see as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which we use in our experiments</phrase>
		<frequency>4</frequency>
		<example>Finally we illustrate the technique for several more models in Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which we use in our experiments&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np which we use in we experiment</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uniformly from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>At step i of the walk choose a relation ri &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uniformly from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set of relations incident on the current entity e Choose the next entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uniformly from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set of entities reachable via ri</example>
		<phraseLemma>np uniformly from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are connected via &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The deduction subset consists of queries q = s/p where the source and target entities JqK &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are connected via&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relations p in the training graph Gtrain but the speciﬁc query q was never seen during training</example>
		<phraseLemma>np be connect via np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; did &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In no case &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;did this additional 1 term improve SINGLEs performance on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the path query or single edge dataset</example>
		<phraseLemma>np do np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can improve the quality of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Dong demonstrated that KBC models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can improve the quality of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relation extraction by serving as graphbased priors</example>
		<phraseLemma>np can improve the quality of np</phraseLemma>
	</can>
	<can>
		<phrase>There is also &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There is also recent work on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; treebank translation via a machine translation system</example>
		<phraseLemma>there be also np on np</phraseLemma>
	</can>
	<can>
		<phrase>We use to denote &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use to denote&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the set of all sentences that receive a full parse under P</example>
		<phraseLemma>we use to denote np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained on the data</phrase>
		<frequency>4</frequency>
		<example>As a 1 evaluation method we can test the accuracy of a model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained on the data&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np train on the datum</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows results using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows results using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple source languages using the concatenation method</example>
		<phraseLemma>table 1 show result use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; is treated as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this paper we formalize the dependency parsing task for a lowresource language as a domain adaptation task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which a target resourcepoor language treebank is treated as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; indomain while a much larger treebank in a highresource language forms the outofdomain data</example>
		<phraseLemma>np in which np be treat as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is then fed into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Each word POS and label is mapped into a lowdimension vector representation using an embedding matrix &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is then fed into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a twolayer neural network classiﬁer to predict the next parsing action</example>
		<phraseLemma>np which be then feed into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; can serve as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This reﬂects the fact that different languages have different lexicon partsofspeech often exhibit different roles and dependency edges serve different functions eg &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Korean a static verb can serve as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an adjective</example>
		<phraseLemma>np in np can serve as np</phraseLemma>
	</can>
	<can>
		<phrase>We compare our approach to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We compare our approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline interlingual model based on the same parsing algorithm as presented in section 1 but with cascaded training</example>
		<phraseLemma>we compare we approach to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; come from &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The translations in PanLex &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;come from various sources such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; glossaries dictionaries automatic inference from other languages etc</example>
		<phraseLemma>np come from np such as np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we use &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For this purpose we use a joint model of English and French using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the available French treebank as well as a bilingual dictionary</example>
		<phraseLemma>for np we use np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; showing clearly that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>The colouring denotes the POS tag &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;showing clearly that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the words with similar POS tags are grouped together regardless of languages</example>
		<phraseLemma>np show clearly that np</phraseLemma>
	</can>
	<can>
		<phrase>In future work we plan to extend &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In future work we plan to extend&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; joint training to several languages and further explore the idea of learning and exploiting crosslingual embeddings</example>
		<phraseLemma>in future work we plan to extend np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that treat &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Since it it is well known that a words form often provides strong evidence regarding its grammatical role in morphologically rich languages this has promise to improve accuracy and statistical efﬁciency relative to traditional approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that treat each word type as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; opaque and independently modeled</example>
		<phraseLemma>np that treat np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; differs slightly from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This formulation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;differs slightly from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classic LSTM formulation in that it makes use of peephole connections” and deﬁnes the forget gate so that it sums with the input gate to 1</example>
		<phraseLemma>np differ slightly from np</phraseLemma>
	</can>
	<can>
		<phrase>Tables &lt;NP&gt; show the results of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Tables 1 and 1 show the results of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parsers for the development sets and the ﬁnal test sets respectively</example>
		<phraseLemma>table np show the result of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provided by &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provided by the characterbased representations using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bidirectional LSTMs are strong for agglutinative languages such as Basque Hungarian Korean and Turkish comparing favorably to POS tags as encoded in those languages currently available treebanks</example>
		<phraseLemma>np provide by np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; comparing favorably to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The improvements provided by the characterbased representations using bidirectional LSTMs are strong for agglutinative languages such as Basque Hungarian Korean and Turkish &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;comparing favorably to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; POS tags as encoded in those languages currently available treebanks</example>
		<phraseLemma>np compare favorably to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been introduced in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Dozens of systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been introduced in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the past two decades and most of them are deletionbased</example>
		<phraseLemma>np have be introduce in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which beneﬁts from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In particular we will present a model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which beneﬁts from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models to output surprisingly readable and informative compressions</example>
		<phraseLemma>np which beneﬁts from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as presented by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Regarding the learning procedure the original model uses a largemargin learning framework namely MIRA but with some minor changes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as presented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; McDonald</example>
		<phraseLemma>np as present by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisting of &lt;NP&gt; is used</phrase>
		<frequency>4</frequency>
		<example>The model is ﬁtted over ten epochs on the whole training data and for model selection a small development set &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisting of 1 previously unseen sentences is used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np consist of np be use</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is fed with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The RNN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is fed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input words Xi until we feed a special symbol GO”</example>
		<phraseLemma>np be feed with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; set in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We evaluate the baseline and our systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the sentence test set in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an experiment with human raters</example>
		<phraseLemma>np on np set in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set in &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We evaluate the baseline and our systems on the sentence test &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set in an experiment with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human raters</example>
		<phraseLemma>np set in np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not perform better than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Although giving overall better performance the syntactic model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not perform better than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Ngram model in all cases</example>
		<phraseLemma>np do not perform better than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are much less than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the test data WHPP SBARQ and WHNP &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are much less than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PP NP VP ADJP ADVP and CONJP on which the syntactic model gives better recalls</example>
		<phraseLemma>np be much less than np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; outperforms &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In both indomain and crossdomain test data the combined system outperforms all other systems with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a BLEU score of 1 been achieved in the WSJ domain</example>
		<phraseLemma>in np outperform np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has looked at &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While much work on this task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has looked at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deletionbased sentence compression techniques among many others studies of human summarizers show that it is common to apply various other operations while condensing such as paraphrasing generalization and reordering</example>
		<phraseLemma>np have look at np</phraseLemma>
	</can>
	<can>
		<phrase>While &lt;NP&gt; has &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While the convolutional encoder has richer capacity than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; bagofwords it still is required to produce a single representation for the entire input sentence</example>
		<phraseLemma>while np have np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as is standard in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We do this by modifying our scoring function to directly estimate the probability of a summary using a loglinear model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as is standard in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; machine translation</example>
		<phraseLemma>np as be standard in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learn &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>They extract tree transduction rules from aligned parsed texts and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learn weights on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; transfomations using a maxmargin learning algorithm</example>
		<phraseLemma>np learn np on np</phraseLemma>
	</can>
	<can>
		<phrase>Figure 1 shows an example of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Figure 1 shows an example of the citationcontext in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the reference article for a citation in the citing article</example>
		<phraseLemma>figure 1 show a example of np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by leveraging &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We address this shortcoming &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by leveraging the citationcontext and the inherent discourse model in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the scientiﬁc articles</example>
		<phraseLemma>np by leverage np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; an example of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In ﬁgure 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;an example of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; citation marker is shown</example>
		<phraseLemma>np a example of np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; for extracting &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used Metamap for extracting&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; biomedical concepts from the citation text</example>
		<phraseLemma>we use np for extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; until we reach &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We select top sentences iteratively from each group &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;until we reach&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the summary length threshold</example>
		<phraseLemma>np until we reach np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We employ a greedy strategy similar to MMR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which sentences from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each group are selected based on the following scoring formula</example>
		<phraseLemma>np in which np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; the &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Where for each sentence S the score is a linear interpolation of similarity of sentence &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with all other sentences and the&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similarity of sentence with the sentences already in the summary and λ is a constant</example>
		<phraseLemma>np with np the np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the best for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Its performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the best for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; short summaries among other baselines</example>
		<phraseLemma>np be the best for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that maximize &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Whereas the diversiﬁcation method selects sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that maximize the gain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; in informativeness and at the same time contributes to the novelty of the summary</example>
		<phraseLemma>np that maximize np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to mark &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Hashtags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to mark&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; keywords or topics in a microblog</example>
		<phraseLemma>np be use to mark np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that &lt;NP&gt; can achieve &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Experimental results on the dataset we construct from a real microblogging service &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that the proposed method can achieve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signiﬁcantly better performance than the stateofthearts methods</example>
		<phraseLemma>np show that np can achieve np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; is viewed as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In standard LDA each document is viewed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a mixture of topics and each topic has probabilities to generate words</example>
		<phraseLemma>in np be view as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is regarded as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>And p = Nk &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is regarded as a prior for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic distribution where Z is the normalized factor</example>
		<phraseLemma>np be regard as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which we consider &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>NHR 1 ” is a degenerate variation of CNHR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which we consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all the hashtags are generated from distribution</example>
		<phraseLemma>np in which we consider np</phraseLemma>
	</can>
	<can>
		<phrase>And we can see that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;And we can see that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the performances of TTM are similar as the results of NHR 1</example>
		<phraseLemma>and we can see that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are similar with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Since TTM and NHR 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are similar with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each other except that TTM is based on LDA and NHR 1 is adapted from DPMM</example>
		<phraseLemma>np be similar with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; doing &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>It is helpful for educationists to select texts appropriate to the reading/grade levels of the students and for web designers to organize texts on web pages for the users &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;doing personalized searches for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; information retrieval</example>
		<phraseLemma>np do np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; to estimate &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The wordbased methods compute word frequencies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in documents to estimate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their readability</example>
		<phraseLemma>np in np to estimate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; built &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We compute the similarity between any pair of documents using the Euclidean distance and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;built the featurebased graph in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same way as above</example>
		<phraseLemma>np build np in np</phraseLemma>
	</can>
	<can>
		<phrase>On &lt;NP&gt; It has &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On ENCT It has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relatively good performance on grade levels 1 and 1 while on the Chinese dataset CPT the performance is not satisfactory</example>
		<phraseLemma>on np it have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>From Figure 1 on the Chinese dataset nearly all the unlabeled documents are classiﬁed as level 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by the TFIDF matrix in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the word frequencies are too few to make meaningful discrimination among the reading levels</example>
		<phraseLemma>np by np in which np</phraseLemma>
	</can>
	<can>
		<phrase>We are interested in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We are interested in generalizable models for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different regions usergenerated content has been created in</example>
		<phraseLemma>we be interested in np for np</phraseLemma>
	</can>
	<can>
		<phrase>We created &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For this purpose &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we created datasets with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than 1 k labeled tweets to train and test models with respect to their generalization</example>
		<phraseLemma>np we create np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As the most simple approach and as used in all related works we represented tweets as a set of words and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of characters with varying lengths</example>
		<phraseLemma>np also as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; report &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We therefore weighted the measure by this ratio and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;report the microaveraged results over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all datasets F 1</example>
		<phraseLemma>np report np over np</phraseLemma>
	</can>
	<can>
		<phrase>Work related to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Work related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; essay scoring can be traced back to when Ellis Page created a computer grading software called Project Essay Grade</example>
		<phraseLemma>work related to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; metric in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We use QWK in this paper which is also the evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;metric in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ASAP competition</example>
		<phraseLemma>np metric in np</phraseLemma>
	</can>
	<can>
		<phrase>We now focus on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We now focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; EasyAdapt Concat and MLρ which are the better domain adaptation methods from our results</example>
		<phraseLemma>we now focus on np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 gives examples of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 gives examples of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; valid CDE and nonvalid CDE according to the deﬁnition mentioned above</example>
		<phraseLemma>table 1 give example of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is known in advance</phrase>
		<frequency>4</frequency>
		<example>In addition we do not limit ourselves to a particular domain nor assume that the topic of the discussion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is known in advance&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be know in advance</phraseLemma>
	</can>
	<can>
		<phrase>Were asked to mark &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Brieﬂy given a topic and a corresponding relevant claim extracted from a Wikipedia article by human annotators the annotators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were asked to mark&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; corresponding evidence – text segments sup porting the claim</example>
		<phraseLemma>np be ask to mark np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not have &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Moreover as evident from Table 1 many claims &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not have any associated CDE in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same article</example>
		<phraseLemma>np do not have np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisted of &lt;NP&gt; composed of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The train and test data for this component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisted of all text segments composed of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 consecutive sentences included within the same paragraph</example>
		<phraseLemma>np consist of np compose of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; exploited &lt;NP&gt; of information</phrase>
		<frequency>4</frequency>
		<example>The features used by this component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;exploited three types of information&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np exploit np of information</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; comes from &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For type Expert this suggests that most of the signal in the contextdependent component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;comes from semantic relatedness between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the claim and candidate CDE</example>
		<phraseLemma>np come from np between np</phraseLemma>
	</can>
	<can>
		<phrase>We assessed the performance of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We assessed the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the proposed approach over a novel benchmark dataset demonstrating the validity of our architecture and the necessity of all its components</example>
		<phraseLemma>we assess the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>We show results on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We show results on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a test set which was annotated by humans and compare against online autocorrection capabilities of three additional web sites</example>
		<phraseLemma>we show result on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that uses &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To our knowledge this is the ﬁrst work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that uses characterbased machine translation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; technology on usergenerated data for spelling correction</example>
		<phraseLemma>np that use np on np</phraseLemma>
	</can>
	<can>
		<phrase>We add &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As a ﬁnal step &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we add frequent queries from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; user event logs but also product titles of the live inventory to the language model</example>
		<phraseLemma>np we add np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; especially for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We also found that tuning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on Matthews Correlation Coefﬁcient balances better precision versus recall especially for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unbalanced classes which is the case here</example>
		<phraseLemma>np on np especially for np</phraseLemma>
	</can>
	<can>
		<phrase>In particular we extract &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular we extract&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the categories from all items which are returned as part of the search API calls</example>
		<phraseLemma>in particular we extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; Search &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The authors would like to thank the Localization team at eBay for creating gold corrections for the evaluation sets and members of the HLT and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Search teams for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; fruitful discussions as well as reading early drafts of this work and giving valuable feedback</example>
		<phraseLemma>np search np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in terms of correlation with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The produced rankings are used to evaluate standard metrics for grammatical error correction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in terms of correlation with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; human judgment</example>
		<phraseLemma>np in term of correlation with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; for &lt;NP&gt; respectively</phrase>
		<frequency>4</frequency>
		<example>This is the last step required to produce the rankings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in Tables 1 b and 1 c for both methods EW and TS respectively&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np for np respectively</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produced by &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Based on Machácˇek and Bojar we use Spearmans rank correlation ρ and Pearsons r to compare the similarity of rankings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produced by various metrics to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the manual ranking from the previous section</example>
		<phraseLemma>np produce by np to np</phraseLemma>
	</can>
	<can>
		<phrase>We investigate the performance of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We investigate the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the constructed deep model when applied to semisupervised text classiﬁcation problems and ﬁnd that our hybrid architecture outperforms all baselines</example>
		<phraseLemma>we investigate the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which differs from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The SBEN may be viewed as a natural vertical ensemble of layerwise experts” where each layer maps latent representations to predictions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which differs from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard methods such as boosting</example>
		<phraseLemma>np which differ from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Multiple researchers for example reported the usage of text features including simple lexical similarity scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between the query and ads word or phrase overlaps and the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; overlapping words and characters</example>
		<phraseLemma>np between np the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; thus lead to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Wang showed that speciﬁc frequently occurring lexical patterns eg x 1 off guaranteed return in x days and ofﬁcial site are effective in triggering users desires and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;thus lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signiﬁcant differences in CTR</example>
		<phraseLemma>np thus lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is known to have &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The ROC AUC &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is known to have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a correlation with the quality of ranking by the predicted score thus is one of the most important metrics for click prediction</example>
		<phraseLemma>np be know to have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on the ideas of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The experimental results show that our method outperforms a baseline bootstrapping system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the ideas of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Agichtein and Gravano which relies on TFIDF representations</example>
		<phraseLemma>np base on the idea of np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; exist between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If no verbs exist between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two entities BREDS extracts all the words between the two entities to build the representations for the BET context</example>
		<phraseLemma>if np exist between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which takes as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Algorithm 1 describes the clustering approach taken by BREDS &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which takes as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; input a list of relationship instances and assigns the ﬁrst instance to a new empty cluster</example>
		<phraseLemma>np which take as np</phraseLemma>
	</can>
	<can>
		<phrase>We discarded &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We discarded the clusters with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; only one relationship instances and ran a maximum of 1 bootstrapping iterations</example>
		<phraseLemma>we discard np with np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each relationship type the best score and the corresponding precision and recall for all combinations of τsim and τt values and considering only extracted relationship instances with conﬁdence scores equal or above 1</example>
		<phraseLemma>table 1 show for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; works better for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The performance results of Snowball and Snowball suggest that selecting words based on a relational pattern to represent the BET context instead of using all the words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;works better for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TFIDF representations</example>
		<phraseLemma>np work better for np</phraseLemma>
	</can>
	<can>
		<phrase>In future work &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In future work&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more robust entitylinking approaches as proposed by Hoffart could be included in our preprocessing pipeline</example>
		<phraseLemma>in future work np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the method of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Gabbard have shown that coreference resolution can increase bootstrapping RE performance and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the method of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Durrett and Klein could also be included in our preprocessing pipeline</example>
		<phraseLemma>np the method of np</phraseLemma>
	</can>
	<can>
		<phrase>There are &lt;NP&gt; like &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There are extensive ontologies like&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Gene Ontology annotated corpora like the GENIA and BioInfer corpora and dedicated shared tasks including BioCreative and BioNLP</example>
		<phraseLemma>there be np like np</phraseLemma>
	</can>
	<can>
		<phrase>Were used to query &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Search terms obtained from domain experts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;were used to query&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Natures OpenSearch API 1 for publications in a limited range of relevant journals after retrieving records including title and abstract</example>
		<phraseLemma>np be use to query np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; specify &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Moreover augmenting documentspeciﬁc gazetteers with KB information lets users &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;specify fewer tags for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same performance reducing cost</example>
		<phraseLemma>np specify np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are also present in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Names and types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are also present in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; large quantities of ﬁnancial news stories from Bloomberg in the form of linked names of companies and people</example>
		<phraseLemma>np be also present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is required for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The beneﬁt of KB tags comes from their type information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is required for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; good performance</example>
		<phraseLemma>np which be require for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allows &lt;NP&gt; to capture &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Using multiple gazetteers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allows feature weights to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different name types and sources</example>
		<phraseLemma>np allow np to capture np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to incorporate &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use an encoding scheme to incorporate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the type information from the KB tag</example>
		<phraseLemma>we use np to incorporate np</phraseLemma>
	</can>
	<can>
		<phrase>When &lt;NP&gt; overlaps with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;When a gazetteer match overlaps with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a CRF match we prefer the gazetteer and remove the latter</example>
		<phraseLemma>when np overlap with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; results in &lt;NP&gt; but &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Matching against KB tag names &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;results in highprecision but&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; low recall with an Fscore of 1 far worse than the baseline CRF at 1</example>
		<phraseLemma>np result in np but np</phraseLemma>
	</can>
	<can>
		<phrase>We also examine &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also examine the pertag Fscores for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TESTA to investigate whether KB tags help some types of entities more than others</example>
		<phraseLemma>we also examine np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as these are &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The distribution of labels in the dataset is skewed towards birth and death events &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as these are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; life events that happen to almost all person entities in Wikipedia</example>
		<phraseLemma>np as these be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allow us to identify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While feature weights from the MAXENT model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allow us to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; verbs that are good features for predicting a particular state change label our distantly supervised training data is inherently noisy</example>
		<phraseLemma>np allow we to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that extracts &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For instance a set of facts like adverseEffectOf interactsWith might be used to train an IE system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that extracts these relations from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; documents</example>
		<phraseLemma>np that extract np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consisting of &lt;NP&gt; mentions</phrase>
		<frequency>4</frequency>
		<example>In distant supervision instances are ﬁrst matched against a corpus and the matching sentences are then used to generate training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consisting of labeled entity mentions&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np consist of np mention</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is contained by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If an item &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is contained by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a list an edge between the item vertex and the list vertex is included in the graph</example>
		<phraseLemma>np be contain by np</phraseLemma>
	</can>
	<can>
		<phrase>This can be viewed as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This can be viewed as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semisupervised learning of the NPs that may denote a type</example>
		<phraseLemma>this can be view as np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; are used to train &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each system different portions of the training set are used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the system as shown in the ﬁrst row</example>
		<phraseLemma>for np be use to train np</phraseLemma>
	</can>
	<can>
		<phrase>We explored &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We explored an alternative approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; distant supervision based on detection of lists in text to overcome the weakness of distant supervision resulted by noisy training data</example>
		<phraseLemma>we explore np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are then passes through &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Tokenized and coreferenceresolved sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are then passes through&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; OpenIEv 1 system 1 to extract triples</example>
		<phraseLemma>np be then pass through np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A noun phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than one canopy hence those groups across canopies are merged if the similarity is greater than certain threshold</example>
		<phraseLemma>np can be in np</phraseLemma>
	</can>
	<can>
		<phrase>We observed that &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We observed that relation mapping has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lesser accuracy due to two reasons</example>
		<phraseLemma>we observe that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; processing &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>With the recent success of neural networks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in natural language processing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different neural network models are proposed to learn syntactic features from raw sequences of words or constituent parse trees which have been proved effective but often suffer from irrelevant subsequences or clauses especially when subjects and objects are in a longer distance</example>
		<phraseLemma>np in np processing np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are proposed to learn &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>With the recent success of neural networks in natural language processing different neural network models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are proposed to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntactic features from raw sequences of words or constituent parse trees which have been proved effective but often suffer from irrelevant subsequences or clauses especially when subjects and objects are in a longer distance</example>
		<phraseLemma>np be propose to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that works on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We therefore propose to learn a more robust relation representation from a convolution neural network model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that works on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the simple dependency path between subjects and objects which naturally characterizes the relationship between two nominals and avoids negative effects from other irrelevant chunks or clauses</example>
		<phraseLemma>np that work on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which we leave for future work</phrase>
		<frequency>4</frequency>
		<example>Beyond the relation extraction task we believed the proposed Negative Sampling method has the potential to beneﬁt other NLP tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which we leave for future work&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np which we leave for future work</phraseLemma>
	</can>
	<can>
		<phrase>While there have been &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While there have been earlier approaches for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; automatic extensions of temporal taggers to further languages these were limited to a few languages and the results were considered less successful in particular for the normalization subtask</example>
		<phraseLemma>while there have be np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are that &lt;NP&gt; are required</phrase>
		<frequency>4</frequency>
		<example>In addition to the fact that this process is time and labor intensive further disadvantages &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are that a language expert and temporally annotated training data are required&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np be that np be require</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will be described in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While details about evaluation measures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will be described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 the results of the automatic approach are obviously lower as expected due to the simpliﬁcation process</example>
		<phraseLemma>np will be describe in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; it is that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>The fewer translations are available for a language the more likely &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;it is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the temporal tagging quality is rather low</example>
		<phraseLemma>np it be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have to be addressed</phrase>
		<frequency>4</frequency>
		<example>In particular issues with morphologyrich languages and those without whitespace tokenization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have to be addressed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np have to be address</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; allows &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Jointly training the embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the multipart objectives allows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁnetuned embeddings to further inﬂuence other embeddings even those that do not appear in the labeled training data</example>
		<phraseLemma>np with np allow np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; fails to ﬁnd &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The strict method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;fails to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the subsumption relation between award winner and person while this subsumption actually holds with a large conﬁdence</example>
		<phraseLemma>np fail to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>In addition because &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; date/time such as Jan 1 th 1 ” often occurs in the object argument while Freebase does not have any such speciﬁc dates as entities we use SUTime to recognize dates as an virtual entity</example>
		<phraseLemma>in addition because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; linked &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Where p is the joint probability of relation and type pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the whole linked&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; tuple set and stands for any relations or type pairs</example>
		<phraseLemma>np in np link np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; holds &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However even if sourcebased feature vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;holds a few dimensions in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our experiments src LDT improves the precision on the basis of GM</example>
		<phraseLemma>np hold np in np</phraseLemma>
	</can>
	<can>
		<phrase>We obtain &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Interestingly our experiment results indicate that our lightweight features are actually very powerful when used in conjunction with the proposed feature transformation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we obtain a significant performance improvement over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the best challenge system</example>
		<phraseLemma>np we obtain np over np</phraseLemma>
	</can>
	<can>
		<phrase>We explored &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We explored the space of lightweight features and their nonlinear transformation with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the goal of supporting online webscale sentence novelty detection</example>
		<phraseLemma>we explore np with np</phraseLemma>
	</can>
	<can>
		<phrase>The experiment results &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The experiment results show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these features are not only efﬁcient but also very powerful a combination of these features with a simple scalable classiﬁcation approach signiﬁcantly surpassed the best challenge system at TREC 1</example>
		<phraseLemma>the experiment result np that np</phraseLemma>
	</can>
	<can>
		<phrase>This is mainly due to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is mainly due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the loose dependencies between the different answer passages in standard QA</example>
		<phraseLemma>this be mainly due to np</phraseLemma>
	</can>
	<can>
		<phrase>We compare it to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Below we apply graphcut and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we compare it to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an integer linear programming formulation for decoding under global constraints we also provide results with a linearchain CRF</example>
		<phraseLemma>np we compare it to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is shown in Figure 1 where &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A simpliﬁed example &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is shown in Figure 1 where&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; answers 1 and 1 are good answer 1 is potentially useful and answer 1 is bad</example>
		<phraseLemma>np be show in figure 1 where np</phraseLemma>
	</can>
	<can>
		<phrase>We merge &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We merge Potential and Bad labels into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Bad and we focus on the 1 class problem</example>
		<phraseLemma>we merge np into np</phraseLemma>
	</can>
	<can>
		<phrase>We also compute &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also compute similarity using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; partial tree kernels on shallow syntactic trees</example>
		<phraseLemma>we also compute np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that predicts whether &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We have shown that using a pairwise classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that predicts whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two comments should get the same label followed by a graphcut global inference improves signiﬁcantly over a very strong baseline as well as over the state of the art</example>
		<phraseLemma>np that predict whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as required by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Moreover parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as required by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Bendersky and Named Entity Recognizers for the medical domain are less effective than the general domain</example>
		<phraseLemma>np as require by np</phraseLemma>
	</can>
	<can>
		<phrase>We employ &lt;NP&gt; to ﬁnd &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We employ turbo topics to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; phrases from these topics</example>
		<phraseLemma>we employ np to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>We employed &lt;NP&gt; to identify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We employed Conditional Random Fields model to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; key concepts which are most in need of explanation by external education materials</example>
		<phraseLemma>we employ np to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; like &lt;NP&gt; do not</phrase>
		<frequency>4</frequency>
		<example>It is clear that while topics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;like the ﬁrst one capture medical concepts others like the 1 one do not&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np like np do not</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; created by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Finally to investigate the gap between medical language and lay language we substituted the medical concepts recognized by MetaMap &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with their consumeroriented counterparts created by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Consumer Health Vocabulary Initiative</example>
		<phraseLemma>np with np create by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are complemented by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Furthermore a query expansion approach in which key concepts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are complemented by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; other medical concepts from pseudorelevant documents further improves the performance</example>
		<phraseLemma>np be complement by np</phraseLemma>
	</can>
	<can>
		<phrase>We deﬁne &lt;NP&gt; for &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We deﬁne IDs for each modality in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each division as given in Table 1</example>
		<phraseLemma>we deﬁne np for np in np</phraseLemma>
	</can>
	<can>
		<phrase>We can ﬁnd &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We can ﬁnd relevant documents in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; another language by computing the distances from the query documents using the coupled canonical subspaces</example>
		<phraseLemma>we can ﬁnd np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; by computing &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We can ﬁnd relevant documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in another language by computing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the distances from the query documents using the coupled canonical subspaces</example>
		<phraseLemma>np in np by compute np</phraseLemma>
	</can>
	<can>
		<phrase>We included &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To establish a new benchmark dataset for imagemediated CLDR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we included a Japanese translation for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each English sentence provided by professional translators as shown in Figure 1</example>
		<phraseLemma>np we include np for np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; work with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the related studies authors work with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similar approaches for extracting sentiment from texts</example>
		<phraseLemma>in np work with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be considered as &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Hence sentiment scores &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be considered as an additional leading indicator for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the future evolution of the Tier 1 capital ratio</example>
		<phraseLemma>np can be consider as np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; derived from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The described systems have the potential to provide valuable insights for banking supervisors in particular because of the strong correlation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between sentiment scores derived from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; textual data and the</example>
		<phraseLemma>np between np derive from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; in question</phrase>
		<frequency>4</frequency>
		<example>By that the reviews express implicitly or explicitly an overall opinion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the topic in question&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np in question</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; refers to &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Argumentation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;refers to the exchange of opinions to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; defending positions and to convincing others of certain stances</example>
		<phraseLemma>np refer to np to np</phraseLemma>
	</can>
	<can>
		<phrase>To ﬁnd &lt;NP&gt; across &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To ﬁnd the most general model of web review argumentation across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; domains we compare sentiment ﬂow variants using three measures</example>
		<phraseLemma>to ﬁnd np across np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; signiﬁcantly improves over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Still the value 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;signiﬁcantly improves over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all baselines under a paired ttest</example>
		<phraseLemma>np signiﬁcantly improve over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; assuming that &lt;NP&gt; are given</phrase>
		<frequency>4</frequency>
		<example>The ﬁrst is targeted sentiment classiﬁcation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;assuming that entity mentions are given&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np assume that np be give</phraseLemma>
	</can>
	<can>
		<phrase>It has been applied to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It has been applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the models of Do and Collobert for example</example>
		<phraseLemma>it have be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; does not suffer from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In contrast maxmargin training &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;does not suffer from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the label skew issue thanks to the use of Hamming loss in the objective function</example>
		<phraseLemma>np do not suffer from np</phraseLemma>
	</can>
	<can>
		<phrase>Because &lt;NP&gt; is based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Because discrete feature instantiation is based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; exact matching the discrete model gives a relatively higher precision</example>
		<phraseLemma>because np be base on np</phraseLemma>
	</can>
	<can>
		<phrase>We explored &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We explored open domain targeted sentiment analysis using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; neural network models which gave competitive results when evaluated against a strong discrete CRF baseline with relatively higher recalls</example>
		<phraseLemma>we explore np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been made to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However few attempts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been made to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; address cases where the validity of an evaluation is restricted on a condition in the source text such as for traveling with small kids”</example>
		<phraseLemma>np have be make to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; include &lt;NP&gt; related to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>By deﬁnition UCFOs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;include expressions related to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; user attributes such as nervosity” in Figure 1</example>
		<phraseLemma>np include np related to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; comprising &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Errors were due to dependency analysis which often mistakenly recognizes sentence boundaries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in an informal writing style and dependency relations in a sentence comprising&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a phrase such as the best location for fully enjoying Asakusa”</example>
		<phraseLemma>np in np comprise np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; about &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Understanding entailment and contradiction is fundamental to understanding natural language and inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;about entailment and contradiction is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a valuable testing ground for the development of semantic representations</example>
		<phraseLemma>np about np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; including &lt;NP&gt; used by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Randomly chosen examples from the development section of our new corpus shown with both the selected gold labels and the full set of labels from the individual annotators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;including the label used by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the initial author of the pair</example>
		<phraseLemma>np include np use by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; approaching &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We further evaluate the LSTM model by taking advantage of its ready support for transfer learning and show that it can be adapted to an existing NLI challenge task yielding the best reported performance by a neural network model and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;approaching the overall state of the art 1 A new corpus for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; NLI To date the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment challenge tasks</example>
		<phraseLemma>np approach np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is available under &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is available under&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a CreativeCommons AttributionShareAlike license the same license used for the Flickr 1 k source captions</example>
		<phraseLemma>np be available under np</phraseLemma>
	</can>
	<can>
		<phrase>It is surprising that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is surprising that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the classiﬁer performs as well as it does without any notion of alignment or tree transformations</example>
		<phraseLemma>it be surprising that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; with &lt;NP&gt; 1</phrase>
		<frequency>4</frequency>
		<example>This choice allows us to focus on existing models for sentence embedding and it allows us to evaluate the ability of those models to learn useful representations of meaning at the cost of excluding from con 1 way softmax classiﬁer 1 d tanh layer 1 d tanh layer 1 d tanh layer 1 d premise 1 d hypothesis sentence model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with premise input sentence model with hypothesis input Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np with np 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; to map &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition all of the models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use an additional tanh neural network layer to map&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these 1 d embeddings into the lowerdimensional phrase and sentence embedding space</example>
		<phraseLemma>np use np to map np</phraseLemma>
	</can>
	<can>
		<phrase>Empirically we ﬁnd that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Empirically we ﬁnd that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top weighted features for the classiﬁer trained on examples tend to be high precision entailments eg playing &amp;gt outside a banana &amp;gt person eating</example>
		<phraseLemma>empirically we ﬁnd that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that are common in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Despite the large size of the training corpus and the distributional information captured by GloVe initialization many lexical relationships are still misanalyzed leading to incorrect predictions of independent even for pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that are common in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training corpus like beach/surf and sprinter/runner</example>
		<phraseLemma>np that be common in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is trained from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To perform transfer we take the parameters of the LSTM RNN model trained on SNLI and use them to initialize a new model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is trained from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that point only on the training portion of SICK</example>
		<phraseLemma>np which be train from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that were used to train &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We use the same model hyperparameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that were used to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the original model with the exception of the regularization strength which is retuned</example>
		<phraseLemma>np that be use to train np</phraseLemma>
	</can>
	<can>
		<phrase>We gratefully acknowledge support from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We gratefully acknowledge support from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a Google Faculty Research Award a gift from Bloomberg LP the Defense Advanced Research Projects Agency Deep Exploration and Filtering of Text Program under Air Force Research Laboratory contract no</example>
		<phraseLemma>we gratefully acknowledge support from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that provide information about &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We annotate verbs with pairs of questions and answers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that provide information about&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; predicateargument structure</example>
		<phraseLemma>np that provide information about np</phraseLemma>
	</can>
	<can>
		<phrase>We measure &lt;NP&gt; against &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We measure the macroaveraged precision and recall of our annotation against&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PropBank with the proportion of our QApairs that are match a PropBank relation and the proportion of PropBank relations covered by our annotation</example>
		<phraseLemma>we measure np against np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are strongly associated with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>There are many unsurprising correlations— who questions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are strongly associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PropBank agents and where and when questions correspond to PropBank temporal and locative roles respectively</example>
		<phraseLemma>np be strongly associate with np</phraseLemma>
	</can>
	<can>
		<phrase>We train &lt;NP&gt; for &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train a binary classiﬁer for every label in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Ltrain using L 1 regularized logistic regression by Liblinear with hyperparameter C = 1</example>
		<phraseLemma>we train np for np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by substituting &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We can map an abstract question to a surface realization &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by substituting the slots with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pronoun values of the predicted labels</example>
		<phraseLemma>np by substitute np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; come from &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The negative samples &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;come from all the words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentence that are not an answer head</example>
		<phraseLemma>np come from np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that predicts &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We also use the baseline method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that predicts a random syntactic child from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 best parse for each question</example>
		<phraseLemma>np that predict np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as those of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>An enterprise is typically interested in public reviews of its own products &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as those of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its competitors the identiﬁcation of these entities is thus critical for further analysis</example>
		<phraseLemma>np as well as those of np</phraseLemma>
	</can>
	<can>
		<phrase>However the lack of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However the lack of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst two types of information makes this problem more challenging</example>
		<phraseLemma>however the lack of np</phraseLemma>
	</can>
	<can>
		<phrase>We conduct a series of experiments on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We conduct a series of experiments on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three practical datasets from different domains</example>
		<phraseLemma>we conduct a series of experiment on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; demonstrate that &lt;NP&gt; has &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The experimental results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;demonstrate that TremenRank has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a similar performance to the stateoftheart when addressing the TED problem at a large scale and the use of MLD graph signiﬁcantly improves our method</example>
		<phraseLemma>np demonstrate that np have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are less similar to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The blue nodes of the seeds in the top layer are the most trustworthy and the other white nodes in the higher layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are less similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the seeds which implies that they are at lower trust levels</example>
		<phraseLemma>np be less similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; we assign &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Finally disambiguation occurs at the document level &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the proposed method we assign&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same scores to the entities that occur in one document because they typically share common context features in short texts</example>
		<phraseLemma>np in np we assign np</phraseLemma>
	</can>
	<can>
		<phrase>In particular we introduce &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular we introduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; topic modeling methodologies into entity biography profiling so as to build a bridge between fuzzy and exact matching</example>
		<phraseLemma>in particular we introduce np</phraseLemma>
	</can>
	<can>
		<phrase>In view of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In view of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the abovementioned investigation we partition the string matching results into two parts exact and fuzzy ones which are used as reliable prior knowledge and unrefined prior knowledge respectively</example>
		<phraseLemma>in view of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that &lt;NP&gt; generates &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We design a generative approach to estimate the biographydocument relevance r which calculates the conditional probability &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that a candidate document D generates&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the biography B</example>
		<phraseLemma>np that np generate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will be added to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The preserved documents &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will be added to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the reference source for both biography reformulation and archiving result output</example>
		<phraseLemma>np will be add to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which may lead to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Another disadvantage of the Top model is that it learns the rank information of the full list but ignores the rank information of partial sequences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which may lead to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ineffective learning</example>
		<phraseLemma>np which may lead to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was released in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This database &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was released in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; early and has been widely used in learning to rank studies</example>
		<phraseLemma>np be release in np</phraseLemma>
	</can>
	<can>
		<phrase>This is not surprising as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;This is not surprising as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the adaptive distribution sampling uses a more reasonable relevance score to balance relevant and irrelevant documents</example>
		<phraseLemma>this be not surprising as np</phraseLemma>
	</can>
	<can>
		<phrase>However if there are &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However if there are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; too many samples the performance starts to decrease</example>
		<phraseLemma>however if there be np</phraseLemma>
	</can>
	<can>
		<phrase>On average &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;On average the ground networks in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these runs were of the order of 1 × 1 ground clauses</example>
		<phraseLemma>on average np in np</phraseLemma>
	</can>
	<can>
		<phrase>What was &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;What was a ﬁrstorder rule in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; FOMLN is now already fully grounded!</example>
		<phraseLemma>what be np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are present in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Due to lexical variability often not all conjuncts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a rules antecedent are present in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the questions setup</example>
		<phraseLemma>np in np be present in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which uses &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This differs from FOMLN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which uses an external alignment system to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁnd parts of the antecedent mentioned in the setup L 1 and creates one rule ⇒ R Comparison with FOMLN</example>
		<phraseLemma>np which use np to np</phraseLemma>
	</can>
	<can>
		<phrase>We create &lt;NP&gt; for &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We create copies of these rules for edges with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the same label r = s with a higher weight and for edges with different labels r 1 = s with a lower weight</example>
		<phraseLemma>we create np for np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was very sensitive to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However the performance of our MLNs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was very sensitive to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the choice of the weight</example>
		<phraseLemma>np be very sensitive to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; on average</phrase>
		<frequency>4</frequency>
		<example>Praline resulted in a x speedup over ERMLN explained in part by much smaller ground networks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with only clauses on average&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np on average</phraseLemma>
	</can>
	<can>
		<phrase>Simply using &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Simply using the top two rules for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; inference turns out to be ineffective as they are often very similar</example>
		<phraseLemma>simply use np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The wordbased approach calculates the entailment score &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the same methods as for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the soft entails evidence earlier between the words in the T/F question and words in a rule in the KB</example>
		<phraseLemma>np use np as for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; linking it to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The entity linking task aims at analyzing each named entity mention in a source document and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;linking it to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its referent in a knowledge base</example>
		<phraseLemma>np link it to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been proved to be &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A collective way of aligning cooccurred mentions to the KB graph &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been proved to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a successful strategy to better represent the source context</example>
		<phraseLemma>np have be prove to be np</phraseLemma>
	</can>
	<can>
		<phrase>We took &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We took a careful look at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the intermediate experiment results and discovered that although CS did not produce a lot more correct linking results than SR did it did promote a great number of good candidates to the top of the ranking list</example>
		<phraseLemma>we take np at np</phraseLemma>
	</can>
	<can>
		<phrase>In Table 1 there is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Table 1 there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a signiﬁcant performance gain after enabling CV</example>
		<phraseLemma>in table 1 there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the number of &lt;NP&gt; is the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For the ﬁnal QCV score computation the upper bound of the computing time to link all the mentions in a document is O where nm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the number of linkable mentions in the document nc is the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; candidates for each mention and nnc is the number of neighbor nodes of a candidate and nnm is the number of neighbors of a mention</example>
		<phraseLemma>np be the number of np be the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consist of &lt;NP&gt; about &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>These KBs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consist of facts about&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the real world mostly in the form of triples eg</example>
		<phraseLemma>np consist of np about np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when one of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The task of knowledge base completion is to complete the triple &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when one of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; h t r is missing</example>
		<phraseLemma>np when one of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; leads to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In contrast since PTransE models relation paths it can take advantage of the relation paths &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between George W Bush and Laura Bush and leads to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more accurate prediction</example>
		<phraseLemma>np between np lead to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we can use &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this subtask we can use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the score function of PTransE to rank candidate relations instead of reranking like in entity prediction</example>
		<phraseLemma>in np we can use np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we explore &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this task we explore&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the effectiveness of PTransE for relation extraction from text</example>
		<phraseLemma>in np we explore np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can learn &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In contrast by successfully integrating the merits of modeling entities and relation paths PTransE &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can learn superior representations of both entities and relations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; knowledge graph completion and relation extraction as shown in our experiments</example>
		<phraseLemma>np can learn np for np</phraseLemma>
	</can>
	<can>
		<phrase>We sincerely thank &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We sincerely thank Yansong Feng for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; insightful discussions and thank all anonymous reviewers for their constructive comments</example>
		<phraseLemma>we sincerely thank np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; more speciﬁcally &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Their method is based on KB information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;more speciﬁcally&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entity descriptions in Wikipedia and Freebase</example>
		<phraseLemma>np more speciﬁcally np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is also associated with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Each node &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is also associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one or more semantic classes called types</example>
		<phraseLemma>np be also associate with np</phraseLemma>
	</can>
	<can>
		<phrase>There is in fact &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;There is in fact&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a signiﬁcant body of work on corpusbased methods for extracting knowledge from text however most of it has addressed relation extraction not the acquisition of type information – roughly corresponding to unary relations</example>
		<phraseLemma>there be in fact np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in the rest of the paper</phrase>
		<frequency>4</frequency>
		<example>We use type” to refer to FIGER types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the rest of the paper&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in the rest of the paper</phraseLemma>
	</can>
	<can>
		<phrase>To work for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To work for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; new or unknown entities we would need an entity linking system such as the ones participating in TAC KBP that identiﬁes and clusters mentions of them</example>
		<phraseLemma>to work for np</phraseLemma>
	</can>
	<can>
		<phrase>To learn &lt;NP&gt; we use &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To learn a score function SGM we use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a multilayer perceptron with one shared hidden layer and an output layer that contains for each type t in T a logistic regression classiﬁer that predicts the probability of t</example>
		<phraseLemma>to learn np we use np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; similar to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use an MLP similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the global model to learn Sc 1 t which predicts the probability of type t occurring in context c Again we use SGD with AdaGrad and minibatch training</example>
		<phraseLemma>we use np similar to np</phraseLemma>
	</can>
	<can>
		<phrase>The main weakness of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The main weakness of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CM is that a large proportion of contexts does not contain sufﬁcient information to infer all types of the entity eg based on our distant supervised training data we label every context of Obama” with author” politician” and Obamas other types in the KB</example>
		<phraseLemma>the main weakness of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; mostly relying on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Early approaches focused on extracting a large number of relations from massive unstructured corpora &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;mostly relying on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependencies at the level of surface text</example>
		<phraseLemma>np mostly rely on np</phraseLemma>
	</can>
	<can>
		<phrase>However the majority of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;However the majority of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; integration approaches nowadays are not designed to deal with many different resources at the same time</example>
		<phraseLemma>however the majority of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; regardless of whether &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;regardless of whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these systems provide links to some generalpurpose inventory come with their own adhoc structure or have no structure at all</example>
		<phraseLemma>np regardless of whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; connect them to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Dutta describe a method for linking arguments in NELL triples to DBPEDIA by combining First Order Logic and Markov Networks Grycner and Weikum semantify PATTYs pattern synsets and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;connect them to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WordNet verbs Lin propose a method to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities</example>
		<phraseLemma>np connect they to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relying solely on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Unlike other sensebased embeddings approaches like which address the inherent polysemy of wordlevel representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relying solely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text corpora SENSEMBED exploits the structured knowledge of a large sense inventory along with the distributional information gathered from text corpora</example>
		<phraseLemma>np rely solely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that associates &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The result of this procedure is a relation speciﬁcity ranking &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that associates each relation r with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its generality Gen</example>
		<phraseLemma>np that associate np with np</phraseLemma>
	</can>
	<can>
		<phrase>As we observed in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we observed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 speciﬁc relations impose constraints on their subjectobject types and tend to show compact domains and ranges in the vector space</example>
		<phraseLemma>as we observe in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; gives us &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The average of sD and sG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;gives us an alignment conﬁdence ζalign for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the pair hri rji</example>
		<phraseLemma>np give we np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; set by extracting &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We created a development &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;set by extracting&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a subset of 1 million triples from the largest linked KB in our experimental setup ie PATTY</example>
		<phraseLemma>np set by extract np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; comprising &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Finally we evaluated the quality of disambiguation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on a publicly available dataset comprising&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; manual annotations for NELL</example>
		<phraseLemma>np on np comprise np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves &lt;NP&gt; showing that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>KBUNIFY &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves the best result showing that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline based on stateoftheart disambiguation is negatively affected by the lack of context for each individual triple</example>
		<phraseLemma>np achieve np show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is an important component of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Therefore developing a robust name error detector for ASR hypotheses &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is an important component of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the system to resolve errors and ambiguity</example>
		<phraseLemma>np be a important component of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained to predict &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>These features are combined in a maximum entropy classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained to predict&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; name errors directly</example>
		<phraseLemma>np train to predict np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is combined with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In our work the problem is factored to use the acoustic confusibility and local word class features for OOV error prediction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is combined with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sentencelevel name posterior for name error prediction</example>
		<phraseLemma>np which be combine with np</phraseLemma>
	</can>
	<can>
		<phrase>We tune &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For RNN models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we tune the hidden layer size and the multitask weight λ for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MEbased wordlevel name error detector we tune the regularization parameter</example>
		<phraseLemma>np we tune np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are categorized into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As shown in Table 1 the BOLT topics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are categorized into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three domains</example>
		<phraseLemma>np be categorize into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; containing &lt;NP&gt; without &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In total we obtain 1 K sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;containing at least one name and 1 K sentences without&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; names</example>
		<phraseLemma>np contain np without np</phraseLemma>
	</can>
	<can>
		<phrase>We also train &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In order to study the effectiveness of using external data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we also train all models on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an enlarged training set including extra nametagged sentences from Reddit</example>
		<phraseLemma>np we also train np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is learned as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Though this example was from the General domain city names represent an important error class in the TRANSTAC data so the term city &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is learned as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a useful cue</example>
		<phraseLemma>np be learn as np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we combine &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we combine&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the language modeling task with the sentencelevel name prediction task and each training sample has labels for both tasks</example>
		<phraseLemma>in this paper we combine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be combined in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For future work it is worthwhile looking into whether continuous word and sentence representations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be combined in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the name error detector to achieve further improvement</example>
		<phraseLemma>np can be combine in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; there is a need for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In order to keep knowledge bases up to date should new facts emerge and to quickly adapt to new domains &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;there is a need for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬂexible and accurate information extraction approaches which do not require manual effort to be developed for new domains</example>
		<phraseLemma>np there be a need for np</phraseLemma>
	</can>
	<can>
		<phrase>Our method does not rely on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our method does not rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; handlabeled training data and is applicable to any domain which is shown in our evaluation on different relations</example>
		<phraseLemma>we method do not rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are combined for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Since different sentences could predict different answers to the query all predictions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are combined for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁnal answer</example>
		<phraseLemma>np be combine for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If the NEC stage concludes that the entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the correct type then the RE stage is not reached</example>
		<phraseLemma>np be not of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are extracted based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition the following NEC features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are extracted based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Nadeau and Hoffmann</example>
		<phraseLemma>np be extract base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by using &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For some relations there is a dramatic improvement &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by using ﬁnegrained FIGER NE features over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; coarsegrained Stanford NE features occasionally FIGER even outperforms OS as for the relation author</example>
		<phraseLemma>np by use np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; show that using &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Liu train a supervised ﬁne grained NERC on Wikipedia and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;show that using those types as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; entity contraints improves precision and recall for a distantly supervised RE on newswire</example>
		<phraseLemma>np show that use np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; proposing the use of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To handle the complexity of extracting MOVELINKs we combine two ideas that have been successfully applied to information extraction tasks namely tree kernels and multipass sieves &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;proposing the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an expanding parse tree as a novel structured feature for training MOVELINK classiﬁers</example>
		<phraseLemma>np propose the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; namely &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Although it has thus far received much less attention than temporal relation extraction there has been a surge of interest in it in recent years as evidenced by the organization of the three shared tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on spatial relation extraction namely&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the spatial role labeling tasks in and as well as this years SpaceEval task</example>
		<phraseLemma>np on np namely np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; described above we create &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Following the joint approach &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;described above we create&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one training instance for each possible role labeling of each triplet of distinct spatial elements in each sentence in a training document</example>
		<phraseLemma>np describe above we create np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; as implemented in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We train the LINK classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using the SVM learning algorithm as implemented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SVMlight software package</example>
		<phraseLemma>np use np as implement in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is either &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If we adopted the aforementioned joint method as is for extracting MOVELINKs each &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;instance would correspond to an octuple of the form where each participant in the octuple is either&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a distinct spatial element with a role or the NULL element</example>
		<phraseLemma>np in np be either np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while the rest of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Positive training instances are those pairs annotated as being part of a MOVELINK in the training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while the rest of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the candidate pairs are negative training instances</example>
		<phraseLemma>np while the rest of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are ordered by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Typically sieves &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are ordered by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision with the hope of reducing the number of erroneous decisions passed from the earlier sieves to the later sieves Lee Chambers</example>
		<phraseLemma>np be order by np</phraseLemma>
	</can>
	<can>
		<phrase>In particular while &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular while&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each sieve exploits the knowledge of whether a spatial element has been assigned a role by an earlier sieve it does not exploit the knowledge of what the role is</example>
		<phraseLemma>in particular while np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; combines &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To train a classiﬁer on instances containing the original features and SRCTs we employ SVMlightT K which trains an SVM classiﬁer using the features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a linear kernel trains an SVM classiﬁer using only the SRCTs with a convolution kernel and combines&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these two kernels using a composite kernel</example>
		<phraseLemma>np with np combine np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are created in &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The test instances with structured features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are created in the same way as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training instances with one notable difference</example>
		<phraseLemma>np be create in np as np</phraseLemma>
	</can>
	<can>
		<phrase>Of particular interest are &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Of particular interest are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the MOVELINK scores under the True” columns</example>
		<phraseLemma>of particular interest be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; is signiﬁcant</phrase>
		<frequency>4</frequency>
		<example>While the results obtained using extracted elements exhibit different trends SieveSRCTss OVERALL improvement of 1 absolute Fscore &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over Baseline is signiﬁcant&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np over np be signiﬁcant</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which resulted from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our approach exploited expanding parse trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which resulted from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a novel combination of multipass sieves and tree kernels achieving stateoftheart results on two key SpaceEval tasks</example>
		<phraseLemma>np which result from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; to identify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Similarly Agarwal and Rambow build character networks using tree kernels &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on parse trees to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; interacting agents</example>
		<phraseLemma>np on np to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may be associated with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Given a literary novel our objective is to produce a list of characters where each character &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may be associated with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; one or more names</example>
		<phraseLemma>np may be associate with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may be due to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While novels written before had slightly more characters on average this effect &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may be due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the smaller number of works available from this period</example>
		<phraseLemma>np may be due to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; resulting in &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We then aggregate these word and phrase kernels into sentence and documents kernels through convolution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;resulting in higher kernel values between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantically related sentences</example>
		<phraseLemma>np result in np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; rather than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In parallel Collins and Duffy developed the ﬁrst tree kernels to compare trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on their topology rather than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the similarity between their nodes</example>
		<phraseLemma>np base on np rather than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; developed &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Similarly Gärtner &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;developed graph kernels based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; random walks and Srivastava used them on dependency trees with Vector Tree Kernels adding node similarity based on word embeddings from SENNA and reporting improvements over SSTK</example>
		<phraseLemma>np develop np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be computed in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>By storing intermediate values of composite vectors a phrase kernel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be computed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; O time regardless of the phrase length therefore the whole computation process has O complexity</example>
		<phraseLemma>np can be compute in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; assigns &lt;NP&gt; to them</phrase>
		<frequency>4</frequency>
		<example>Additionally food items that appear in instructions but are not part of the recipe ingredient list text are not included in SIMMR although MILK &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;assigns ingredient ids to them&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np assign np to they</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; describing &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Finally the 1 group deals &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with term vectors describing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the source and target instructions as well as the instructions ﬁrst words</example>
		<phraseLemma>np with np describe np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; about what &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>On the other hand people often have prior knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;about what&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; potential topics should exist in a given text corpus</example>
		<phraseLemma>np about what np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is proportional to the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As shown in the process the prior probability of assigning a document to a topic &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is proportional to the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; documents already assigned to the topic</example>
		<phraseLemma>np be proportional to the number of np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; for &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our proposed TSDPMM model for document clustering on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 datasets where each cluster corresponds to a topic</example>
		<phraseLemma>we evaluate np for np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; learnt by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The results show that TSDPMM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using prior topics learnt by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; DPMM outperforms DPMM</example>
		<phraseLemma>np use np learn by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been increasingly focused on for &lt;NP&gt; to minimize &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Recently neural network based sentence modeling approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been increasingly focused on for their ability to minimize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the efforts in feature engineering such as Neural BagofWords Recurrent Neural Network Recursive Neural Network and Convolutional Neural Network</example>
		<phraseLemma>np have be increasingly focus on for np to minimize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; among the &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Intuiativly the reset gates control how to select the output information of the left and right children which result to the current new activation h By the update gates the activation of a parent neuron can be regarded as a choice &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;among the&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the current new activation h the left child and the right child</example>
		<phraseLemma>np among the np</phraseLemma>
	</can>
	<can>
		<phrase>We set &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We set word embedding size d = 1 on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the TREC dataset and d = 1 on the Stanford Sentiment Treebank dataset</example>
		<phraseLemma>we set np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; while the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Although CNN based methods outperform our model on SST 1 and SST 1 the number of parameters of GRNN ranges from 1 K to 1 K &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;while the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parameters is about 1 K in CNN</example>
		<phraseLemma>np while the number of np</phraseLemma>
	</can>
	<can>
		<phrase>As a result we set &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a result we set&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; c and j to 1 and respectively</example>
		<phraseLemma>as a result we set np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is coupled with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the EMbased theme extraction scheme the log odds score pzC indid cates the extent to which a document d &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is coupled with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a speciﬁc theme C</example>
		<phraseLemma>np be couple with np</phraseLemma>
	</can>
	<can>
		<phrase>At &lt;NP&gt; we select &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;At each iteration we select&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; any cluster in which there are more than documents with PAV scores higher than 1</example>
		<phraseLemma>at np we select np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be implemented in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While PAVEM and LDA &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be implemented in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parallel computation this indicates that PAVEM may be more efﬁcient to obtain themes for a larger set of PubMed documents</example>
		<phraseLemma>np can be implement in np</phraseLemma>
	</can>
	<can>
		<phrase>We compare it with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To assess the performance of our system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we compare it with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a baseline approach considering only the most frequent words in section titles</example>
		<phraseLemma>np we compare it with np</phraseLemma>
	</can>
	<can>
		<phrase>If we look at &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If we look at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the average length of the false positive sequences it is 1 for SVMs and 1 for CRFs</example>
		<phraseLemma>if we look at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; also that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>The results in Table 1 show &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;also that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a simple baseline relying on the 1 most frequent tokens in section titles achieves surprisingly good results especially with the intersectionbased metrics</example>
		<phraseLemma>np also that np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we presented &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we presented a simple yet effective approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extract sequences of biographical sections from Wikipedia persons’ pages</example>
		<phraseLemma>in this work we present np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; provided by</phrase>
		<frequency>4</frequency>
		<example>We evaluate our algorithm &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the benchmark dataset provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np provide by</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we develop &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For phonetic information we develop an encoding scheme for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Roman Urdu UrduPhone motivated from Soundex</example>
		<phraseLemma>for np we develop np for np</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows statistics of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows statistics of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the datasets in comparison with the gold standard</example>
		<phraseLemma>table 1 show statistics of np</phraseLemma>
	</can>
	<can>
		<phrase>We make &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We make prediction using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a 1 KT jV jdimensional matrix O Different from the original CBOW model the extra parameter introduced in the matrix O allows us to maintain the relative order of the components and treat the radical differently from the rest components</example>
		<phraseLemma>we make np use np</phraseLemma>
	</can>
	<can>
		<phrase>As &lt;NP&gt; of fact &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As a matter of fact&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a majority of Chinese words are compounds of two characters</example>
		<phraseLemma>as np of fact np</phraseLemma>
	</can>
	<can>
		<phrase>The results shown in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The results shown in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 1 rows of Table 1 are similar to those in the word similarity evaluation</example>
		<phraseLemma>the result show in np</phraseLemma>
	</can>
	<can>
		<phrase>The work described in this paper was supported by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The work described in this paper was supported by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the grants from the Research Grants Council of Hong Kong the grants from the National Natural Science Foundation of China and and a PolyU internal grant BCB 1</example>
		<phraseLemma>the work describe in this paper be support by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; produced by &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Recently a series of methods have been developed which train a classiﬁer for each label organize the classiﬁers in a partially ordered structure and take predictions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;produced by the former classiﬁers as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the latter classiﬁers features</example>
		<phraseLemma>np produce by np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can model &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>When training a classiﬁer for one label the predictionsasfeatures methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can model dependencies between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; former labels and the current label but they cant model dependencies between the current label and the latter labels</example>
		<phraseLemma>np can model np between np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are organized in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the predictionsasfeatures methods the classiﬁers are organized in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a partially ordered structure and take predictions produced by the former classiﬁers as features</example>
		<phraseLemma>in np be organize in np</phraseLemma>
	</can>
	<can>
		<phrase>We use the sum of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use the sum of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; negative log likelihood losses of all classiﬁers as the global loss function</example>
		<phraseLemma>we use the sum of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; results in &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Minimizing this global loss function is inequivalent to minimizing the loss function of each base classiﬁer separately since minimizing the global loss function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;results in feedbacks from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; latter classiﬁers</example>
		<phraseLemma>np result in np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are updated according to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Consequently when minimizing the global loss function the weights of the kth classiﬁer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are updated according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; not only the loss of the kth classiﬁer but also the losses of the latter classiﬁers</example>
		<phraseLemma>np be update accord to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; due to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Besides they can also model the dependencies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between current label and latter labels due to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the feedbacks incorporated by the joint learning algorithm</example>
		<phraseLemma>np between np due to np</phraseLemma>
	</can>
	<can>
		<phrase>We reports &lt;NP&gt; in terms of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We reports the detailed results in terms of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different evaluation metrics on different data sets in table 1</example>
		<phraseLemma>we report np in term of np</phraseLemma>
	</can>
	<can>
		<phrase>Of course there are &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Of course there are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; numerous additional examples across many domains</example>
		<phraseLemma>of course there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used to infer &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Such measures &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used to infer&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similarity among document groups</example>
		<phraseLemma>np can be use to infer np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we identiﬁed &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each NSF program we identiﬁed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top n most similar programs ranked by our sim function where n ∈</example>
		<phraseLemma>for np we identiﬁed np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; due to &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We were unsuccessful in evaluating alternative weighted similarity measures mentioned in Section 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;due to their aforementioned issues with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; scalability and the size of the NSF dataset</example>
		<phraseLemma>np due to np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; consider &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To illustrate the potential synergies &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between CCR and NEL consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 documents in Figure 1 containing 1 mentions with candidate entities from a KB</example>
		<phraseLemma>np between np consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; mentions to &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In its NEL procedure C 1 EL disambiguates &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;mentions to entities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the YAGO knowledge base yagoknowledge</example>
		<phraseLemma>np mention to np in np</phraseLemma>
	</can>
	<can>
		<phrase>We perform &lt;NP&gt; on &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We perform NEL on the sentences of a mention group using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; namedentity popularity statistics and context to obtain the best matching entity its conﬁdence score and the corresponding Wikipedia page</example>
		<phraseLemma>we perform np on np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is indexed with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Consider a sentence that consists of n words where each word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is indexed with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its position in the sentence</example>
		<phraseLemma>np be index with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; shares &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We call such com binations typeII combinations such a combination &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;shares the same representation as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the above subhypergraph</example>
		<phraseLemma>np share np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is not directly applicable to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The technique &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is not directly applicable to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our task where a hypergraph representation is used to encode overlapping mentions</example>
		<phraseLemma>np be not directly applicable to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; relied on &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Speciﬁcally both of these two previous models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;relied on an additional million words from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; PubMed abstracts to learn word clusters as additional features which we do not have access to</example>
		<phraseLemma>np rely on np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; ranging from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>FINET generates candidate types &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using a sequence of multiple extractors ranging from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; explicitly mentioned types to implicit types and subsequently selects the most appropriate using ideas from wordsense disambiguation</example>
		<phraseLemma>np use np range from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is reasonable in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Named entities with the same CG type in a coordinating relation and identical mentions share the candidate set the latter &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is reasonable in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; short input</example>
		<phraseLemma>np be reasonable in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by adding &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Moreover if the named entity is the subject of the clause and if the clause contains a direct object we form a new lexical type &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by adding the direct object as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a noun modiﬁer of the deverbal noun</example>
		<phraseLemma>np by add np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consists of &lt;NP&gt; in which &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>This relevant part &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consists of the clause in which&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the entity occurs and the subordinate clauses that do not contain another entity</example>
		<phraseLemma>np consist of np in which np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with respect to &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>WSD aims to disambiguate a word or phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with respect to a type system as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WordNet eg from player” to hplayer 1 i</example>
		<phraseLemma>np with respect to np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to enrich &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Its goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to enrich&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; contextual information to boost disambiguation</example>
		<phraseLemma>np be to enrich np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; vs 1 for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>FINET also tended to use more distinct correct types in NYT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;vs 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Hyena</example>
		<phraseLemma>np vs 1 for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to extract &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The goal of ERD &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to extract named entities in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; text and link extracted names to a knowledge base usually Wikipedia or Freebase</example>
		<phraseLemma>np be to extract np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; jointly models &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In summary JERL &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;jointly models&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the NER and linking and leverages mutual dependency between them to predict coherent outputs</example>
		<phraseLemma>np jointly model np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to ﬁnd &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>From the mention generation perspective JERL actually considers every possible assignment and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the optimal a</example>
		<phraseLemma>np be able to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; in &lt;NP&gt; there is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each entity in a knowledge base there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; category information available</example>
		<phraseLemma>for np in np there be np</phraseLemma>
	</can>
	<can>
		<phrase>Of course in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Of course in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our application we will ﬁnd it useful to develop generic algorithms that can compress any text</example>
		<phraseLemma>of course in np</phraseLemma>
	</can>
	<can>
		<phrase>If we start with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If we start with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a uniform distribution the ﬁrst few characters may not compress very well but soon we will converge onto a good tree and good compression</example>
		<phraseLemma>if we start with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that combines &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>PPMZ also implements an adaptive P &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that combines context length number of previous ESC in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the context etc</example>
		<phraseLemma>np that combine np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; may be useful for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We may therefore suppose that a set of Viterbi word alignments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;may be useful for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; compression</example>
		<phraseLemma>np may be useful for np</phraseLemma>
	</can>
	<can>
		<phrase>In order to interpret &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to interpret&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the bits produced by the compressor our decompressor must also have access to the same Viterbi alignments</example>
		<phraseLemma>in order to interpret np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; works better on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Huffman &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;works better on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; relative offsets because the common 1 ” gets a short bit code</example>
		<phraseLemma>np work better on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains more than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Therefore if the source segment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains more than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words we use only monolingual PPMC to compress the target</example>
		<phraseLemma>np contain more than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is signiﬁcantly faster than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Because singlepass alignment &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is signiﬁcantly faster than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; traditional multipass we also investigate its impact on an overall Moses pipeline for phrasebased machine translation</example>
		<phraseLemma>np be signiﬁcantly faster than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; should be in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For sentences in one document talking about one or several speciﬁc topics the adjacent sentences &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;should be in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a coherent order</example>
		<phraseLemma>np should be in np</phraseLemma>
	</can>
	<can>
		<phrase>In this section we describe how &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this section we describe how&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; to leverage recurrent neural network for sentencelevel language modeling</example>
		<phraseLemma>in this section we describe how np</phraseLemma>
	</can>
	<can>
		<phrase>We integrate &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To capture the longer history &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we integrate the sentence history into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the wordlevel language model from sentencelevel language model which forms a hierarchical recurrent neural network</example>
		<phraseLemma>np we integrate np into np</phraseLemma>
	</can>
	<can>
		<phrase>We also apply &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also apply our HRNNLM to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SMT reranking task in an open ChineseEnglish translation dataset</example>
		<phraseLemma>we also apply np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained with &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The language model for SMT is a 1 gram language model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained with the English documents in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training data</example>
		<phraseLemma>np train with np in np</phraseLemma>
	</can>
	<can>
		<phrase>We also include &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also include these models in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions</example>
		<phraseLemma>we also include np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be split into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our objective function &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be split into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two parts a convex and differentiable part and a convex but nondifferentiable part</example>
		<phraseLemma>np can be split into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be solved by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In fact the two problems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be solved by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; nearly identical algorithms because they are convex conjugates of each other</example>
		<phraseLemma>np can be solve by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; introduce &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We will review graphical models over strings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in section 1 and brieﬂy introduce&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our sample problem in section 1</example>
		<phraseLemma>np in np introduce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is guaranteed to be &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However if DD converges then its solution &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is guaranteed to be&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the true MAP assignment</example>
		<phraseLemma>np be guarantee to be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are determined by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The probabilities of competing edits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in a given context are determined by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a loglinear model with weight vector θ and features that are meant to pick up on phonological phenomena</example>
		<phraseLemma>np in np be determine by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; varied from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Similarly &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the CELEX data the runtime on Model 1 varied from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; about 1 hour to nearly 1 days</example>
		<phraseLemma>np on np vary from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; presented in &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In order to perform constrained decoding over local probability distributions we have opted for a strategy ﬁrst &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;presented in Muller for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SDRT</example>
		<phraseLemma>np present in np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the values for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In MaxEnt &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the values for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the different parameters w are obtained by maximizing the loglikelihood of the training data T with respect to the model</example>
		<phraseLemma>np the value for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that we applied to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We introduce a new approach to argumentation mining &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that we applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a parallel German/English corpus of short texts annotated with argumentation structure</example>
		<phraseLemma>np that we apply to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which enables the use of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>All relations pointing to edges are rewritten to point to the source node of the original target edge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which enables the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; standard graph algorithms</example>
		<phraseLemma>np which enable the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is shown in Figure 1</phrase>
		<frequency>4</frequency>
		<example>An example text from the corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in its reduced form is shown in Figure 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np be show in figure 1</phraseLemma>
	</can>
	<can>
		<phrase>Identifying &lt;NP&gt; according to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Identifying the structure of argumentation according to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our scheme involves choosing one segment as the central claim of the text deciding how the other segments are related to the central claim and to each other identifying the argumentative role of each segment and ﬁnally the argumentative function of each relation</example>
		<phraseLemma>identify np accord to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In our corpus the ﬁrst segment is the central claim &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the texts 1</example>
		<phraseLemma>np in of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which was also used in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Finally we compare our models to the wellknown mstparser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which was also used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the discourse parsing experiments of Baldridge</example>
		<phraseLemma>np which be also use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is signiﬁcant in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The improvement of MP over EP equal and best &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is signiﬁcant in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both languages</example>
		<phraseLemma>np be signiﬁcant in np</phraseLemma>
	</can>
	<can>
		<phrase>Finally note that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Finally note that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the EG best model gives the highest total score when summed over all levels followed by EG equal and then MPpr</example>
		<phraseLemma>finally note that np</phraseLemma>
	</can>
	<can>
		<phrase>It produces &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It produces top results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; extrinsic evaluation as well</example>
		<phraseLemma>it produce np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; use &lt;NP&gt; within &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Existing supervised aligners &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;use various contextual features within&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a learning algorithm for this purpose</example>
		<phraseLemma>np use np within np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; into the other</phrase>
		<frequency>4</frequency>
		<example>MacCartney Thadani and McKeown and Thadani frame alignment as a set of phrase edit operations that transform one snippet &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;into the other&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np into the other</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is important because &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This distinction &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is important because&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; certain features apply only to certain types of words and certain features can be more important for certain types of words</example>
		<phraseLemma>np be important because np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; apply only to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This distinction is important because certain features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;apply only to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; certain types of words and certain features can be more important for certain types of words</example>
		<phraseLemma>np apply only to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that encodes whether &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For named entities we consider acronymy as exact match use membership in two lists of alternative country names and countrynationality pairs as features and include a feature &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that encodes whether&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; T and T belong to the same named entity</example>
		<phraseLemma>np that encode whether np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allows the use of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The two representations are complementary – the entitybased representation can capture equivalences between mentions of different lengths of a named entity while the wordbased representation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allows the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; similarity resources for named entity words</example>
		<phraseLemma>np allow the use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; normalized by the total number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We use as a feature the sum of similarities between the matched neighbors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;normalized by the total number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; content words in the two neighborhoods</example>
		<phraseLemma>np normalize by the total number of np</phraseLemma>
	</can>
	<can>
		<phrase>We employ &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To promote high recall &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we employ the higher of a word pairs own lexical similarity and the lexical similarity of the cooperating pair with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the highest stage 1 probability as a stage 1 feature</example>
		<phraseLemma>np we employ np with np</phraseLemma>
	</can>
	<can>
		<phrase>From &lt;NP&gt; this is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;From a design perspective this is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an aligner that does not address scenarios 1 and 1 of Section 1</example>
		<phraseLemma>from np this be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are more important for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The extreme overall performance degradation indicates that contextual features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are more important for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the aligner than lexical features</example>
		<phraseLemma>np be more important for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; labeling them with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Semantic role labeling is the task of identifying the semantic arguments of a predicate and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;labeling them with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their semantic roles</example>
		<phraseLemma>np label they with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; rely on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Most prior work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on SRL rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; syntactic parses provided as input and use locally estimated classiﬁers for each spanrole pair that are only combined at prediction time</example>
		<phraseLemma>np on np rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; falls short of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For FrameNetstyle SRL Kshirsagar recently proposed the use of PropBankbased features but their system performance &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;falls short of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the state of the art</example>
		<phraseLemma>np fall short of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Formally we are given a sentence x &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which a predicate t with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lexical unit has been marked</example>
		<phraseLemma>np in which np with np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we compare to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For CoNLL 1 we compare to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Pradhan who report results with the ASSERT system and to the model of Zhou and Xu</example>
		<phraseLemma>for np we compare to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; leading to an increase in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Overall using structured learning improves recall at a slight expense of precision when compared to local learning &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;leading to an increase in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the complete argument structure accuracy Comp</example>
		<phraseLemma>np lead to a increase in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which contains &lt;NP&gt; as well as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Prominent alternatives are WiseNet which offers 1 synsets of relational phrases PPDB &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which contains over million paraphrase pairs as well as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; DIRT and VerbOcean which inspired the approach and results pursued here</example>
		<phraseLemma>np which contain np as well as np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; are aligned to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If relational phrases and are aligned to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; WordNet verb senses V b 1 and V b 1 which are in a hyponymy relationship then this is evidence that is more speciﬁc than</example>
		<phraseLemma>if np be align to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; out of &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For comparison PATTY produced 1 subsumption links &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;out of 1 phrases with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; approximately 1 roots</example>
		<phraseLemma>np out of np with np</phraseLemma>
	</can>
	<can>
		<phrase>For each of &lt;NP&gt; we used &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For each of the three systems we used&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the full set of hypernymy links they produce which consisted of 1 K links from PATTY 1 K links from HARPY and 1 K links from RELLY</example>
		<phraseLemma>for each of np we use np</phraseLemma>
	</can>
	<can>
		<phrase>We provided &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For each relational phrase &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we provided annotators with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; type information about the phrase arguments and examples of sentences that use the relational phrase</example>
		<phraseLemma>np we provide np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with a &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Additionally removing YAGO type information harms precision &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; precision of 1 ± 1 with 1 conﬁdence Wilson score interval for a random sample of examples</example>
		<phraseLemma>np with a np</phraseLemma>
	</can>
	<can>
		<phrase>Given &lt;NP&gt; in the form of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Given a query in the form of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an anonymized plot description from one website the task is to rank the anonymized plot descriptions from the other dataset using relational phrase similarity</example>
		<phraseLemma>give np in the form of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is above &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A phrase is mapped to a synset if the Jaccard similarity between tokens of extracted relation and tokens of one of the phrases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the synset is above&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a threshold</example>
		<phraseLemma>np in np be above np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was extended by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the WordNet taxonomy &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was extended by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 novel noun synsets with hypernymhyponym links</example>
		<phraseLemma>np be extend by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represent &lt;NP&gt; across &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Given a recipe our task is to segment it into text spans that describe individual actions and construct an action graph whose nodes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represent actions and edges represent the ﬂow of arguments across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; actions for example as seen in Fig 1</example>
		<phraseLemma>np represent np across np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; determines whether &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The syntactic type &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;determines whether the argument is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the direct object or a prepositional phrase argument of the verb in the recipe text</example>
		<phraseLemma>np determine whether np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; then &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If a span introduces raw ingredient or new location into the recipe &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;then o = 1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Fig 1 this occurs for each of the spans that represent raw ingredients as well as oven” and into loaf pan”</example>
		<phraseLemma>np then np in np</phraseLemma>
	</can>
	<can>
		<phrase>We generate &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For each argument &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we generate a string using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a bigram model P = Q P where w is the word of a</example>
		<phraseLemma>np we generate np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by searching for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We collected recipes from allrecipescom &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by searching for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dish names</example>
		<phraseLemma>np by search for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Most of these approaches &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do interactive learning in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; virtual environments or simulations while we learn from the redundancy seen in the text of different instances of similar recipes</example>
		<phraseLemma>np do np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; for learning &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our overall approach to be described in this section is similar to the works on joint inference &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with global constraints for learning&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; event relations and process structures</example>
		<phraseLemma>np with np for learn np</phraseLemma>
	</can>
	<can>
		<phrase>We need to identify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To address the ﬁrst challenge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we need to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a set of words from sarcastic utterances which have a ﬁgurative/sarcastic sense</example>
		<phraseLemma>np we need to identify np</phraseLemma>
	</can>
	<can>
		<phrase>We also considered &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We also considered a statistical machine translation alignment method IBM Model 1 with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; HMM alignment implemented in Giza</example>
		<phraseLemma>we also consider np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is similar to &lt;NP&gt; described in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The similarity measure in the kernel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is similar to the algorithm M V M Ewe described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Algorithm 1 but instead of measuring the similarity between the sense vectors of t and the vector representation of t in test message now we measure the similarity between two tweets ur and us</example>
		<phraseLemma>np be similar to np describe in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; shows &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition our new SVM kernel method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using word embeddings shows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signiﬁcantly better results when compared to the SV Mbl</example>
		<phraseLemma>np use np show np</phraseLemma>
	</can>
	<can>
		<phrase>To get &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To get the balance between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two approaches Yang and Callan Zhu and Tuan combine both statistical and linguistic features in the process of ﬁnding taxonomic relations</example>
		<phraseLemma>to get np between np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to collect &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We randomly pick pairs of synonyms in WordNet and for each pair &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we use the Web search engine to collect&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sample sentences in which both terms of the pair are mentioned</example>
		<phraseLemma>np we use np to collect np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; achieves signiﬁcantly &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our combined method &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;achieves signiﬁcantly better performance than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the previous stateoftheart methods in terms of Fmeasure and Recall for all the 1 domains</example>
		<phraseLemma>np achieve signiﬁcantly np than np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by running &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition we also examine the interdependence of the 1 introduced aspects of trustiness &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by running the system with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the combination of only two aspects Importance and Accuracy</example>
		<phraseLemma>np by run np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; can achieve &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>It shows that the method of ranking of the sites &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the knowledgebased facts can achieve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the effectiveness as good as the traditional ranking method using PageRank score</example>
		<phraseLemma>np base on np can achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; described in &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This is the ﬁrst learning system that can solve logic grid puzzles &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;described in natural language in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a fully automated manner</example>
		<phraseLemma>np describe in np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is tagged with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the training phase each entity in a clue is tagged with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; its respective class</example>
		<phraseLemma>np in np be tag with np</phraseLemma>
	</can>
	<can>
		<phrase>Features in &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Features in Maximum Entropy model are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; functions from context and classes to the set of real numbers</example>
		<phraseLemma>feature in np be np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; which make use of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For puzzles which make use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a different set of constraints such as Lynda sat on an even numbered position” can be easily integrated into the vocabulary and the system can then be trained to identify those relations for new puzzles</example>
		<phraseLemma>for np which make use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used in &lt;NP&gt; used in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Word alignment is used to extract translation rules in various way such as the phrase pairs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used in a phrasebased SMT system the hierarchical rules used in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a HIERO system and the sophisticated translation templates used in treebased SMT systems</example>
		<phraseLemma>np use in np use in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was set to 1 in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Within each iteration fast align was run with default settings except initial diagonal tension &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was set to 1 in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ﬁrst iteration to avoid overly strong monotone preference at the beginning of training</example>
		<phraseLemma>np be set to 1 in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are listed in Table 1</phrase>
		<frequency>4</frequency>
		<example>Experimental results for JapaneseEnglish and GermanEnglish translations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in both directions are listed in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np be list in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; according to &lt;NP&gt; provided by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Then all biphrases from the SMT systems phrase table that match an annotated ngram &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;according to the source token alignments provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the decoder are removed from the main phrase table and stored in a separate positive” phrase table</example>
		<phraseLemma>np accord to np provide by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; compared to &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Furthermore results shown in Table 1 point out the complementarity between negative models and positive models with a drop of almost BLEU points &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;compared to the corresponding conﬁguration using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; all models when removing one type of models on both translation directions</example>
		<phraseLemma>np compare to np use np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; have been applied to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In particular these techniques have been applied to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Statistical Machine Translation ﬁrst to estimate continuousspace translation models and more recently to implement endtoend translation systems</example>
		<phraseLemma>in np have be apply to np</phraseLemma>
	</can>
	<can>
		<phrase>We propose to train &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We propose to train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these two sets of parameters by alternatively updating θ through SGD on the training corpus and updating λ using conventional algorithms on the development data</example>
		<phraseLemma>we propose to train np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is trained on &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the training scenario the CTM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is trained on the same parallel data as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the one used for the baseline system</example>
		<phraseLemma>np be train on np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are based on &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>All translation systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are based on the open source implementation of the bilingual ngram approach to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; MT For the NN structure each vocabularys word is projected into a dimension space followed by two hidden layers of and units</example>
		<phraseLemma>np be base on np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; by using &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this work we initialize CTMs parameters &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;by using a pretraining procedure based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the models probabilistic interpretation and NCE algorithm to produce quasinormalized scores while similar work in only uses unnormalized scores</example>
		<phraseLemma>np by use np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; exist such as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Finally although N best rescoring is used in this work to facilitate the discriminative training other CTMs integration into SMT systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;exist such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; lattice reranking or direct decoding with CTM</example>
		<phraseLemma>np exist such as np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we have proposed &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we have proposed a new discriminative training procedure for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; continuousspace translation models which correlates better with translation quality than conventional training methods</example>
		<phraseLemma>in this paper we have propose np for np</phraseLemma>
	</can>
	<can>
		<phrase>In this paper we propose &lt;NP&gt; to address &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we propose a paraphrasing model to address&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of system combination for machine translation</example>
		<phraseLemma>in this paper we propose np to address np</phraseLemma>
	</can>
	<can>
		<phrase>In addition to consider &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In addition to consider&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a diverse set of plausible fused translations we develop a hybrid combination architecture where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target and then make the final selection among all fused translations</example>
		<phraseLemma>in addition to consider np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; represents &lt;NP&gt; represents &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In Table 1 CN represents&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; confusion network LD represents Lattice Decoding PARA represents paraphrasing model proposed in this paper Backbone_ represents that is carried out on selected backbones in contrast with the hybrid combination architecture</example>
		<phraseLemma>in np represent np represent np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can adapt to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this way the system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can adapt to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; multiple genres while preventing crossgenre contamination</example>
		<phraseLemma>np can adapt to np</phraseLemma>
	</can>
	<can>
		<phrase>We adapt &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In largescale experiments &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we adapt a multigenre baseline system to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; patents lectures and news articles</example>
		<phraseLemma>np we adapt np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is conceptually similar to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our adaptation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is conceptually similar to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hierarchical Bayesian domain adaptation but both weights and feature values depend on Di and we use regularization</example>
		<phraseLemma>np be conceptually similar to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that comes with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Model tuning and adaptation are performed with AdaGrad an online subgradient method with an adaptive learning rate &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that comes with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; good theoretical guarantees</example>
		<phraseLemma>np that come with np</phraseLemma>
	</can>
	<can>
		<phrase>In order to extend &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In order to extend&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model eﬃciently within a streaming data environment we make use of a suﬃxarray implementation for our phrase table</example>
		<phraseLemma>in order to extend np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; over &lt;NP&gt; of time</phrase>
		<frequency>4</frequency>
		<example>LSTM is a sequence learning technique which uses a memory cell to preserve a state &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;over a long period of time&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np over np of time</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; are &lt;NP&gt; respectively</phrase>
		<frequency>4</frequency>
		<example>The compositional parameters for our TreeLSTM systems &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with memory dimensions and are 1 and 1 respectively&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np be np respectively</phraseLemma>
	</can>
	<can>
		<phrase>Table 1 shows that for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Table 1 shows that for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; LSick and LSick we obtained an average 1 best Pearson correlation and best Spearman correlation coefﬁcient</example>
		<phraseLemma>table 1 show that for np</phraseLemma>
	</can>
	<can>
		<phrase>We conclude that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We conclude that our densevectorspacebased ReVal metric is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; simple elegant and effective with stateoftheart results</example>
		<phraseLemma>we conclude that np be np</phraseLemma>
	</can>
	<can>
		<phrase>We are able to achieve &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For these two tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we are able to achieve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stateoftheart performance by adding the two CSLM features to all available or selected feature sets</example>
		<phraseLemma>np we be able to achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; as shown in Table 1</phrase>
		<frequency>4</frequency>
		<example>CSLM features bring further improvements &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on preselected feature sets as shown in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np on np as show in table 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to regard &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our idea &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to regard&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word translation distributions derived from sourcetarget bilingual data as the correct translation distributions and use them to learn discriminately</example>
		<phraseLemma>np be to regard np</phraseLemma>
	</can>
	<can>
		<phrase>To generalize beyond &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To generalize beyond&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the vocabulary of the sourcetarget data we appeal to word embeddings</example>
		<phraseLemma>to generalize beyond np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; matches the performance of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We show empirically that our method outperforms prior work on multilingual tasks &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;matches the performance of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; prior work on monolingual tasks and scales linearly with the size of the input data</example>
		<phraseLemma>np match the performance of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used to obtain &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For instance a resource such as the Paraphrase Database could be used to further constrain the embeddings obtained this could be useful if the resource &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used to obtain&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a paraphrase dictionary contained more or different information than the corpus statistics used in the decomposition</example>
		<phraseLemma>np use to obtain np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that we have &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Second note &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that we have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two dictionaries one for the words and one for the contexts</example>
		<phraseLemma>np that we have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is required to provide &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However this approach may be unsuitable for simultaneous interpretation where the machine translation system &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is required to provide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translations within a reasonably short space of time after words have been spoken</example>
		<phraseLemma>np be require to provide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when used as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In it was shown that the prediction and use of soft boundaries in the source language text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reordering constraints can improve the quality of a speech translation system</example>
		<phraseLemma>np when use as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was segmented using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The Chinese corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was segmented using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Stanford Chinese word segmenter according to the Chinese Penn Treebank standard</example>
		<phraseLemma>np be segmented use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to parse &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Berkeley parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to parse&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the English side of each parallel corpus as well as for parsing the French source side</example>
		<phraseLemma>np be use to parse np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as well as by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Language use is known to be inﬂuenced by personality traits &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as well as by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sociodemographic characteristics such as age or mother tongue</example>
		<phraseLemma>np as well as by np</phraseLemma>
	</can>
	<can>
		<phrase>In English for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In English for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; example women tend to use ﬁrstperson pronouns such as I ” more than men but this does not guarantee a genderbased usage difference for say je” in French</example>
		<phraseLemma>in english for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We assess this by investigating whether trait detection performs as well on translated data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; native text</example>
		<phraseLemma>np as on np</phraseLemma>
	</can>
	<can>
		<phrase>We therefore turned to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We therefore turned to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TEDx 1 for fren data</example>
		<phraseLemma>we therefore turn to np</phraseLemma>
	</can>
	<can>
		<phrase>Unlike &lt;NP&gt; there is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Unlike enfr there is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; no easily accessible parallel data available for fren where the source is native French</example>
		<phraseLemma>unlike np there be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as if &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Interestingly the manual translation scores higher than the native English &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the translators are adding more gender indications to the text</example>
		<phraseLemma>np as if np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; to compute &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use our new method to compute aligned wordembeddings for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; twentyone languages using English as a pivot language</example>
		<phraseLemma>we use np to compute np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; minimizes &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Then the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;minimizes the Euclidean distance between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both vectors</example>
		<phraseLemma>np minimize np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; there are &lt;NP&gt; of interest</phrase>
		<frequency>4</frequency>
		<example>At the level 1 &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;there are two discontinuous relations of interest&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np there be np of interest</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is considered by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>EXPANSION &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is considered by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Mexp as the 1 highest marked relation whereas the continuity hypothesis predicts it to be one of the lowest marked relation which is correctly captured by Mall</example>
		<phraseLemma>np be consider by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be performed in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our ﬁndings show that while deception detection &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be performed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; short texts even in the absence of a predetermined domain gender and age prediction in deceptive texts is a challenging task</example>
		<phraseLemma>np can be perform in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; of 1 and &lt;NP&gt; of 1</phrase>
		<frequency>4</frequency>
		<example>Participants ages range from to years &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with an average age of 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; and a standard deviation of 1</example>
		<phraseLemma>np with np of 1 and np of 1</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; represents the number of &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Each feature &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;represents the number of words in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sentence belonging to a speciﬁc semantic class</example>
		<phraseLemma>np represent the number of np in np</phraseLemma>
	</can>
	<can>
		<phrase>We performed &lt;NP&gt; at &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We performed the evaluations at&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; user level by collapsing all the lies from one user into one instance and all the truths into another instance</example>
		<phraseLemma>we perform np at np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; ranging from 1 to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The remaining sets of features achieved accuracy values &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;ranging from 1 to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 which still represent a noticeable improvement over the random baseline</example>
		<phraseLemma>np range from 1 to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; even after &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Participants continued to generalize to CONFUNATT patterns &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;even after&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; signiﬁcant exposure to the language</example>
		<phraseLemma>np even after np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; make &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We build a test set of over 1 problems and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;make a quantitative comparison with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; stateoftheart statistical methods</example>
		<phraseLemma>np make np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; per instance</phrase>
		<frequency>4</frequency>
		<example>We model exactly one STOP event &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;per instance&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np per instance</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; expressed in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While we rely heavily on the relationships &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between words intext and concept nodes expressed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parallel training data we ﬁnd this is not sufﬁcient for complete coverage</example>
		<phraseLemma>np between np express in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to compare &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However a very convenient alternative &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to compare&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the AMRese yields of candidate AMR parses to those of reference AMRese strings using a BLEU objective and forestbased MIRA</example>
		<phraseLemma>np be to compare np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; uses &lt;NP&gt; proposed by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To illustrate how a CNN works the following example &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;uses a simpliﬁed model proposed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Collobert which consists of one convolutional layer with the max pooling operation followed by one fully connected layer</example>
		<phraseLemma>np use np propose by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; then &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We now ﬁrst propose a solution to the issues and called Recursive convolutional neural network and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;then a solution to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the 1 issue called Chart Neural Network</example>
		<phraseLemma>np then np to np</phraseLemma>
	</can>
	<can>
		<phrase>It is obvious that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It is obvious that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this RCNN can solve the 1 issue we now show how it can make the composition functions adaptive</example>
		<phraseLemma>it be obvious that np</phraseLemma>
	</can>
	<can>
		<phrase>We have &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We can set those weight matrices such that when multiplying by the vector of a head &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we have a vector with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; highvalue entries</example>
		<phraseLemma>np we have np with np</phraseLemma>
	</can>
	<can>
		<phrase>We found that &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We found that the beam width for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dependency parser and the log probability beam for the other worked best</example>
		<phraseLemma>we find that np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is less sensitive to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The approach presented in this paper on the other hand does not need the information about the head word position and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is less sensitive to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parsing errors</example>
		<phraseLemma>np be less sensitive to np</phraseLemma>
	</can>
	<can>
		<phrase>Related to &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Related to our FCN is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Gated recursive convolutional neural network model proposed by Cho which is stacking n 1 convolutional neural layers using a windowsize gated kernel</example>
		<phraseLemma>related to np be np</phraseLemma>
	</can>
	<can>
		<phrase>By &lt;NP&gt; to &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;By analogy to the representation of instructions as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; parse trees we assume that each triple can be characterized by a grounding graph</example>
		<phraseLemma>by np to np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be characterized by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>By analogy to the representation of instructions as parse trees we assume that each triple &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be characterized by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a grounding graph</example>
		<phraseLemma>np can be characterize by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains &lt;NP&gt; corresponding to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The example &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains a node corresponding to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the primitive action move and several nodes corresponding to locations in the environment that are visible after the action is performed</example>
		<phraseLemma>np contain np correspond to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been the subject of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This task &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been the subject of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; focused attention in semantic parsing for several years resulting in a variety of sophisticated approaches</example>
		<phraseLemma>np have be the subject of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in G &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this work we ﬁrstly address bigram alignment models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in G&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 P</example>
		<phraseLemma>np in g np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be converted into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Typically such alignment algorithms &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be converted into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unsupervised algorithms in which similarity measures sim are learnt iteratively eg in an EMlike fashion Eger however in this paper we only investigate the supervised base version as indicated</example>
		<phraseLemma>np can be convert into np</phraseLemma>
	</can>
	<can>
		<phrase>To measure &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;To measure alignment quality for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the different systems for English we run experiments on sets of size x 1 where x = 1 1 1 1 1 1 1 1 and 1</example>
		<phraseLemma>to measure np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; suffers from &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Phonetisaurus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;suffers from the same problems as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; M 1 M but to a lesser degree</example>
		<phraseLemma>np suffer from np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicating that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Results are quite similar except that unigram and bigram alignment model have indistinguishable performance on the German data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicating that G 1 P is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a more complex task in English apparently not requiring bigram alignment models</example>
		<phraseLemma>np indicate that np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; which has &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While all matchups &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in both alignments are plausible the bigram model assigns here higher probability to the correct ed/d matchup in terminal position which has&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a particular meaning there namely that of a sufﬁx marker for past tense 1 In the German data there is a single instance where the unigram and bigram alignment model disagree namely in the alignment of stoffflasche/StOffl&amp;S@ which the unigram model falsely aligns as stoffflasche/StOffl&amp;S@ note that in the correct alignment f must follow ff not vice versa which depends on context information eg that o/O signiﬁes a short vowel which is followed by a double consonant not a single consonant</example>
		<phraseLemma>np in np which have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is followed by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>While all matchups in both alignments are plausible the bigram model assigns here higher probability to the correct ed/d matchup in terminal position which has a particular meaning there namely that of a sufﬁx marker for past tense 1 In the German data there is a single instance where the unigram and bigram alignment model disagree namely in the alignment of stoffflasche/StOffl&amp;S@ which the unigram model falsely aligns as stoffflasche/StOffl&amp;S@ note that in the correct alignment f must follow ff not vice versa which depends on context information eg that o/O signiﬁes a short vowel &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is followed by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a double consonant not a single consonant</example>
		<phraseLemma>np which be follow by np</phraseLemma>
	</can>
	<can>
		<phrase>As we explain in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;As we explain in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 in our experiments both our IM engine and existing ones delivered accuracy of 1</example>
		<phraseLemma>as we explain in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; sends &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In each phase the client &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;sends the typed keys to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the server with a timestamp and its IP address</example>
		<phraseLemma>np send np to np</phraseLemma>
	</can>
	<can>
		<phrase>We made &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We made a pSTC for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our IM engine from 1 sentences randomly obtained from Twitter by following the procedure which we explained in Subsection 1</example>
		<phraseLemma>we make np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the number of &lt;NP&gt; divided by the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Recall &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the number of correctly segmented words divided by the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words in the test corpus</example>
		<phraseLemma>np be the number of np divide by the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is mainly caused by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This oversegmentation problem &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is mainly caused by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; OOV words being divided into known words</example>
		<phraseLemma>np be mainly cause by np</phraseLemma>
	</can>
	<can>
		<phrase>We measured &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To investigate the impact of the log size &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we measured WS accuracy on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; TWItest when varying the log size during training</example>
		<phraseLemma>np we measure np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; detects &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Intuitively if the LSTM unit &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;detects an important feature from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an input sequence at early stage it easily carries this information over a long distance hence capturing the potential useful longdistance information</example>
		<phraseLemma>np detect np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has been applied successfully in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition the LSTM &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has been applied successfully in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; many NLP tasks such as text classification and machine translation</example>
		<phraseLemma>np have be apply successfully in np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we divide &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For CTB 1 dataset we divide&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the training development and test sets according to All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag</example>
		<phraseLemma>for np we divide np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to initialize &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The obtained embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to initialize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the character lookup table instead of random initialization</example>
		<phraseLemma>np be use to initialize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been made in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Semisupervised CWS Methods Considerable efforts &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been made in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the NLP community in the study of Chinese word segmentation</example>
		<phraseLemma>np have be make in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was &lt;NP&gt; used for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The conditional random ﬁeld model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was ﬁrst used for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; CWS tasks by who treated the CWS task as a sequence tagging problem and demonstrated this models effectiveness in detecting OOV words</example>
		<phraseLemma>np be np use for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; is trained using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A 1 gram language model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with KneserNey smoothing is trained using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; SRILM on the target language</example>
		<phraseLemma>np with np be train use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; still rely on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However these methods &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;still rely on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a heuristically determined threshold parameter</example>
		<phraseLemma>np still rely on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used for &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Slice sampling for an SCFG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used for efﬁciently sampling a derivation tree from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a reduced space of possible derivations</example>
		<phraseLemma>np be use for np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; is calculated by</phrase>
		<frequency>4</frequency>
		<example>By using IBM Model 1 probabilities &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in two directions Inside is calculated by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in np be calculate by</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as presented in Table 1</phrase>
		<frequency>4</frequency>
		<example>We used the full europarl GermanEnglish corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as presented in Table 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as present in table 1</phraseLemma>
	</can>
	<can>
		<phrase>In contrast &lt;NP&gt; achieved &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In contrast our hierarchical backoff model achieved&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; gains in translation quality without increasing the size of the extracted grammar when compared to the previous generative model</example>
		<phraseLemma>in contrast np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; were included in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The major differences were the use of the minimal phrase pairs used &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the previous work in which only minimal phrase pairs in the leaves of derivation trees were included in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model</example>
		<phraseLemma>np in np be include in np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we propose to use &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we propose to use&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; coverage which measures how well extracted phrases can recover the training data to bridge word alignment and phrasebased translation</example>
		<phraseLemma>in this work we propose to use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; consists &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For ChineseEnglish the training data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;consists of 1 M pairs of sentences with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 M Chinese words and 1 M English words</example>
		<phraseLemma>np consist np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; fails to distinguish between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>On the contrary Cht yields the lowest BLEU score because hard coverage &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;fails to distinguish between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; partially recoverable training examples as it assigns zero to all partially recoverable data</example>
		<phraseLemma>np fail to distinguish between np</phraseLemma>
	</can>
	<can>
		<phrase>Our work is inspired by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our work is inspired by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; three lines of research</example>
		<phraseLemma>we work be inspire by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; instead of &lt;NP&gt; results in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition we ﬁnd that using coverage &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;instead of phrase count results in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; better translation performance</example>
		<phraseLemma>np instead of np result in np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we have presented &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we have presented&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a general framework for optimizing word alignment with respect to machine translation</example>
		<phraseLemma>in this work we have present np</phraseLemma>
	</can>
	<can>
		<phrase>If we know that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If we know that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the translation of went´ı” is issue” the relatedness between and between can provide evidences that hold” and position” are the correct translations of ch´ıyoˇu” and lıchaˇng” respectively</example>
		<phraseLemma>if we know that np</phraseLemma>
	</can>
	<can>
		<phrase>In this way for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this way for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each bilingual rule with word alignments we will obtain a new lexical weight which can be used together with the original translation probabilities and lexical weight to improve lexical selection in SMT</example>
		<phraseLemma>in this way for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; propose &lt;NP&gt; to capture &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In order to capture sourceside context for lexical selection some researchers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;propose triggerbased lexicon models to capture&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; longdistance dependencies and many more researchers build classiﬁers with rich context information to select desirable translations during decoding</example>
		<phraseLemma>np propose np to capture np</phraseLemma>
	</can>
	<can>
		<phrase>Experiments on &lt;NP&gt; show that our model achieves &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Experiments on NIST ChineseEnglish test sets show that our model achieves&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a substantial improvement of up to 1 BLEU points over the baseline</example>
		<phraseLemma>experiment on np show that we model achieve np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; until &lt;NP&gt; is generated</phrase>
		<frequency>4</frequency>
		<example>This combination and reconstruction process of autoencoder repeats at each node &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;until the vector of the entire phrase is generated&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np until np be generate</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which incorporates &lt;NP&gt; into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To address this drawback we propose the BCorrRAE for bilingual phrase embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which incorporates bilingual correspondence information into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the learning process of structures and embeddings via word alignments</example>
		<phraseLemma>np which incorporate np into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to encourage &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>More speciﬁcally the former &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to encourage&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; alignmentconsistent generation of substructures while the latter is to minimize semantic distances between bilingual subphrases</example>
		<phraseLemma>np be to encourage np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; as follows</phrase>
		<frequency>4</frequency>
		<example>We exploit two kinds of phrasal similarity features &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on the learned phrase representations and their tree structures as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np base on np as follow</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are optimized by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Hyperparameters in all neural models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are optimized by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; random search based on related joint errors</example>
		<phraseLemma>np be optimize by np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; learns &lt;NP&gt; over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>They proposed a neural network joint model which augments streams of source with target ngrams and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;learns a NN model over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vector representation of such streams</example>
		<phraseLemma>np learn np over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; using &lt;NP&gt; trained from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The idea is to score the outdomain data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;using a model trained from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the indomain data and apply a cutoff based on the resulting scores</example>
		<phraseLemma>np use np train from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; apply &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The idea is to score the outdomain data using a model trained from the indomain data and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;apply a cutoff based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the resulting scores</example>
		<phraseLemma>np apply np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; integrated it into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Recently Devlin proposed a neural network joint model and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;integrated it into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the decoder as an additional feature</example>
		<phraseLemma>np integrate it into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; needs to compute &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Unfortunately training NNLMs are impractically slow because for each training instance the softmax output layer &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;needs to compute&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a summation over all words in the output vocabulary</example>
		<phraseLemma>np need to compute np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; we ﬁrst &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In both cases we ﬁrst&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; present the regularized loss function for the normalized output layer with the standard softmax followed by the corresponding unnormalized one using the noise contrastive estimation</example>
		<phraseLemma>in np we ﬁrst np</phraseLemma>
	</can>
	<can>
		<phrase>We ﬁrst &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In both cases &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we ﬁrst present the regularized loss function for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the normalized output layer with the standard softmax followed by the corresponding unnormalized one using the noise contrastive estimation</example>
		<phraseLemma>np we ﬁrst np for np</phraseLemma>
	</can>
	<can>
		<phrase>We need to generate &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To score outdomain sequences using a model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we need to generate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the sequences using the same vocabulary based on which the model was trained</example>
		<phraseLemma>np we need to generate np</phraseLemma>
	</can>
	<can>
		<phrase>We demonstrate that &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We demonstrate that sentence length and punctuation usage in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Chinese are not sufﬁcient clues for accurately detecting heavy sentences and present a richer classiﬁcation model that accurately identiﬁes these sentences</example>
		<phraseLemma>we demonstrate that np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; which in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this paper we introduce a novel ﬂavor of the task in the crosslingual setting which in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the long term may guide improvements in machine translation</example>
		<phraseLemma>np in np which in np</phraseLemma>
	</can>
	<can>
		<phrase>We seek to identify &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We seek to identify&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences in Chinese that would result in heavy sentences in English if translated to a single sentence</example>
		<phraseLemma>we seek to identify np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; are considered &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this crosslingual analysis sentences in Chinese are considered&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; contentheavy if their content would be more felicitously expressed in multiple sentences in English</example>
		<phraseLemma>np in np be consider np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; thus &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Moreover existing syntactically parsed corpora conveniently provide numerous examples of these fullstop commas and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;thus training data for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; systems to identify them</example>
		<phraseLemma>np thus np for np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we use &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For binary features we use an or operation&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on the feature values for each individual comma</example>
		<phraseLemma>for np we use np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that have &lt;NP&gt; across &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The next selected class is typed dependencies over universal POS tags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that have an edge across&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; commas in the sentence with an 1 increase in accuracy</example>
		<phraseLemma>np that have np across np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are added with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Finally partofspeech tags before each comma &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are added with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a 1 improvement of accuracy</example>
		<phraseLemma>np be add with np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; which are consistent with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For a pair of source and target span which are consistent with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; word alignment the string of target span is used as the span reference for the source span</example>
		<phraseLemma>for np which be consistent with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; restricted to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Of course our goal is not to produce a tagger &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;restricted to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Biblical lexicon</example>
		<phraseLemma>np restricted to np</phraseLemma>
	</can>
	<can>
		<phrase>We choose &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Based upon these assumptions &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we choose the best translation in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a language based on a comparison to a reference Bible the Modern King James Version in English</example>
		<phraseLemma>np we choose np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; the number of &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The correct number of chapters and verses are predeﬁned on MKJV volume and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;the number of matched verses on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each volume is greater than 1</example>
		<phraseLemma>np the number of np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provide information about &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Wordclasssequence features obtained by supervised clustering of the annotated training set replace the hidden tagsequence features frequently used for POS tagging and additional wordclass features obtained by unsupervised clustering of a very large unannotated corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provide information about&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; words not occurring in the training set</example>
		<phraseLemma>np provide information about np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; which consists of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>To obtain this corpus we ran a POS tagger &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on the LDC English Gigaword Fifth Edition corpus which consists of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; more than 1 billion words of English text from seven newswire sources</example>
		<phraseLemma>np on np which consist of np</phraseLemma>
	</can>
	<can>
		<phrase>We tagged &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We tagged this corpus using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the model described in Section 1 and a KNsmoothed tag dictionary as described in Section 1 with a threshold T = 1</example>
		<phraseLemma>we tag np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are needed to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>They suggest that syntactic analysis may help with automatic diacrtization but stop short of testing the idea and instead demonstrate that complex linguistic features and rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are needed to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; model complex Arabic case using gold syntactic analyses</example>
		<phraseLemma>np be need to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; used as &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The PATB tokenization as well as the CATiB POS tags were produced by the baseline system MADAMIRA and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;used as input to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the parser</example>
		<phraseLemma>np use as np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; achieving &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition using an extra hidden layer a neural network is capable of learning nonlinear relations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between automatic features achieving&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; feature combinations automatically</example>
		<phraseLemma>np between np achieve np</phraseLemma>
	</can>
	<can>
		<phrase>They use &lt;NP&gt; to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;They use simple binarization and clustering to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this end ﬁnding that the latter works better</example>
		<phraseLemma>they use np to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; features &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We use the arcstandard &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;features Φe as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Chen and Manning which is also based on the arceager templates of Zhang and Nivre similar to those of the baseline model L</example>
		<phraseLemma>np feature np as np</phraseLemma>
	</can>
	<can>
		<phrase>In our experiments we ﬁnd that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In our experiments we ﬁnd that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dense NN model and our combined model achieve better performances by using dropout but the other models do not beneﬁt from dropout</example>
		<phraseLemma>in we experiment we ﬁnd that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; integrating &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>There has been recent work &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;integrating continuous and discrete features for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the task of POS tagging</example>
		<phraseLemma>np integrate np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; needed to solve &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The component &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;needed to solve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this dilemma is an innertoouter greedy approximation to avoid an exhaustive search</example>
		<phraseLemma>np need to solve np</phraseLemma>
	</can>
	<can>
		<phrase>It has &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;It has also ﬁne grained empty categories of pro such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; speaker and hearer but we uniﬁed them into pro in our experiment</example>
		<phraseLemma>it have np as np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; without &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In gold parse condition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we used the trees of Keyaki Treebank without&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; empty categories as input to the systems</example>
		<phraseLemma>np we use np without np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; namely &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this paper we explore a particular source of usergenerated text &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;namely posts from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; technical support forums which are a popular means for customers to resolve their queries about a product</example>
		<phraseLemma>np namely np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; contains &lt;NP&gt; produced by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A learner corpus &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;contains utterances produced by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; language learners and serves as a resource for 1 language acquisition computational linguistic and computeraided language learning research</example>
		<phraseLemma>np contain np produce by np</phraseLemma>
	</can>
	<can>
		<phrase>The table also shows that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The table also shows that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the OOV rate of Foreebank with respect to WSJ/FTB is high</example>
		<phraseLemma>the table also show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is randomly split into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We do this using a 1 fold cross validation in which Foreebank &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is randomly split into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁve parts with each part used for the evaluation of the parsers trained on WSJ/FTB plus the other 1 parts</example>
		<phraseLemma>np be randomly split into np</phraseLemma>
	</can>
	<can>
		<phrase>Considering that &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Considering that Foreebank is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; orders of magnitude smaller than the WSJ/FTB these gains are encouraging</example>
		<phraseLemma>consider that np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; features result in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>As expected combining both our features and the browncluster &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;features result in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some additional gains</example>
		<phraseLemma>np feature result in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; trained on sections 1</phrase>
		<frequency>4</frequency>
		<example>All trees have automatically assigned partofspeech tags assigned by the TurboTagger POStagger 1 The trainset POStags were derived in a fold jackniﬁng and the different test datasets receive tags from a tagger &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;trained on sections 1&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np train on section 1</phraseLemma>
	</can>
	<can>
		<phrase>We parse &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For autoparsed data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we parse the text of the BLLIP corpus using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our baseline parser</example>
		<phraseLemma>np we parse np use np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; on &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our parser on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CoNLL 1 shared task dependency treebanks as well as on two English setups achieving the best published numbers in many cases</example>
		<phraseLemma>we evaluate np on np on np</phraseLemma>
	</can>
	<can>
		<phrase>We then train &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We then train a structured perceptron using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the output of all network activations as features as in Weiss</example>
		<phraseLemma>we then train np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which is important for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This transition system is able to produce nonprojective parse trees &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which is important for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some languages</example>
		<phraseLemma>np which be important for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is speciﬁed in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Whether punctuation is included in the evaluation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is speciﬁed in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each subsection</example>
		<phraseLemma>np be speciﬁed in np</phraseLemma>
	</can>
	<can>
		<phrase>We train on &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We train on the union&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; of each corporas training set and test on each domain separately</example>
		<phraseLemma>we train on np on np</phraseLemma>
	</can>
	<can>
		<phrase>We empirically show that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We empirically show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; six nbest parsers beneﬁt from parse fusion across six domains obtaining stateoftheart results</example>
		<phraseLemma>we empirically show that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as if they are &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our extension takes the nbest trees from a parser &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as if they are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; 1 best parses from n parsers then follows Sagae and Lavie</example>
		<phraseLemma>np as if they be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; provide &lt;NP&gt; along with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We assume that nbest parsers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;provide trees along with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; some kind of scores</example>
		<phraseLemma>np provide np along with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; as future work</phrase>
		<frequency>4</frequency>
		<example>We leave developing other nonlinear functions for fusion &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;as future work&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np as future work</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is obtained by averaging &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The context vector &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is obtained by averaging&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the embeddings of each word c wi and the prediction of the center word is obtained by performing a softmax over all the vocabulary V</example>
		<phraseLemma>np be obtain by average np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; illustrates &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>On the other hand the skipngram model process words at only 1 k words per 1 as it must predict every word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the window b Figure 1 illustrates&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the attention model for the prediction of the word south in the sentence antartica has little rainfall with the south pole making it a continental desert</example>
		<phraseLemma>np in np illustrate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; in both &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For syntax we evaluate our embeddings &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in the domain of partofspeech tagging in both&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; supervised and unsupervised tasks</example>
		<phraseLemma>np in np in both np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can apply to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This process is nondeterministic since usually more than one transition &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can apply to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a parse state</example>
		<phraseLemma>np can apply to np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; would make &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the monotonic system the Shift action would make&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the gold arc saw &amp;gt Jack newly unreachable</example>
		<phraseLemma>in np would make np</phraseLemma>
	</can>
	<can>
		<phrase>We plan to evaluate &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We plan to evaluate&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; this option more rigorously in future work</example>
		<phraseLemma>we plan to evaluate np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; when faced with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>There are many cases eg &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;when faced with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different word senses where transfer of a translation is not appropriate</example>
		<phraseLemma>np when face with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to produce &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Our goal &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to produce translations for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; OOV phrases by exploiting paraphrases from the multilingual PPDB by using graph propagation</example>
		<phraseLemma>np be to produce np for np</phraseLemma>
	</can>
	<can>
		<phrase>We try to ﬁnd &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Note that in this step OOVs are part of these regular nodes and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we try to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; translation in the propagation step for all of these regular nodes</example>
		<phraseLemma>np we try to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>We want to ﬁnd &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Based on MAD &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we want to ﬁnd&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; soft label vectors for each node by optimizing the objective function below</example>
		<phraseLemma>np we want to ﬁnd np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been added to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The original phrase table is now augmented with new entries providing translation candidates for potential OOVs Last column in Table 1 shows how many entries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been added to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the phrase table for each experimental settings</example>
		<phraseLemma>np have be add to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is the mean of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>MRR &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is the mean of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; reciprocal rank of the candidate list compared to the gold list Eqn</example>
		<phraseLemma>np be the mean of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are explored in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Among approaches which directly reﬁne the single label to more ﬁnegrained labels syntactic and semantic knowledge &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are explored in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; various ways</example>
		<phraseLemma>np be explore in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; utilize &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>They use these distributions to decorate nonterminal Xs in SCFG rules &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with a realvalued feature vectors and utilize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; these vectors to measure the similarities between source phrases and applied rules</example>
		<phraseLemma>np with np utilize np</phraseLemma>
	</can>
	<can>
		<phrase>Similar to &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Similar to this work Huang utilize treebank tags based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; dependency parsing to learn latent distributions</example>
		<phraseLemma>similar to np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; based on &lt;NP&gt; to learn &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Similar to this work Huang utilize treebank tags &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;based on dependency parsing to learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; latent distributions</example>
		<phraseLemma>np base on np to learn np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; will have &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>ﬁnds a point &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in semantic space to minimize the sum of Eu Similar to the center of gravity the semantic vector learned by this method acts as a semantic centroid for all vectors of phrases that are substituted by X Nonterminals in different hierarchical translation rules will have&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; different semantic centroids</example>
		<phraseLemma>np in np will have np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; will have &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The phrase p and nonterminal X &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;will have a high similarity score in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the representation space if they are semantically similar</example>
		<phraseLemma>np will have np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; learn &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In the twopass decoding we collect target phrase candidates from best translations for each source sentence generated by the baseline in the ﬁrst pass and learn&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; vector representations for these target phrase candidates</example>
		<phraseLemma>np in np learn np</phraseLemma>
	</can>
	<can>
		<phrase>We used &lt;NP&gt; of 1 for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We used a learning rate of 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; our minimum distance method that learned the centroid of phrase representations as the vector representation of the corresponding nonterminal</example>
		<phraseLemma>we use np of 1 for np</phraseLemma>
	</can>
	<can>
		<phrase>The features used in &lt;NP&gt; includes &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The features used in the baseline system includes&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a 1 gram language model trained on the Xinhua section of the English Gigaword corpus a 1 gram language model trained on the target part of the bilingual training data bidirectional translation probabilities bidirectional lexical weights a word count a phrase count and a glue rule count</example>
		<phraseLemma>the feature use in np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; respectively over &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>and achieve up to an absolute improvement of 1 and 1 BLEU points &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;respectively over&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the baseline on the development test set MT 1</example>
		<phraseLemma>np respectively over np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is consistent with &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>This ﬁnding &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is consistent with many other multiplepass systems in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; natural language processing eg twopass parsing</example>
		<phraseLemma>np be consistent with np in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which exploits &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Feng and Cohn present another generative wordbased Markov chain translation model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which exploits a hierarchical PitmanYor process for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; smoothing but it is only applied to induce word alignments</example>
		<phraseLemma>np which exploit np for np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we make use of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we make use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; hard alignments instead where we encode the alignments in the source and target sequences requiring no modiﬁcations of existing feedforward and recurrent NN architectures</example>
		<phraseLemma>in this work we make use of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; enable the use of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Unlike feedforward NNs recurrent NNs &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;enable the use of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unbounded context</example>
		<phraseLemma>np enable the use of np</phraseLemma>
	</can>
	<can>
		<phrase>We apply &lt;NP&gt; in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Therefore &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we apply RNNs in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; rescoring in this work and to allow for a direct comparison between FFNNs and RNNs we apply FFNNs in rescoring as well</example>
		<phraseLemma>np we apply np in np</phraseLemma>
	</can>
	<can>
		<phrase>The difference between &lt;NP&gt; is that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The difference between the FFNN and the URNN is that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the latter captures the unbounded source and target history that extends until the beginning of the sentences giving it an advantage over the FFNN</example>
		<phraseLemma>the difference between np be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is able to handle &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Second the FFNN &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is able to handle&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; unseen sequences by design without the need for the backing off workaround</example>
		<phraseLemma>np be able to handle np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; supporting &lt;NP&gt; that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>Rescoring with the feedforward and the recurrent network improves this even further &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;supporting the previous observation that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the ngram KN JTR and NNs complement each other</example>
		<phraseLemma>np support np that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; adjacent to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Unlike German the corresponding English phrase come back” has the words &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;adjacent to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; each other</example>
		<phraseLemma>np adjacent to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are not dependent on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The JTR models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are not dependent on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the phrasebased framework and one of the longterm goals is to perform standalone decoding with the JTR models independently of phrasebased systems</example>
		<phraseLemma>np be not dependent on np</phraseLemma>
	</can>
	<can>
		<phrase>For &lt;NP&gt; we achieve &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;For English to German translation we achieve&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; new stateoftheart results for both WMT 1 and WMT 1 outperforming previous SOTA systems backed by NMT models and ngram LM rerankers by more than 1 BLEU</example>
		<phraseLemma>for np we achieve np</phraseLemma>
	</can>
	<can>
		<phrase>We achieve &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For English to German translation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we achieve new stateoftheart results for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; both WMT 1 and WMT 1 outperforming previous SOTA systems backed by NMT models and ngram LM rerankers by more than 1 BLEU</example>
		<phraseLemma>np we achieve np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; do not capture &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Essentially they use local ngram information and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;do not capture semantic relations between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences</example>
		<phraseLemma>np do not capture np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; exploit &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We can go one step further to use preceding histories and following evidences in the same way and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;exploit bidirectional gated RNN as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the calculator</example>
		<phraseLemma>np exploit np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; between &lt;NP&gt; predicted &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>For model training we use the crossentropy error &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;between gold sentiment distribution P g and predicted&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentiment distribution P as the loss function</example>
		<phraseLemma>np between np predict np</phraseLemma>
	</can>
	<can>
		<phrase>We can conclude that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>Comparing between CNN and AverageSG &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;we can conclude that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; deep semantic compositionality is crucial for understanding the semantics and the sentiment of documents</example>
		<phraseLemma>np we can conclude that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; as described in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Speciﬁcally after obtaining sentence vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with convolutional neural network as described in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; Section 1 we carry out experiments in following settings</example>
		<phraseLemma>np with np as describe in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is that as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>One common issue with BPTT &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is that as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the errors get propagated they may soon become very small or very large that can lead to undesired values in weight matrices causing the training to fail</example>
		<phraseLemma>np be that as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; at &lt;NP&gt; are computed as follows</phrase>
		<frequency>4</frequency>
		<example>In an Elmantype bidirectional RNN the forward hidden layer h and the backward hidden layer h &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;at time t are computed as follows&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np at np be compute as follow</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; datasets provided by &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In our experiments we use the two review &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;datasets provided by&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the SemEval 1 task 1</example>
		<phraseLemma>np dataset provide by np</phraseLemma>
	</can>
	<can>
		<phrase>We use &lt;NP&gt; as &lt;NP&gt; which is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We use a linearchain CRF of order 1 as our baseline which is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the stateoftheart model for opinion target extraction</example>
		<phraseLemma>we use np as np which be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are more effective than &lt;NP&gt; for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>These results demonstrate that RNNs as sequence labelers &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are more effective than CRFs for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ﬁnegrained opinion mining tasks</example>
		<phraseLemma>np be more effective than np for np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been achieved with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Joint models of syntactic and semantic parsing have the potential to improve performance on both tasks—but to date the best results &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been achieved with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; pipelines</example>
		<phraseLemma>np have be achieve with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; depends on &lt;NP&gt; such as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>CCG also helps resolve cases where interpretation &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;depends on the valency of the predicate such as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; ergative verbs by learning lexical entries that pair syntactic arguments with semantic roles such as open</example>
		<phraseLemma>np depend on np such as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that pair &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>CCG also helps resolve cases where interpretation depends on the valency of the predicate such as ergative verbs by learning lexical entries &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that pair syntactic arguments with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic roles such as open</example>
		<phraseLemma>np that pair np with np</phraseLemma>
	</can>
	<can>
		<phrase>If &lt;NP&gt; occurs with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;If an argument of a category occurs with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a particular semantic role less than 1 times in the aligned data it is pruned from the training and decoding charts</example>
		<phraseLemma>if np occur with np</phraseLemma>
	</can>
	<can>
		<phrase>We evaluate &lt;NP&gt; as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We evaluate our parser as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a dependencybased SRL model on PropBank comparing with CoNLL 1 systems</example>
		<phraseLemma>we evaluate np as np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; purely from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Using latent syntax allows us to train the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;purely from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; semantic dependencies enabling future work to train against other annotations such as FrameNet Ontonotes or QASRL</example>
		<phraseLemma>np purely from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; across &lt;NP&gt; including &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Evaluation on the standard GeoQuery benchmark dataset shows that our approach achieves the stateoftheart &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;across various languages including&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; English German and Greek</example>
		<phraseLemma>np across np include np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; is not &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Motivated by the above observations we believe that semantic parsing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with standard SMT components is not&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an ideal approach</example>
		<phraseLemma>np with np be not np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be generalized to &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Casting the interpretation problem as selecting the most likely subset of literals &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be generalized to&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; grounded semantic parsing domains such as navigational instructions</example>
		<phraseLemma>np can be generalize to np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; that aligns &lt;NP&gt; with &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The closest work to ours is the recent work of Seo &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;that aligns geometric shapes with&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; their textual mentions but does not identify geometric relations or solve geometry problems</example>
		<phraseLemma>np that align np with np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; with &lt;NP&gt; that includes</phrase>
		<frequency>4</frequency>
		<example>We formally represent logical expressions in the geometry domain &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;with the language Ω a subset of typed ﬁrstorder logic that includes&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np with np that include</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; then computes &lt;NP&gt; between &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>With simple textual processing this baseline extracts numerical relations from the question text and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;then computes the scale between&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the units in the question and the pixels in the diagram</example>
		<phraseLemma>np then compute np between np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; which determines if &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We address this task by extending a vision model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;which determines if&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a sentence is depicted by a video</example>
		<phraseLemma>np which determine if np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in which &lt;NP&gt; is expressed</phrase>
		<frequency>4</frequency>
		<example>In this work we focus on the problem of grounding language in the visual modality and introduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in which the linguistic content is expressed&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np in which np be express</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; have been extensively studied in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The interactions between linguistic and visual information in human sentence processing &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;have been extensively studied in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; psycholinguistics and cognitive psychology</example>
		<phraseLemma>np have be extensively study in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; described in detail in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In this framework &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;described in detail in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; section 1 a sentence and an accompanying interpretation encoded in ﬁrst order logic give rise to a grounded model that matches a video against the provided sentence interpretation</example>
		<phraseLemma>np describe in detail in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are combined to form &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Together the re forestation for the sentence and the candidate object locations &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are combined to form&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a model which can determine if a given interpretation is depicted by the video</example>
		<phraseLemma>np be combine to form np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; along with &lt;NP&gt; from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Figure 1 presents the syntactic parses for this example &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;along with frames from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the respective videos</example>
		<phraseLemma>np along with np from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; along with the number of &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Table 1 presents the corpus templates for each ambiguity class &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;along with the number of&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; sentences generated from each template</example>
		<phraseLemma>np along with the number of np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; described in &lt;NP&gt; on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We tested the performance of the model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;described in the previous section&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; on the LAVA dataset presented in section 1</example>
		<phraseLemma>np describe in np on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; allowing for &lt;NP&gt; than &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In addition to being conceptually simpler than PRA SFE is much more efﬁcient reducing computation by an order of magnitude and more expressive &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;allowing for much richer features than&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; paths between two nodes in a graph</example>
		<phraseLemma>np allow for np than np</phraseLemma>
	</can>
	<can>
		<phrase>The feature generated from &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The feature generated from&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the path typeGENDER would be COMPARISONGENDER</example>
		<phraseLemma>the feature generate from np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; on &lt;NP&gt; is used as &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We judge statistical signiﬁcance using a paired permutation test where the average precision &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;on each relation is used as&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; paired data</example>
		<phraseLemma>np on np be use as np</phraseLemma>
	</can>
	<can>
		<phrase>In this work we train &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In this work we train&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; continuous representations of knowledge base and textual relations jointly which allows for deeper interactions between the sources of information</example>
		<phraseLemma>in this work we train np</phraseLemma>
	</can>
	<can>
		<phrase>We begin by introducing &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;We begin by introducing&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; notation to deﬁne the task largely following the terminology in Nickel</example>
		<phraseLemma>we begin by introduce np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; score &lt;NP&gt; based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>They all learn latent continuous representations of relations and entities or entity pairs and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;score possible triples based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the learned continuous representations</example>
		<phraseLemma>np score np base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is mapped to &lt;NP&gt; using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the ﬁrst layer each word or directed labeled arc &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is mapped to a continuous representation using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; an embedding matrix V</example>
		<phraseLemma>np be map to np use np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is ranked in &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We report the mean reciprocal rank of the correct entity as well as HITS@ 1 — the percentage of test triples for which the correct entity &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is ranked in&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the top</example>
		<phraseLemma>np be rank in np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; was &lt;NP&gt; 1 for &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The best dimension for latent feature vectors &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;was for most KBonly models and 1 for&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the two model conﬁgurations including F</example>
		<phraseLemma>np be np 1 for np</phraseLemma>
	</can>
	<can>
		<phrase>The method is based on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;The method is based on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a probabilistic model that incorporates assumptions about how authors decide what to write how joint decisions work when papers are coauthored and how individual and community preferences shift over time</example>
		<phraseLemma>the method be base on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; would be that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>In such languages a more reasonable assumption &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;would be that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; orthographic similarity is evidence for functional similarity</example>
		<phraseLemma>np would be that np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; combine them into &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We use bidirectional LSTMs to read” the character sequences that constitute each word and &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;combine them into&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a vector representation of the word</example>
		<phraseLemma>np combine they into np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to initialize &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>A common practice to this end &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to initialize&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the word lookup table with the parameters trained on an unsupervised task</example>
		<phraseLemma>np be to initialize np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in &lt;NP&gt; based &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In a distributional model of meaning the semantic representation of a word is given as a vector in some high dimensional vector space obtained either by explicitly collecting cooccurrence statistics of the target word with words belonging to a representative subset of the vocabulary or by directly optimizing the word vectors against an objective function in some neuralnetwork based&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; architecture</example>
		<phraseLemma>np in np base np</phraseLemma>
	</can>
	<can>
		<phrase>In &lt;NP&gt; are &lt;NP&gt; so &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;In some cases the vectors are POStag speciﬁc so&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; that book as noun and book as verb are represented by different vectors</example>
		<phraseLemma>in np be np so np</phraseLemma>
	</can>
	<can>
		<phrase>While we focus on &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;While we focus on&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; recursive and recurrent neural network architectures the general ideas we will discuss are in principle modelindependent</example>
		<phraseLemma>while we focus on np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is used to describe &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Especially for cases of homonymy where the same word &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is used to describe&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; two or more completely unrelated concepts this approach is problematic</example>
		<phraseLemma>np be use to describe np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; in order to select &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We can therefore make use of the contextual information &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;in order to select&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the most appropriate sense for each ambiguous word</example>
		<phraseLemma>np in order to select np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; indicating whether &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>Reply links are sometimes augmented with a dialog act label &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;indicating whether the child post is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a question answer or conﬁrmation to the parent post</example>
		<phraseLemma>np indicate whether np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are in &lt;NP&gt; is &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>If these nodes &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are in a governing relation then ℓ is&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the dominating node</example>
		<phraseLemma>np be in np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; can be used for training</phrase>
		<frequency>4</frequency>
		<example>For the PCFG model only the projective data &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;can be used for training&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np can be use for training</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; has a number of advantages</phrase>
		<frequency>4</frequency>
		<example>Compared to PCFG the LCFRS model &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;has a number of advantages&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt;</example>
		<phraseLemma>np have a number of advantage</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; are used to denote &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>In the block sampler indicators &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;are used to denote&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a customer creating a table up to level u as the root 1 for collection level and 1 for the document level and u = ∅ indicates no table has been created</example>
		<phraseLemma>np be use to denote np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; after &lt;NP&gt; are &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>We ﬁt a model with common and noncommon initial topics using CHDP which produced root topics &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;after 1 iterationsThe perplexity scores are&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; roughly the same when CLDA uses the same average number of topics per collection except when numbers of topics are very asymmetric</example>
		<phraseLemma>np after np be np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is to enable &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>However the goal of our models &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is to enable&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; a deeper analysis of large weaklyrelated corpora which we next discuss</example>
		<phraseLemma>np be to enable np</phraseLemma>
	</can>
	<can>
		<phrase>&lt;NP&gt; is implemented in Python using &lt;NP&gt;</phrase>
		<frequency>4</frequency>
		<example>The source code &lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;is implemented in Python using&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the Theano library a ﬂexible linear algebra compiler that can optimize userspeciﬁed computations with efﬁcient automatic lowlevel implementations including gradient calculation</example>
		<phraseLemma>np be implement in python use np</phraseLemma>
	</can>
	<can>
		<phrase>Our results also show that &lt;CL&gt;</phrase>
		<frequency>4</frequency>
		<example>&lt;strong&gt;&lt;i&gt;&lt;font color=#EE6C01&gt;Our results also show that&lt;/font&gt;&lt;/i&gt;&lt;/strong&gt; the CNN model where our feature map is replaced with traditional linear map performs worse than our full model</example>
		<phraseLemma>we result also show that np</phraseLemma>
	</can>
</result>
